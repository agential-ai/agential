{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hotpotqa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hotpotqa.py\n",
    "\"\"\"Run ReAct on HotpotQA.\"\"\"\n",
    "from agential.eval.metrics.classification import EM, f1, precision, recall\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tiktoken\n",
    "import warnings\n",
    "\n",
    "from agential.agents.react.agent import ReAct\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agential.core.llm import LLM\n",
    "\n",
    "from experiments.utils import set_seed\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run ReAct experiments.\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\", help=\"The model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "parser.add_argument(\"--max_steps\", type=int, default=6, help=\"Maximum number of steps\")\n",
    "parser.add_argument(\"--max_tokens\", type=int, default=5000, help=\"Maximum number of tokens\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "set_seed(args.seed)\n",
    "root_dir = \"output\"\n",
    "method_name = \"react\"\n",
    "benchmark = \"hotpotqa\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = load_dataset(\"alckasoc/hotpotqa_500\")['train']\n",
    "\n",
    "    model = args.model\n",
    "    seed = args.seed\n",
    "    max_steps = args.max_steps\n",
    "    max_tokens = args.max_tokens\n",
    "\n",
    "    output_path = os.path.join(root_dir, benchmark)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    llm = LLM(\n",
    "        model, \n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\"), \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(args.model)\n",
    "    except:\n",
    "        enc = tiktoken.get_encoding(\"gpt-3.5-turbo\")\n",
    "\n",
    "    method = ReAct(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    "        max_steps=max_steps,\n",
    "        max_tokens=max_tokens,\n",
    "        enc=enc,\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=benchmark, \n",
    "        entity=\"agential\",\n",
    "        config={\n",
    "            \"model\": model,\n",
    "            \"seed\": seed,\n",
    "            \"max_steps\": max_steps,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        },\n",
    "        group=method_name,\n",
    "        tags=[f\"method={method_name}\", f\"model={model}\", f\"seed={seed}\", f\"max_steps={max_steps}\", f\"max_tokens={max_tokens}\"],\n",
    "    )\n",
    "\n",
    "    eval_table_data = []\n",
    "    perf_table_data = []\n",
    "    em_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    outputs = []\n",
    "\n",
    "    for instance in data:\n",
    "        question = instance[\"question\"]\n",
    "        answer = instance[\"answer\"]\n",
    "\n",
    "        # Inference.\n",
    "        out = method.generate(\n",
    "            question=question,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics.\n",
    "        is_correct = int(EM(out.answer, answer))\n",
    "        precision_score = precision(out.answer, answer)\n",
    "        recall_score = recall(out.answer, answer)\n",
    "        f1_score = f1(out.answer, answer)\n",
    "\n",
    "        # Update scores.\n",
    "        em_scores.append(is_correct)\n",
    "        precision_scores.append(precision_score)\n",
    "        recall_scores.append(recall_score)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # Update tables.\n",
    "        eval_table_data.append([question, answer, out.answer, is_correct, precision_score, recall_score, f1_score])\n",
    "        perf_table_data.append([\n",
    "            out.total_prompt_tokens, \n",
    "            out.total_completion_tokens, \n",
    "            out.total_tokens, \n",
    "            out.total_prompt_cost,\n",
    "            out.total_completion_cost,\n",
    "            out.total_cost,\n",
    "            out.total_prompt_time,\n",
    "            out.total_time\n",
    "        ])\n",
    "\n",
    "        # Update outputs.\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Log metrics.\n",
    "        run.log({\n",
    "            \"em\": is_correct,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        })\n",
    "\n",
    "    total_em = sum(em_scores) / len(em_scores)\n",
    "    total_precision = sum(precision_scores) / len(precision_scores)\n",
    "    total_recall = sum(recall_scores) / len(recall_scores)\n",
    "    total_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    eval_table = wandb.Table(data=eval_table_data, columns=[\"question\", \"answer\", \"predicted_answer\", \"EM\", \"precision\", \"recall\", \"f1\"])\n",
    "    perf_table = wandb.Table(data=perf_table_data, columns=[\"total_prompt_tokens\", \"total_completion_tokens\", \"total_tokens\", \"total_prompt_cost\", \"total_completion_cost\", \"total_cost\", \"total_prompt_time\", \"total_time\"])\n",
    "\n",
    "    outputs_save_path = os.path.join(output_path, f\"{run.name}.pkl\")\n",
    "    with open(outputs_save_path, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    artifact = wandb.Artifact(name=run.name, type=\"output\")\n",
    "    artifact.add_file(local_path=outputs_save_path, name=\"outputs.pkl\")\n",
    "    artifact.save()\n",
    "\n",
    "    run.log({\n",
    "        f\"{run.name}_eval\": eval_table,\n",
    "        f\"{run.name}_perf\": perf_table\n",
    "    })\n",
    "\n",
    "    run.log({\n",
    "        \"total_em\": total_em,\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1,\n",
    "    })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fever.py\n",
    "\"\"\"Run ReAct on FEVER.\"\"\"\n",
    "from agential.eval.metrics.classification import EM, f1, precision, recall\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tiktoken\n",
    "import warnings\n",
    "\n",
    "from agential.agents.react.agent import ReAct\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agential.core.llm import LLM\n",
    "\n",
    "from experiments.utils import set_seed\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run ReAct experiments.\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\", help=\"The model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "parser.add_argument(\"--max_steps\", type=int, default=6, help=\"Maximum number of steps\")\n",
    "parser.add_argument(\"--max_tokens\", type=int, default=5000, help=\"Maximum number of tokens\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "set_seed(args.seed)\n",
    "root_dir = \"output\"\n",
    "method_name = \"react\"\n",
    "benchmark = \"fever\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open(\"../../data/fever/paper_dev_s42_sample500.json\", 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model = args.model\n",
    "    seed = args.seed\n",
    "    max_steps = args.max_steps\n",
    "    max_tokens = args.max_tokens\n",
    "\n",
    "    output_path = os.path.join(root_dir, benchmark)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    llm = LLM(\n",
    "        model, \n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\"), \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(args.model)\n",
    "    except:\n",
    "        enc = tiktoken.get_encoding(\"gpt-3.5-turbo\")\n",
    "\n",
    "    method = ReAct(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    "        max_steps=max_steps,\n",
    "        max_tokens=max_tokens,\n",
    "        enc=enc,\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=benchmark, \n",
    "        entity=\"agential\",\n",
    "        config={\n",
    "            \"model\": model,\n",
    "            \"seed\": seed,\n",
    "            \"max_steps\": max_steps,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        },\n",
    "        group=method_name,\n",
    "        tags=[f\"method={method_name}\", f\"model={model}\", f\"seed={seed}\", f\"max_steps={max_steps}\", f\"max_tokens={max_tokens}\"],\n",
    "    )\n",
    "\n",
    "    eval_table_data = []\n",
    "    perf_table_data = []\n",
    "    em_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    outputs = []\n",
    "\n",
    "    for instance in data:\n",
    "        question = instance[\"claim\"]\n",
    "        answer = instance[\"label\"]\n",
    "\n",
    "        # Inference.\n",
    "        out = method.generate(\n",
    "            question=question,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics.\n",
    "        is_correct = int(EM(out.answer, answer))\n",
    "        precision_score = precision(out.answer, answer)\n",
    "        recall_score = recall(out.answer, answer)\n",
    "        f1_score = f1(out.answer, answer)\n",
    "\n",
    "        # Update scores.\n",
    "        em_scores.append(is_correct)\n",
    "        precision_scores.append(precision_score)\n",
    "        recall_scores.append(recall_score)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # Update tables.\n",
    "        eval_table_data.append([question, answer, out.answer, is_correct, precision_score, recall_score, f1_score])\n",
    "        perf_table_data.append([\n",
    "            out.total_prompt_tokens, \n",
    "            out.total_completion_tokens, \n",
    "            out.total_tokens, \n",
    "            out.total_prompt_cost,\n",
    "            out.total_completion_cost,\n",
    "            out.total_cost,\n",
    "            out.total_prompt_time,\n",
    "            out.total_time\n",
    "        ])\n",
    "\n",
    "        # Update outputs.\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Log metrics.\n",
    "        run.log({\n",
    "            \"em\": is_correct,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        })\n",
    "\n",
    "    total_em = sum(em_scores) / len(em_scores)\n",
    "    total_precision = sum(precision_scores) / len(precision_scores)\n",
    "    total_recall = sum(recall_scores) / len(recall_scores)\n",
    "    total_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    eval_table = wandb.Table(data=eval_table_data, columns=[\"question\", \"answer\", \"predicted_answer\", \"EM\", \"precision\", \"recall\", \"f1\"])\n",
    "    perf_table = wandb.Table(data=perf_table_data, columns=[\"total_prompt_tokens\", \"total_completion_tokens\", \"total_tokens\", \"total_prompt_cost\", \"total_completion_cost\", \"total_cost\", \"total_prompt_time\", \"total_time\"])\n",
    "\n",
    "    outputs_save_path = os.path.join(output_path, f\"{run.name}.pkl\")\n",
    "    with open(outputs_save_path, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    artifact = wandb.Artifact(name=run.name, type=\"output\")\n",
    "    artifact.add_file(local_path=outputs_save_path, name=\"outputs.pkl\")\n",
    "    artifact.save()\n",
    "\n",
    "    run.log({\n",
    "        f\"{run.name}_eval\": eval_table,\n",
    "        f\"{run.name}_perf\": perf_table\n",
    "    })\n",
    "\n",
    "    run.log({\n",
    "        \"total_em\": total_em,\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1,\n",
    "    })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ambignq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ambignq.py\n",
    "\"\"\"Run ReAct on AmbigNQ.\"\"\"\n",
    "from agential.eval.metrics.classification import EM, f1, precision, recall\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tiktoken\n",
    "import warnings\n",
    "\n",
    "from agential.agents.react.agent import ReAct\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agential.core.llm import LLM\n",
    "\n",
    "from experiments.utils import set_seed\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run ReAct experiments.\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\", help=\"The model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "parser.add_argument(\"--max_steps\", type=int, default=6, help=\"Maximum number of steps\")\n",
    "parser.add_argument(\"--max_tokens\", type=int, default=5000, help=\"Maximum number of tokens\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "set_seed(args.seed)\n",
    "root_dir = \"output\"\n",
    "method_name = \"react\"\n",
    "benchmark = \"ambignq\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open(\"../../data/ambignq/dev_light_s42_sample500.json\", 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model = args.model\n",
    "    seed = args.seed\n",
    "    max_steps = args.max_steps\n",
    "    max_tokens = args.max_tokens\n",
    "\n",
    "    output_path = os.path.join(root_dir, benchmark)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    llm = LLM(\n",
    "        model, \n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\"), \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(args.model)\n",
    "    except:\n",
    "        enc = tiktoken.get_encoding(\"gpt-3.5-turbo\")\n",
    "\n",
    "    method = ReAct(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    "        max_steps=max_steps,\n",
    "        max_tokens=max_tokens,\n",
    "        enc=enc,\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=benchmark, \n",
    "        entity=\"agential\",\n",
    "        config={\n",
    "            \"model\": model,\n",
    "            \"seed\": seed,\n",
    "            \"max_steps\": max_steps,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        },\n",
    "        group=method_name,\n",
    "        tags=[f\"method={method_name}\", f\"model={model}\", f\"seed={seed}\", f\"max_steps={max_steps}\", f\"max_tokens={max_tokens}\"],\n",
    "    )\n",
    "\n",
    "    eval_table_data = []\n",
    "    perf_table_data = []\n",
    "    em_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    outputs = []\n",
    "\n",
    "    for instance in data:\n",
    "        question = instance[\"question\"]\n",
    "        annotations = instance[\"annotations\"]\n",
    "\n",
    "        # Collect all answers.\n",
    "        answers = []\n",
    "        for ann in annotations:\n",
    "            if ann['type'] =='singleAnswer':\n",
    "                answers.extend(ann['answer'])\n",
    "            else:\n",
    "                for qa_pair in ann['qaPairs']:\n",
    "                    answers.extend(qa_pair['answer'])\n",
    "        answers = list(set(answers))\n",
    "\n",
    "        # Inference.\n",
    "        out = method.generate(\n",
    "            question=question,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics.\n",
    "        is_correct = int(any([EM(out.answer, answer) for answer in answers]))\n",
    "        precision_score = max([precision(out.answer, answer) for answer in answers])\n",
    "        recall_score = max([recall(out.answer, answer) for answer in answers])\n",
    "        f1_score = max([f1(out.answer, answer) for answer in answers])\n",
    "\n",
    "        # Update scores.\n",
    "        em_scores.append(is_correct)\n",
    "        precision_scores.append(precision_score)\n",
    "        recall_scores.append(recall_score)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # Update tables.\n",
    "        eval_table_data.append([question, str(answers), out.answer, is_correct, precision_score, recall_score, f1_score])\n",
    "        perf_table_data.append([\n",
    "            out.total_prompt_tokens, \n",
    "            out.total_completion_tokens, \n",
    "            out.total_tokens, \n",
    "            out.total_prompt_cost,\n",
    "            out.total_completion_cost,\n",
    "            out.total_cost,\n",
    "            out.total_prompt_time,\n",
    "            out.total_time\n",
    "        ])\n",
    "\n",
    "        # Update outputs.\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Log metrics.\n",
    "        run.log({\n",
    "            \"em\": is_correct,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        })\n",
    "\n",
    "    total_em = sum(em_scores) / len(em_scores)\n",
    "    total_precision = sum(precision_scores) / len(precision_scores)\n",
    "    total_recall = sum(recall_scores) / len(recall_scores)\n",
    "    total_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    eval_table = wandb.Table(data=eval_table_data, columns=[\"question\", \"answer\", \"predicted_answer\", \"EM\", \"precision\", \"recall\", \"f1\"])\n",
    "    perf_table = wandb.Table(data=perf_table_data, columns=[\"total_prompt_tokens\", \"total_completion_tokens\", \"total_tokens\", \"total_prompt_cost\", \"total_completion_cost\", \"total_cost\", \"total_prompt_time\", \"total_time\"])\n",
    "\n",
    "    outputs_save_path = os.path.join(output_path, f\"{run.name}.pkl\")\n",
    "    with open(outputs_save_path, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    artifact = wandb.Artifact(name=run.name, type=\"output\")\n",
    "    artifact.add_file(local_path=outputs_save_path, name=\"outputs.pkl\")\n",
    "    artifact.save()\n",
    "\n",
    "    run.log({\n",
    "        f\"{run.name}_eval\": eval_table,\n",
    "        f\"{run.name}_perf\": perf_table\n",
    "    })\n",
    "\n",
    "    run.log({\n",
    "        \"total_em\": total_em,\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1,\n",
    "    })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing triviaqa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile triviaqa.py\n",
    "\"\"\"Run ReAct on TriviaQA.\"\"\"\n",
    "from agential.eval.metrics.classification import EM, f1, precision, recall\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import tiktoken\n",
    "import warnings\n",
    "\n",
    "from agential.agents.react.agent import ReAct\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agential.core.llm import LLM\n",
    "\n",
    "from experiments.utils import set_seed\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run ReAct experiments.\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\", help=\"The model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "parser.add_argument(\"--max_steps\", type=int, default=6, help=\"Maximum number of steps\")\n",
    "parser.add_argument(\"--max_tokens\", type=int, default=5000, help=\"Maximum number of tokens\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "set_seed(args.seed)\n",
    "root_dir = \"output\"\n",
    "method_name = \"react\"\n",
    "benchmark = \"triviaqa\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = load_dataset(\"alckasoc/triviaqa_500\")['train']\n",
    "\n",
    "    model = args.model\n",
    "    seed = args.seed\n",
    "    max_steps = args.max_steps\n",
    "    max_tokens = args.max_tokens\n",
    "\n",
    "    output_path = os.path.join(root_dir, benchmark)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    llm = LLM(\n",
    "        model, \n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\"), \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(args.model)\n",
    "    except:\n",
    "        enc = tiktoken.get_encoding(\"gpt-3.5-turbo\")\n",
    "\n",
    "    method = ReAct(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    "        max_steps=max_steps,\n",
    "        max_tokens=max_tokens,\n",
    "        enc=enc,\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=benchmark, \n",
    "        entity=\"agential\",\n",
    "        config={\n",
    "            \"model\": model,\n",
    "            \"seed\": seed,\n",
    "            \"max_steps\": max_steps,\n",
    "            \"max_tokens\": max_tokens,\n",
    "        },\n",
    "        group=method_name,\n",
    "        tags=[f\"method={method_name}\", f\"model={model}\", f\"seed={seed}\", f\"max_steps={max_steps}\", f\"max_tokens={max_tokens}\"],\n",
    "    )\n",
    "\n",
    "    eval_table_data = []\n",
    "    perf_table_data = []\n",
    "    em_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    outputs = []\n",
    "\n",
    "    for instance in data:\n",
    "        question = instance[\"question\"]\n",
    "        answers = list(set(instance[\"answer\"]['normalized_aliases']))\n",
    "\n",
    "        # Inference.\n",
    "        out = method.generate(\n",
    "            question=question,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics.\n",
    "        is_correct = int(any([EM(out.answer, answer) for answer in answers]))\n",
    "        precision_score = max([precision(out.answer, answer) for answer in answers])\n",
    "        recall_score = max([recall(out.answer, answer) for answer in answers])\n",
    "        f1_score = max([f1(out.answer, answer) for answer in answers])\n",
    "\n",
    "        # Update scores.\n",
    "        em_scores.append(is_correct)\n",
    "        precision_scores.append(precision_score)\n",
    "        recall_scores.append(recall_score)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # Update tables.\n",
    "        eval_table_data.append([question, str(answers), out.answer, is_correct, precision_score, recall_score, f1_score])\n",
    "        perf_table_data.append([\n",
    "            out.total_prompt_tokens, \n",
    "            out.total_completion_tokens, \n",
    "            out.total_tokens, \n",
    "            out.total_prompt_cost,\n",
    "            out.total_completion_cost,\n",
    "            out.total_cost,\n",
    "            out.total_prompt_time,\n",
    "            out.total_time\n",
    "        ])\n",
    "\n",
    "        # Update outputs.\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Log metrics.\n",
    "        run.log({\n",
    "            \"em\": is_correct,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        })\n",
    "\n",
    "    total_em = sum(em_scores) / len(em_scores)\n",
    "    total_precision = sum(precision_scores) / len(precision_scores)\n",
    "    total_recall = sum(recall_scores) / len(recall_scores)\n",
    "    total_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    eval_table = wandb.Table(data=eval_table_data, columns=[\"question\", \"answer\", \"predicted_answer\", \"EM\", \"precision\", \"recall\", \"f1\"])\n",
    "    perf_table = wandb.Table(data=perf_table_data, columns=[\"total_prompt_tokens\", \"total_completion_tokens\", \"total_tokens\", \"total_prompt_cost\", \"total_completion_cost\", \"total_cost\", \"total_prompt_time\", \"total_time\"])\n",
    "\n",
    "    outputs_save_path = os.path.join(output_path, f\"{run.name}.pkl\")\n",
    "    with open(outputs_save_path, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    artifact = wandb.Artifact(name=run.name, type=\"output\")\n",
    "    artifact.add_file(local_path=outputs_save_path, name=\"outputs.pkl\")\n",
    "    artifact.save()\n",
    "\n",
    "    run.log({\n",
    "        f\"{run.name}_eval\": eval_table,\n",
    "        f\"{run.name}_perf\": perf_table\n",
    "    })\n",
    "\n",
    "    run.log({\n",
    "        \"total_em\": total_em,\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1,\n",
    "    })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
