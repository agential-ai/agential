{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hotpotqa.py\n",
    "\"\"\"Run Self-Refine on HotpotQA.\"\"\"\n",
    "from agential.eval.metrics.classification import EM, f1, precision, recall\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "from agential.agents.self_refine.agent import SelfRefine\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agential.core.llm import LLM\n",
    "\n",
    "from experiments.utils import set_seed\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run Self-Refine experiments.\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\", help=\"The model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "parser.add_argument(\"--patience\", type=int, default=1, help=\"Patience\")\n",
    "parser.add_argument(\"--fewshot_type\", type=str, default=\"cot\", help=\"The few-shot type\")\n",
    "parser.add_argument(\"--max_interactions\", type=int, default=3, help=\"Max interactions\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "set_seed(args.seed)\n",
    "root_dir = \"output\"\n",
    "method_name = \"self_refine\"\n",
    "benchmark = \"hotpotqa\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = load_dataset(\"alckasoc/hotpotqa_500\")['train']\n",
    "\n",
    "    model = args.model\n",
    "    seed = args.seed\n",
    "    patience = args.patience\n",
    "    fewshot_type = args.fewshot_type\n",
    "    max_interactions = args.max_interactions\n",
    "\n",
    "    output_path = os.path.join(root_dir, benchmark)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    llm = LLM(\n",
    "        model, \n",
    "        organization=os.getenv(\"OPENAI_ORGANIZATION\"), \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    method = SelfRefine(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    "        patience=patience\n",
    "    )\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=benchmark, \n",
    "        entity=\"agential\",\n",
    "        config={\n",
    "            \"model\": model,\n",
    "            \"seed\": seed,\n",
    "            \"patience\": patience,\n",
    "            \"fewshot_type\": fewshot_type,\n",
    "            \"max_interactions\": max_interactions\n",
    "        },\n",
    "        group=method_name,\n",
    "        tags=[f\"method={method_name}\", f\"model={model}\", f\"seed={seed}\", f\"patience={patience}\", f\"fewshot_type={fewshot_type}\", f\"max_interactions={max_interactions}\"],\n",
    "    )\n",
    "\n",
    "    eval_table_data = []\n",
    "    perf_table_data = []\n",
    "    em_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    outputs = []\n",
    "\n",
    "    for instance in data:\n",
    "        question = instance[\"question\"]\n",
    "        answer = instance[\"answer\"]\n",
    "\n",
    "        # Inference.\n",
    "        out = method.generate(\n",
    "            question=question,\n",
    "            fewshot_type=fewshot_type,\n",
    "            max_interactions=max_interactions\n",
    "        )\n",
    "\n",
    "        # Calculate metrics.\n",
    "        is_correct = int(EM(out.answer, answer))\n",
    "        precision_score = precision(out.answer, answer)\n",
    "        recall_score = recall(out.answer, answer)\n",
    "        f1_score = f1(out.answer, answer)\n",
    "\n",
    "        # Update scores.\n",
    "        em_scores.append(is_correct)\n",
    "        precision_scores.append(precision_score)\n",
    "        recall_scores.append(recall_score)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "        # Update tables.\n",
    "        eval_table_data.append([question, answer, out.answer, is_correct, precision_score, recall_score, f1_score])\n",
    "        perf_table_data.append([\n",
    "            out.total_prompt_tokens, \n",
    "            out.total_completion_tokens, \n",
    "            out.total_tokens, \n",
    "            out.total_prompt_cost,\n",
    "            out.total_completion_cost,\n",
    "            out.total_cost,\n",
    "            out.total_prompt_time,\n",
    "            out.total_time\n",
    "        ])\n",
    "\n",
    "        # Update outputs.\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Log metrics.\n",
    "        run.log({\n",
    "            \"em\": is_correct,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        })\n",
    "\n",
    "    total_em = sum(em_scores) / len(em_scores)\n",
    "    total_precision = sum(precision_scores) / len(precision_scores)\n",
    "    total_recall = sum(recall_scores) / len(recall_scores)\n",
    "    total_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    eval_table = wandb.Table(data=eval_table_data, columns=[\"question\", \"answer\", \"predicted_answer\", \"EM\", \"precision\", \"recall\", \"f1\"])\n",
    "    perf_table = wandb.Table(data=perf_table_data, columns=[\"total_prompt_tokens\", \"total_completion_tokens\", \"total_tokens\", \"total_prompt_cost\", \"total_completion_cost\", \"total_cost\", \"total_prompt_time\", \"total_time\"])\n",
    "\n",
    "    outputs_save_path = os.path.join(output_path, f\"{run.name}.pkl\")\n",
    "    with open(outputs_save_path, 'wb') as f:\n",
    "        pickle.dump(outputs, f)\n",
    "\n",
    "    artifact = wandb.Artifact(name=run.name, type=\"output\")\n",
    "    artifact.add_file(local_path=outputs_save_path, name=\"outputs.pkl\")\n",
    "    artifact.save()\n",
    "\n",
    "    run.log({\n",
    "        f\"{run.name}_eval\": eval_table,\n",
    "        f\"{run.name}_perf\": perf_table\n",
    "    })\n",
    "\n",
    "    run.log({\n",
    "        \"total_em\": total_em,\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1,\n",
    "    })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
