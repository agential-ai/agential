{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m swebench.harness.run_evaluation \\\n",
    "    --dataset_name princeton-nlp/SWE-bench_Lite \\\n",
    "    --predictions_path <path_to_predictions> \\\n",
    "    --max_workers <num_workers> \\\n",
    "    --run_id <run_id>\n",
    "    # use --predictions_path 'gold' to verify the gold patches\n",
    "    # use --run_id to name the evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from harness.constants import (\n",
    "    SWEbenchInstance,\n",
    "    KEY_INSTANCE_ID,\n",
    ")\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "from datasets import Dataset, load_dataset\n",
    "from harness.constants import (\n",
    "    KEY_INSTANCE_ID,\n",
    "    KEY_MODEL,\n",
    "    KEY_PREDICTION\n",
    ")\n",
    "\n",
    "def load_swebench_dataset(name=\"princeton-nlp/SWE-bench\", split=\"test\", instance_ids=None) -> list[SWEbenchInstance]:\n",
    "    \"\"\"\n",
    "    Load SWE-bench dataset from Hugging Face Datasets or local .json/.jsonl file\n",
    "    \"\"\"\n",
    "    # check that all instance IDs are in the dataset\n",
    "    if instance_ids:\n",
    "        instance_ids = set(instance_ids)\n",
    "    # Load from local .json/.jsonl file\n",
    "    if name.endswith(\".json\") or name.endswith(\".jsonl\"):\n",
    "        dataset = json.loads(Path(name).read_text())\n",
    "        dataset_ids = {instance[KEY_INSTANCE_ID] for instance in dataset}\n",
    "    else:\n",
    "        # Load from Hugging Face Datasets\n",
    "        if name.lower() in {\"swe-bench\", \"swebench\", \"swe_bench\"}:\n",
    "            name = \"princeton-nlp/SWE-bench\"\n",
    "        elif name.lower() in {\"swe-bench-lite\", \"swebench-lite\", \"swe_bench_lite\", \"swe-bench_lite\", \"lite\"}:\n",
    "            name = \"princeton-nlp/SWE-bench_Lite\"\n",
    "        dataset = cast(Dataset, load_dataset(name, split=split))\n",
    "        dataset_ids = {instance[KEY_INSTANCE_ID] for instance in dataset}\n",
    "    if instance_ids:\n",
    "        if instance_ids - dataset_ids:\n",
    "            raise ValueError(\n",
    "                (\n",
    "                    \"Some instance IDs not found in dataset!\"\n",
    "                    f\"\\nMissing IDs:\\n{' '.join(instance_ids - dataset_ids)}\"\n",
    "                )\n",
    "            )\n",
    "        dataset = [instance for instance in dataset if instance[KEY_INSTANCE_ID] in instance_ids]\n",
    "    return [cast(SWEbenchInstance, instance) for instance in dataset]\n",
    "\n",
    "\n",
    "def get_gold_predictions(dataset_name: str, split: str):\n",
    "    \"\"\"\n",
    "    Get gold predictions for the given dataset and split.\n",
    "    \"\"\"\n",
    "    dataset = load_swebench_dataset(dataset_name, split)\n",
    "    return [\n",
    "        {\n",
    "            KEY_INSTANCE_ID: datum[KEY_INSTANCE_ID],\n",
    "            KEY_PREDICTION: datum[\"patch\"],\n",
    "            KEY_MODEL: \"gold\",\n",
    "        } for datum in dataset\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Run evaluation harness for the given dataset and predictions.\n",
    "    \"\"\"\n",
    "    # set open file limit\n",
    "    # assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "\n",
    "    # load predictions as map of instance_id to prediction\n",
    "\n",
    "    print(\"Using gold predictions - ignoring predictions_path\")\n",
    "    predictions = get_gold_predictions(\"princeton-nlp/SWE-bench_Lite\", \"test\")\n",
    "    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}\n",
    "    print(predictions)\n",
    "        # get dataset from predictions\n",
    "        # dataset = get_dataset_from_preds(dataset_name, split, instance_ids, predictions, run_id)\n",
    "        # full_dataset = load_swebench_dataset(dataset_name, split, instance_ids)\n",
    "        # existing_images = list_images(client)\n",
    "        # print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "        # if not dataset:\n",
    "        #     print(\"No instances to run.\")\n",
    "        # else:\n",
    "        #     # build environment images + run instances\n",
    "        #     build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "        #     run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)\n",
    "\n",
    "        # # clean images + make final report\n",
    "        # clean_images(client, existing_images, cache_level, clean)\n",
    "        # make_run_report(predictions, full_dataset, client, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
