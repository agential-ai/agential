{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.cog.prompts.critic import (\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_POT, \n",
    "    CRITIC_POT_INSTRUCTION_HUMANEVAL,\n",
    "    CRITIC_POT_INSTRUCTION_TEST_HUMANEVAL,\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_POT_TEST,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_HUMANEVAL,\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_CRITIC\n",
    ")\n",
    "from agential.cog.functional.critic import _prompt_agent, safe_execute, _prompt_critique\n",
    "\n",
    "import dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"from typing import List \n",
    "\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool: \n",
    "    \\\"\\\"\\\"Check if in given list of numbers, are any two numbers closer to each other than given threshold. \n",
    "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False \n",
    "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True\n",
    "    \\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "code = _prompt_agent(\n",
    "    llm=llm,\n",
    "    question=question,\n",
    "    examples=HUMANEVAL_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_HUMANEVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = _prompt_agent(\n",
    "    llm=llm,\n",
    "    question=question,\n",
    "    examples=HUMANEVAL_FEWSHOT_EXAMPLES_POT_TEST,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_TEST_HUMANEVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tests) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_answer, execution_status = safe_execute(\n",
    "    code + \"\\n\\n\" + tests #+ \"assert False\" + \"\\n\\n\"\n",
    ")  # Can be None, \"Exception\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"\n",
    "def are_anagrams(s1: str, s2: str) -> bool:\n",
    "    \\\"\\\"\\\"Check if two strings are anagrams of each other, ignoring case.\n",
    "    >>> are_anagrams('Listen', 'Silent')\n",
    "    True\n",
    "    >>> are_anagrams('Hello', 'World')\n",
    "    False\n",
    "    >>> are_anagrams('Angel', 'Glean')\n",
    "    True\n",
    "    \\\"\\\"\\\"\n",
    "    from collections import Counter\n",
    "    return Counter(s1) == Counter(s2)\n",
    "\n",
    "assert are_anagrams('Listen', 'Silent') == True, \"Test failed: are_anagrams('Listen', 'Silent') should return True\"\n",
    "assert are_anagrams('Hello', 'World') == False, \"Test failed: are_anagrams('Hello', 'World') should return False\"\n",
    "assert are_anagrams('Angel', 'Glean') == True, \"Test failed: are_anagrams('Angel', 'Glean') should return True\"\n",
    "\"\"\"\n",
    "\n",
    "safe_execute(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_additional_keys = {\n",
    "    \"execution_status\": execution_status,\n",
    "    \"code_answer\": code_answer,\n",
    "}\n",
    "\n",
    "critique = _prompt_critique(\n",
    "    llm=llm,\n",
    "    question=code,\n",
    "    examples=critique_examples,\n",
    "    answer=tests,\n",
    "    critique=\"\",\n",
    "    additional_keys=critique_additional_keys,\n",
    "    prompt=critique_prompt\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
