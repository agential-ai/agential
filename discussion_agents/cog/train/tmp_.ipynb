{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import dotenv\n",
    "import os\n",
    "import joblib\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from typing import Dict, List\n",
    "\n",
    "from discussion_agents.cog.functional.expel import (\n",
    "    # Experience Gathering.\n",
    "    gather_experience,\n",
    "    categorize_experiences,\n",
    "    get_folds,\n",
    "    # Insight Extraction.\n",
    "    _build_compare_prompt,\n",
    "    _prompt_compare_critique,\n",
    "    parse_rules,\n",
    "    retrieve_rule_index,\n",
    "    is_existing_rule,\n",
    "    remove_err_operations,\n",
    "    update_rules,\n",
    ")\n",
    "from discussion_agents.utils.general import random_divide_list\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_rules = 20\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)\n",
    "experiences = joblib.load(\"exp_15_compare_fake.joblib\")\n",
    "categories = categorize_experiences(experiences)\n",
    "folds = get_folds(categories, len(experiences['idxs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from discussion_agents.cog.functional.expel import _prompt_all_success_critique\n",
    "\n",
    "def create_rules(\n",
    "    llm: BaseChatModel,\n",
    "    experiences: Dict[str, List], \n",
    "    categories: Dict[str, int], \n",
    "    train_idxs: List[int], \n",
    "    rules: List[str], \n",
    "    rules_with_count: List[Tuple[str, int]],\n",
    "    max_num_rules: int,\n",
    "    success_critique_num: int = 8\n",
    ") -> Tuple[List[str], List[Tuple[str, int]]]:\n",
    "    # Intersect between train_idxs and each category (compare, success, fail).\n",
    "    train_category_idxs = {\n",
    "        category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "            for category, category_idxs in categories.items()\n",
    "    }\n",
    "\n",
    "    # Compare.\n",
    "    for train_idx in train_category_idxs[\"compare\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "\n",
    "        # Compare the successful trial with all previous failed trials.\n",
    "        success_trial = trajectory[-1][-1]\n",
    "        for failed_trial in trajectory[:-1]:\n",
    "            # Prompt.\n",
    "            out = _prompt_compare_critique(\n",
    "                rules, \n",
    "                question, \n",
    "                success_trial, \n",
    "                failed_trial, \n",
    "                max_num_rules < len(rules_with_count),\n",
    "                llm\n",
    "            )\n",
    "\n",
    "            # Parse.\n",
    "            operations = parse_rules(out)\n",
    "            \n",
    "            # Remove no-ops.\n",
    "            operations = remove_err_operations(rules_with_count, operations)\n",
    "\n",
    "            # Update rules_with_count and rules with comparison insights.\n",
    "            rules_with_count = update_rules(rules_with_count, operations, is_full=max_num_rules+5 <= len(rules_with_count))\n",
    "            rules = [rule[0] for rule in rules_with_count]\n",
    "\n",
    "    # Success.\n",
    "    batched_success_trajs_idxs = random_divide_list(train_category_idxs['success'], success_critique_num)\n",
    "    for success_idxs in batched_success_trajs_idxs:\n",
    "        # Concatenate batched successful trajectories.\n",
    "        concat_success_trajs = [\n",
    "            f\"{experiences['questions'][idx]}\\n{experiences['trajectories'][idx][0][-1]}\"  # Get this successful trajectory's zero-th trial output.\n",
    "            for idx in success_idxs\n",
    "        ]\n",
    "        success_trajs_str = \"\\n\\n\".join(concat_success_trajs)\n",
    "\n",
    "        # Prompt.\n",
    "        out = _prompt_all_success_critique(\n",
    "            rules, \n",
    "            success_trajs_str, \n",
    "            max_num_rules < len(rules_with_count), \n",
    "            llm\n",
    "        )\n",
    "\n",
    "        # Parse.\n",
    "        operations = parse_rules(out)\n",
    "\n",
    "        # Remove no-ops.\n",
    "        operations = remove_err_operations(rules_with_count, operations)\n",
    "\n",
    "        # Update rules_with_count and rules with success insights.\n",
    "        rules_with_count = update_rules(rules_with_count, operations, is_full=max_num_rules+5 <= len(rules_with_count))\n",
    "        rules = [rule[0] for rule in rules_with_count]\n",
    "\n",
    "    return rules, rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 6, 11, 14]\n"
     ]
    }
   ],
   "source": [
    "rules, rules_with_count = [], []\n",
    "for fold, train_idxs in folds.items():\n",
    "    print(fold, train_idxs)\n",
    "    rules, rules_with_count = create_rules(llm, experiences, categories, train_idxs, rules, rules_with_count, max_num_rules)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_idxs = {\n",
    "    category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "        for category, category_idxs in categories.items()\n",
    "}\n",
    "train_category_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.utils.general import random_divide_list\n",
    "\n",
    "success_critique_num = 8\n",
    "batched_success_trajs_idxs = random_divide_list(train_category_idxs['success'], success_critique_num)\n",
    "batched_success_trajs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules, rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for success_idxs in batched_success_trajs_idxs:\n",
    "    print(success_idxs)\n",
    "\n",
    "    concat_success_trajs = [\n",
    "        f\"{experiences['questions'][idx]}\\n{experiences['trajectories'][idx][0][-1]}\"  # Get this successful trajectory's zero-th trial output.\n",
    "        for idx in success_idxs\n",
    "    ]\n",
    "    success_trajs_str = \"\\n\\n\".join(concat_success_trajs)\n",
    "    # self.rule_items, llm_output = extend_rules(self.rule_items, success_trials.strip(), None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.cog.prompts.expel import (\n",
    "    SYSTEM_TEMPLATE,\n",
    "    SYSTEM_CRITIQUE_ALL_SUCCESS_EXISTING_RULES_INSTRUCTION,\n",
    "    NON_EXISTENT_RULES_AT_NAME,\n",
    "    EXISTING_RULES_AI_NAME,\n",
    ")\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "critique_history = []\n",
    "\n",
    "# System prompt.\n",
    "prefix = (\n",
    "    HumanMessagePromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "    .format_messages(\n",
    "        ai_name=NON_EXISTENT_RULES_AT_NAME if not rules else EXISTING_RULES_AI_NAME,\n",
    "        instruction=SYSTEM_CRITIQUE_ALL_SUCCESS_EXISTING_RULES_INSTRUCTION\n",
    "    )\n",
    ")\n",
    "critique_history.extend(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_format_dict = {\n",
    "    \"success_trajs\": success_trajs_str,\n",
    "    \"existing_rules\": '\\n'.join([f'{i}. {r}' for i, r in enumerate(rules, 1)])\n",
    "}\n",
    "human_format_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.cog.prompts.expel import FORMAT_RULES_OPERATION_TEMPLATE\n",
    "HUMAN_CRITIQUE_EXISTING_RULES_ALL_SUCCESS_TEMPLATE = \"\"\"\n",
    "Here are the trials:\n",
    "{success_trajs}\n",
    "\n",
    "Here are the EXISTING RULES:\n",
    "{existing_rules}\n",
    "\n",
    "By examining the successful trials, and the list of existing rules, you can perform the following operations: add, edit, remove, or agree so that the new list of rules are general and high level insights of the successful trials or proposed way of Thought so they can be used as helpful tips to different tasks in the future. Have an emphasis on tips that help the agent perform better Thought and Action. Follow the below format:\n",
    "\n",
    "\"\"\" + FORMAT_RULES_OPERATION_TEMPLATE\n",
    "\n",
    "human_critique_summary_message = HumanMessagePromptTemplate.from_template(HUMAN_CRITIQUE_EXISTING_RULES_ALL_SUCCESS_TEMPLATE).format_messages(**human_format_dict)[0]\n",
    "human_critique_summary_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx in train_category_idxs[\"success\"]:\n",
    "    question = experiences[\"questions\"][train_idx]\n",
    "    trajectory = experiences[\"trajectories\"][train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessage\n",
    "from discussion_agents.cog.prompts.expel import (\n",
    "    CRITIQUE_SUMMARY_SUFFIX_FULL,\n",
    "    CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    ")\n",
    "\n",
    "def _build_all_success_prompt(\n",
    "    rules: List[str], \n",
    "    success_trajs_str: str,\n",
    "    is_full: bool,\n",
    ") -> List[HumanMessage]:\n",
    "    # is_full = self.max_num_rules <= len(self.rules_with_count)   ->    20 <= len(self.rules_with_count)\n",
    "\n",
    "    critique_history = []\n",
    "\n",
    "    if rules == []:\n",
    "        rules = ['']\n",
    "\n",
    "    # System prompt.\n",
    "    prefix = (\n",
    "        HumanMessagePromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "        .format_messages(\n",
    "            ai_name=NON_EXISTENT_RULES_AT_NAME if not rules else EXISTING_RULES_AI_NAME,\n",
    "            instruction=SYSTEM_CRITIQUE_ALL_SUCCESS_EXISTING_RULES_INSTRUCTION\n",
    "        )\n",
    "    )\n",
    "    critique_history.extend(prefix)\n",
    "\n",
    "    # Task prompt.\n",
    "    human_format_dict = {\n",
    "        \"success_trajs\": success_trajs_str,\n",
    "        \"existing_rules\": '\\n'.join([f'{i}. {r}' for i, r in enumerate(rules, 1)])\n",
    "    }\n",
    "\n",
    "    human_critique_summary_message = HumanMessagePromptTemplate.from_template(HUMAN_CRITIQUE_EXISTING_RULES_ALL_SUCCESS_TEMPLATE).format_messages(**human_format_dict)[0]\n",
    "    critique_summary_suffix = CRITIQUE_SUMMARY_SUFFIX_FULL if is_full else CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    "    human_critique_summary_message.content = human_critique_summary_message.content + critique_summary_suffix\n",
    "    critique_history.append(human_critique_summary_message)\n",
    "\n",
    "    return critique_history\n",
    "\n",
    "def _prompt_all_success_critique(\n",
    "    rules: List[str], \n",
    "    success_trajs_str: str, \n",
    "    is_full: bool,\n",
    "    llm: BaseChatModel, \n",
    "    replace_newline: bool = False\n",
    ") -> str:\n",
    "    compare_prompt_msgs = _build_all_success_prompt(\n",
    "        rules=rules,\n",
    "        success_trajs_str=success_trajs_str,\n",
    "        is_full=is_full\n",
    "    )\n",
    "    compare_prompt_msgs = collapse_prompts(compare_prompt_msgs)\n",
    "    out = llm(compare_prompt_msgs).content.strip('\\n').strip()\n",
    "    if replace_newline:\n",
    "        out = out.replace('\\n', '')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = _prompt_all_success_critique(rules, success_trajs_str, max_num_rules < len(rules_with_count), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_critique_num = 8\n",
    "\n",
    "\n",
    "all_success = random_divide_list(all_success, self.success_critique_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
