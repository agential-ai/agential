{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.cog.agent.reflexion import ReflexionReActAgent\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "hotpot = joblib.load('../agent/hotpot-qa-distractor-sample.joblib').reset_index(drop=True)\n",
    "hotpot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_experience(\n",
    "    reflexion_react_agent: ReflexionReActAgent,\n",
    "    questions: List[str],\n",
    "    keys: List[str],\n",
    "    strategy: Optional[str] = \"reflexion\",\n",
    ") -> Dict[str, List]:\n",
    "    experiences = {\n",
    "        \"idxs\": [],\n",
    "        \"questions\": [],\n",
    "        \"keys\": [],\n",
    "        \"trajectories\": [],\n",
    "        \"reflections\": []\n",
    "    }\n",
    "    for idx, (question, key) in enumerate(zip(questions, keys)):\n",
    "        trajectory = reflexion_react_agent.generate(\n",
    "            question=question, key=key, strategy=strategy, reset=True\n",
    "        )\n",
    "\n",
    "        experiences[\"idxs\"].append(idx)\n",
    "        experiences[\"questions\"].append(question)\n",
    "        experiences[\"keys\"].append(key)\n",
    "        experiences[\"trajectories\"].append(trajectory)\n",
    "        experiences[\"reflections\"].append(reflexion_react_agent.reflector.reflections)\n",
    "        \n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    self_reflect_llm=llm,\n",
    "    action_llm=llm,\n",
    "    max_steps=7,\n",
    "    max_trials=3,\n",
    ")\n",
    "\n",
    "# experiences_tmp = gather_experience(agent, questions=hotpot.question.values.tolist()[10:10+k], keys=hotpot.answer.values.tolist()[10:10+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(experiences, \"experiences_10.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "experiences = joblib.load(\"exp_15_compare_fake.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compare': [10, 11, 12, 13, 14],\n",
       " 'success': [1, 3, 6, 7, 8],\n",
       " 'fail': [0, 2, 4, 5, 9]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def categorize_experiences(experiences: Dict[str, List]) -> Dict[str, List]:\n",
    "    count_dict = {\n",
    "        \"compare\": [],\n",
    "        \"success\": [],\n",
    "        \"fail\": []\n",
    "    }\n",
    "\n",
    "    for idx in experiences[\"idxs\"]:  # Index for a particular task.\n",
    "        trajectory = experiences[\"trajectories\"][idx]\n",
    "        trials_are_correct = [trial[0] for trial in trajectory]  # (is_correct, answer, output)[0]\n",
    "\n",
    "        # Success.\n",
    "        if all(trials_are_correct) and len(trials_are_correct) == 1:  # If success @ first trial, then stop generation.\n",
    "            count_dict[\"success\"].append(idx)\n",
    "        # Compare.\n",
    "        elif trials_are_correct[-1]:  # If fail(s), then succeeds, then only last trial is True.\n",
    "            count_dict[\"compare\"].append(idx)\n",
    "        # Fail.\n",
    "        elif not all(trials_are_correct):  # All trials failed, then fail case.\n",
    "            count_dict[\"fail\"].append(idx)\n",
    "        else:\n",
    "            raise ValueError(f\"Unhandled scenario for trajectory at index {idx}.\")\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "categories = categorize_experiences(experiences)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 4, 5, 8, 11, 13], 1: [0, 2, 3, 6, 7, 9, 10, 12, 14]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_folds(categories: Dict[str, List], n_instances: int, n_folds: int = 2) -> Dict[str, List]:\n",
    "    folds = {fold: [] for fold in range(n_folds)}\n",
    "\n",
    "    # Assign labels for 'compare', 'success', and  'fail'.\n",
    "    for _, indices in categories.items():\n",
    "        random.shuffle(indices)\n",
    "        for count, idx in enumerate(indices):\n",
    "            folds[count % n_folds].append(idx)\n",
    "\n",
    "    # Each fold is a validation set. Take the difference to get the training set of each fold.\n",
    "    folds = {fold: list(set(list(range(n_instances))).difference(values)) for fold, values in folds.items()}\n",
    "\n",
    "    return folds\n",
    "\n",
    "folds = get_folds(categories, 15)\n",
    "folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = folds[0]\n",
    "\n",
    "train_category_idxs = {\n",
    "    category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "        for category, category_idxs in categories.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "from discussion_agents.cog.prompts.expel import (\n",
    "    SYSTEM_TEMPLATE, \n",
    "    SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION,\n",
    "    EXISTING_RULES_AI_NAME,\n",
    "    NON_EXISTENT_RULES_AT_NAME,\n",
    "    HUMAN_CRITIQUE_EXISTING_RULES_TEMPLATE,\n",
    "    CRITIQUE_SUMMARY_SUFFIX_FULL,\n",
    "    CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    ")\n",
    "\n",
    "def _build_compare_prompt(\n",
    "    rules: List[str], \n",
    "    question: str,\n",
    "    success_trial: str, \n",
    "    failed_trial: str, \n",
    "    is_full: bool,\n",
    ") -> List[HumanMessage]:\n",
    "    # is_full = self.max_num_rules <= len(self.rules_with_count)   ->    20 <= len(self.rules_with_count)\n",
    "\n",
    "    critique_history = []\n",
    "\n",
    "    if rules == []:\n",
    "        rules = ['']\n",
    "\n",
    "    # System prompt.\n",
    "    prefix = (\n",
    "        HumanMessagePromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "        .format_messages(\n",
    "            ai_name=NON_EXISTENT_RULES_AT_NAME if not rules else EXISTING_RULES_AI_NAME,\n",
    "            instruction=SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION\n",
    "        )\n",
    "    )\n",
    "    critique_history.extend(prefix)\n",
    "\n",
    "    # Task prompt.\n",
    "    human_format_dict = {\n",
    "        'question': question,\n",
    "        'failed_traj': failed_trial,\n",
    "        'success_traj': success_trial,\n",
    "        'existing_rules': '\\n'.join([f'{i}. {r}' for i, r in enumerate(rules, 1)])\n",
    "    }\n",
    "\n",
    "    human_critique_summary_message = HumanMessagePromptTemplate.from_template(HUMAN_CRITIQUE_EXISTING_RULES_TEMPLATE).format_messages(**human_format_dict)[0]\n",
    "    critique_summary_suffix = CRITIQUE_SUMMARY_SUFFIX_FULL if is_full else CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    "    human_critique_summary_message.content = human_critique_summary_message.content + critique_summary_suffix\n",
    "    critique_history.append(human_critique_summary_message)\n",
    "\n",
    "    return critique_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_traj = experiences['trajectories'][11][0][-1]\n",
    "success_traj = experiences['trajectories'][11][-1][-1]\n",
    "question = experiences['questions'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_rules = 20\n",
    "rules = []\n",
    "rules_with_count = []\n",
    "is_full = max_num_rules < len(rules_with_count)\n",
    "compare_prompt_msgs = _build_compare_prompt(rules, question, failed_traj, success_traj, is_full=is_full)\n",
    "compare_prompt_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.chat import ChatMessage\n",
    "\n",
    "def collapse_prompts(prompt_history: List[ChatMessage]) -> List[ChatMessage]:\n",
    "    \"\"\"Courtesy of GPT4\"\"\"\n",
    "    if not prompt_history:\n",
    "        return []\n",
    "\n",
    "    new_prompt_history = []\n",
    "    scratch_pad = prompt_history[0].content\n",
    "    last_message_type = type(prompt_history[0])\n",
    "\n",
    "    for message in prompt_history[1:]:\n",
    "        current_message_type = type(message)\n",
    "        if current_message_type == last_message_type:\n",
    "            scratch_pad += '\\n' + message.content\n",
    "        else:\n",
    "            new_prompt_history.append(last_message_type(content=scratch_pad))\n",
    "            scratch_pad = message.content\n",
    "            last_message_type = current_message_type\n",
    "\n",
    "    # Handle the last accumulated message.\n",
    "    new_prompt_history.append(last_message_type(content=scratch_pad))\n",
    "\n",
    "    return new_prompt_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_prompt_msgs = collapse_prompts(compare_prompt_msgs)\n",
    "compare_prompt_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "def _prompt_compare_critique(compare_prompt_msgs: List[HumanMessage], llm: BaseChatModel, replace_newline: bool = False):\n",
    "    out = llm(compare_prompt_msgs).content.strip('\\n').strip()\n",
    "    if replace_newline:\n",
    "        out = out.replace('\\n', '')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _prompt_compare_critique(compare_prompt_msgs, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_rules(llm_text):\n",
    "    pattern = r'((?:REMOVE|EDIT|ADD|AGREE)(?: \\d+|)): (?:[a-zA-Z\\s\\d]+: |)(.*)'\n",
    "    matches = re.findall(pattern, llm_text)\n",
    "\n",
    "    res = []\n",
    "    banned_words = ['ADD', 'AGREE', 'EDIT']\n",
    "    for operation, text in matches:\n",
    "        text = text.strip()\n",
    "        if text != '' and not any([w in text for w in banned_words]) and text.endswith('.'):\n",
    "        # if text is not empty\n",
    "        # if text doesn't contain banned words (avoid weird formatting cases from llm)\n",
    "        # if text ends with a period (avoid cut off sentences from llm)\n",
    "            if 'ADD' in operation:\n",
    "                res.append(('ADD', text))\n",
    "            else:\n",
    "                res.append((operation.strip(), text))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = parse_rules(out)\n",
    "operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def retrieve_rule_index(rules: List[Tuple[str, int]], operation_rule_text: str) -> int:\n",
    "    for i in range(len(rules)):\n",
    "        if rules[i][0] in operation_rule_text:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def is_existing_rule(rules: List[Tuple[str, int]], operation_rule_text: str) -> bool:\n",
    "    for i in range(len(rules)):\n",
    "        if rules[i][0] in operation_rule_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_err_operations(rules: List[Tuple[str, int]], operations: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    cleaned_operations = operations.copy()\n",
    "    \n",
    "    delete_indices = []\n",
    "    for i in range(len(cleaned_operations)):\n",
    "        # Split the operation into action type and optional rule number.\n",
    "        operation, operation_rule_text = cleaned_operations[i]\n",
    "        operation_type = operation.split(' ')[0]\n",
    "        rule_num = int(operation.split(' ')[1]) if ' ' in operation else None\n",
    "\n",
    "        if operation_type == 'ADD':\n",
    "            if is_existing_rule(rules, operation_rule_text): # If new rule_text is an existing rule ('in').\n",
    "                delete_indices.append(i)\n",
    "        else:\n",
    "            if operation_type == 'EDIT':\n",
    "                if is_existing_rule(rules, operation_rule_text): # If rule is matching ('in') existing rule, change it to AGREE.\n",
    "                    rule_num = retrieve_rule_index(rules, operation_rule_text)\n",
    "                    cleaned_operations[i] = (f'AGREE {rule_num+1}', rules[rule_num][0])\n",
    "                elif (rule_num is None) or (rule_num > len(rules)):   # If rule doesn't exist, remove.\n",
    "                    delete_indices.append(i)\n",
    "                    \n",
    "            elif operation_type == 'REMOVE' or operation_type == 'AGREE':\n",
    "                if not is_existing_rule(rules, operation_rule_text): # If new operation_rule_text is not an existing rule.\n",
    "                    delete_indices.append(i)\n",
    "\n",
    "    # Remove problematic operations.\n",
    "    cleaned_operations = [cleaned_operations[i] for i in range(len(cleaned_operations)) if i not in delete_indices]\n",
    "    \n",
    "    return cleaned_operations\n",
    "\n",
    "def update_rules(rules: List[Tuple[str, int]], operations: List[Tuple[str, str]], is_full: bool = False) -> List[Tuple[str, int]]:\n",
    "    updated_rules = rules.copy()\n",
    "    \n",
    "    for op in ['REMOVE', 'AGREE', 'EDIT', 'ADD']: # Order is important\n",
    "        for i in range(len(operations)):\n",
    "            operation, operation_rule_text = operations[i]\n",
    "            operation_type = operation.split(' ')[0]\n",
    "            if operation_type != op:\n",
    "                continue\n",
    "\n",
    "            if operation_type == 'REMOVE': # remove rule: -1\n",
    "                rule_index = retrieve_rule_index(updated_rules, operation_rule_text) # if rule_num doesn't match but text does\n",
    "                remove_strength = 3 if is_full else 1\n",
    "                updated_rules[rule_index] = (updated_rules[rule_index][0], updated_rules[rule_index][1]-remove_strength) # -1 (-3 if list full) to the counter\n",
    "            elif operation_type == 'AGREE': # agree with rule: +1\n",
    "                rule_index = retrieve_rule_index(updated_rules, operation_rule_text) # if rule_num doesn't match but text does\n",
    "                updated_rules[rule_index] = (updated_rules[rule_index][0], updated_rules[rule_index][1]+1) # +1 to the counter\n",
    "            elif operation_type == 'EDIT': # edit the rule: +1 // NEED TO BE AFTER REMOVE AND AGREE\n",
    "                rule_index = int(operation.split(' ')[1])-1\n",
    "                updated_rules[rule_index] = (operation_rule_text, updated_rules[rule_index][1]+1) # +1 to the counter\n",
    "            elif operation_type == 'ADD': # add new rule: +2\n",
    "                updated_rules.append((operation_rule_text, 2))\n",
    "    updated_rules = [updated_rules[i] for i in range(len(updated_rules)) if updated_rules[i][1] > 0] # remove rules when counter reach 0\n",
    "    updated_rules.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return updated_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_full = max_num_rules+5 <= len(rules_with_count)\n",
    "\n",
    "# Remove problematic operations.\n",
    "operations = remove_err_operations(rules_with_count, operations)\n",
    "rules_with_count = update_rules(rules_with_count, operations, is_full=is_full)\n",
    "rules = [rule[0] for rule in rules_with_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def create_rules(\n",
    "    llm: BaseChatModel,\n",
    "    experiences: Dict[str, List], \n",
    "    categories: Dict[str, int], \n",
    "    train_idxs: List[int], \n",
    "    rules: List[str], \n",
    "    rules_with_count: List[Tuple[str, int]],\n",
    "    max_num_rules: int,\n",
    "):\n",
    "    # Intersect between train_idxs and each category (compare, success, fail).\n",
    "    train_category_idxs = {\n",
    "        category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "            for category, category_idxs in categories.items()\n",
    "    }\n",
    "\n",
    "    # Compare.\n",
    "    for train_idx in train_category_idxs[\"compare\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "        success_trial = trajectory[-1][-1]\n",
    "        for failed_trial in trajectory[:-1]:\n",
    "            compare_prompt_msgs = _build_compare_prompt(rules, question, success_trial, failed_trial, is_full=max_num_rules < len(rules_with_count))\n",
    "            compare_prompt_msgs = collapse_prompts(compare_prompt_msgs)\n",
    "            out = _prompt_compare_critique(compare_prompt_msgs, llm)\n",
    "            operations = parse_rules(out)\n",
    "            operations = remove_err_operations(rules_with_count, operations)\n",
    "            rules_with_count = update_rules(rules_with_count, operations, is_full=max_num_rules+5 <= len(rules_with_count))\n",
    "            rules = [rule[0] for rule in rules_with_count]\n",
    "\n",
    "    # # Success.\n",
    "    # for train_idx in train_category_idxs[\"success\"]:\n",
    "    #     question = experiences[\"questions\"][train_idx]\n",
    "    #     key = experiences[\"keys\"][train_idx]\n",
    "    #     trajectory = experiences[\"trajectories\"][train_idx]\n",
    "    #     reflection = experiences[\"reflections\"][train_idx]\n",
    "    \n",
    "    # # Fail.\n",
    "    # for train_idx in train_category_idxs[\"fail\"]:\n",
    "    #     question = experiences[\"questions\"][train_idx]\n",
    "    #     key = experiences[\"keys\"][train_idx]\n",
    "    #     trajectory = experiences[\"trajectories\"][train_idx]\n",
    "    #     reflection = experiences[\"reflections\"][train_idx]\n",
    "\n",
    "    return rules, rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 2, 6, 8, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "max_num_rules = 20\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)\n",
    "experiences = joblib.load(\"exp_15_compare_fake.joblib\")\n",
    "categories = categorize_experiences(experiences)\n",
    "folds = get_folds(categories, len(experiences['idxs']))\n",
    "\n",
    "rules, rules_with_count = [], []\n",
    "for fold, train_idxs in folds.items():\n",
    "    print(fold, train_idxs)\n",
    "    rules, rules_with_count = create_rules(llm, experiences, categories, train_idxs, rules, rules_with_count, max_num_rules)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, eval_idxs in enumerate(eval_idx_list):\n",
    "    if k < starting_fold:\n",
    "        continue\n",
    "    training_ids = set(range(num_training_tasks)) - set(eval_idxs)\n",
    "    (SAVE_PATH / f\"fold_{k}\").mkdir(exist_ok=True)\n",
    "    log += f'################## FOLD {k} ##################\\n'\n",
    "    log += react_agent.create_rules(\n",
    "        list(training_ids),\n",
    "        cache_fold=k,\n",
    "        logging_dir=str(SAVE_PATH / f\"fold_{k}\"),\n",
    "        run_name=cfg.run_name,\n",
    "        loaded_dict=dicts[-1] if resume and resume_starting_fold == starting_fold else None,\n",
    "        loaded_log=critique_summary_log if resume and resume_starting_fold == starting_fold else None,\n",
    "        eval_idx_list=eval_idx_list,\n",
    "        saving_dict=True,\n",
    "    )\n",
    "    starting_fold += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
