{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.cog.agent.reflexion import ReflexionReActAgent\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>supporting_facts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a7613c15542994ccc9186bf</td>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['VIVA Media', 'Gesellschaft mit bes...</td>\n",
       "      <td>{'title': ['Constantin Medien', 'VIVA Poland',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf2fa35542993344016c11</td>\n",
       "      <td>Which of Jonny Craig and Pete Doherty has been...</td>\n",
       "      <td>Jonny\" Craig</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Jonny Craig', 'Jonny Craig', 'Pete...</td>\n",
       "      <td>{'title': ['Pete Doherty', 'Relativity (Emaros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adfdef9554299025d62a36b</td>\n",
       "      <td>Where was the first governor after the The Mis...</td>\n",
       "      <td>Bath, Maine</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Maine gubernatorial election, 1820...</td>\n",
       "      <td>{'title': ['Compromise of 1790', 'Anti-Nebrask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a7180205542994082a3e856</td>\n",
       "      <td>The creator of \"Wallace and Gromit\" also creat...</td>\n",
       "      <td>Creature Comforts</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Creature Comforts', 'Creature Comf...</td>\n",
       "      <td>{'title': ['Creature Comforts', 'Tata Steel Zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a78bc6b554299148911f979</td>\n",
       "      <td>Woman's Era and Naj are what kind of magazines?</td>\n",
       "      <td>fortnightly women interest magazine</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Woman's Era', 'Naj'], 'sent_id': [...</td>\n",
       "      <td>{'title': ['Lifestyle trends and media', 'Chin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5a7613c15542994ccc9186bf   \n",
       "1  5adf2fa35542993344016c11   \n",
       "2  5adfdef9554299025d62a36b   \n",
       "3  5a7180205542994082a3e856   \n",
       "4  5a78bc6b554299148911f979   \n",
       "\n",
       "                                            question  \\\n",
       "0  VIVA Media AG changed it's name in 2004. What ...   \n",
       "1  Which of Jonny Craig and Pete Doherty has been...   \n",
       "2  Where was the first governor after the The Mis...   \n",
       "3  The creator of \"Wallace and Gromit\" also creat...   \n",
       "4    Woman's Era and Naj are what kind of magazines?   \n",
       "\n",
       "                                  answer        type level  \\\n",
       "0  Gesellschaft mit beschränkter Haftung      bridge  hard   \n",
       "1                           Jonny\" Craig  comparison  hard   \n",
       "2                            Bath, Maine      bridge  hard   \n",
       "3                      Creature Comforts      bridge  hard   \n",
       "4    fortnightly women interest magazine  comparison  hard   \n",
       "\n",
       "                                    supporting_facts  \\\n",
       "0  {'title': ['VIVA Media', 'Gesellschaft mit bes...   \n",
       "1  {'title': ['Jonny Craig', 'Jonny Craig', 'Pete...   \n",
       "2  {'title': ['Maine gubernatorial election, 1820...   \n",
       "3  {'title': ['Creature Comforts', 'Creature Comf...   \n",
       "4  {'title': ['Woman's Era', 'Naj'], 'sent_id': [...   \n",
       "\n",
       "                                             context  \n",
       "0  {'title': ['Constantin Medien', 'VIVA Poland',...  \n",
       "1  {'title': ['Pete Doherty', 'Relativity (Emaros...  \n",
       "2  {'title': ['Compromise of 1790', 'Anti-Nebrask...  \n",
       "3  {'title': ['Creature Comforts', 'Tata Steel Zo...  \n",
       "4  {'title': ['Lifestyle trends and media', 'Chin...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "hotpot = joblib.load('../agent/hotpot-qa-distractor-sample.joblib').reset_index(drop=True)\n",
    "hotpot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_experience(\n",
    "    reflexion_react_agent: ReflexionReActAgent,\n",
    "    questions: List[str],\n",
    "    keys: List[str],\n",
    "    strategy: Optional[str] = \"reflexion\",\n",
    ") -> Dict[str, List]:\n",
    "    experiences = {\n",
    "        \"idxs\": [],\n",
    "        \"questions\": [],\n",
    "        \"keys\": [],\n",
    "        \"trajectories\": [],\n",
    "        \"reflections\": []\n",
    "    }\n",
    "    for idx, (question, key) in enumerate(zip(questions, keys)):\n",
    "        trajectory = reflexion_react_agent.generate(\n",
    "            question=question, key=key, strategy=strategy, reset=True\n",
    "        )\n",
    "\n",
    "        experiences[\"idxs\"].append(idx)\n",
    "        experiences[\"questions\"].append(question)\n",
    "        experiences[\"keys\"].append(key)\n",
    "        experiences[\"trajectories\"].append(trajectory)\n",
    "        experiences[\"reflections\"].append(reflexion_react_agent.reflector.reflections)\n",
    "        \n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    self_reflect_llm=llm,\n",
    "    action_llm=llm,\n",
    "    max_steps=7,\n",
    "    max_trials=3,\n",
    ")\n",
    "\n",
    "# experiences_tmp = gather_experience(agent, questions=hotpot.question.values.tolist()[10:10+k], keys=hotpot.answer.values.tolist()[10:10+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(experiences, \"experiences_10.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "experiences = joblib.load(\"exp_15_compare_fake.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled scenario for trajectory at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count_dict\n\u001b[1;32m---> 26\u001b[0m categories \u001b[38;5;241m=\u001b[39m categorize_experiences(\u001b[43mexperiences\u001b[49m)\n\u001b[0;32m     27\u001b[0m categories\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def categorize_experiences(experiences: Dict[str, List]) -> Dict[str, List]:\n",
    "    count_dict = {\n",
    "        \"compare\": [],\n",
    "        \"success\": [],\n",
    "        \"fail\": []\n",
    "    }\n",
    "\n",
    "    for idx in experiences[\"idxs\"]:  # Index for a particular task.\n",
    "        trajectory = experiences[\"trajectories\"][idx]\n",
    "        trials_are_correct = [trial[0] for trial in trajectory]  # (is_correct, answer, output)[0]\n",
    "\n",
    "        # Success.\n",
    "        if all(trials_are_correct) and len(trials_are_correct) == 1:  # If success @ first trial, then stop generation.\n",
    "            count_dict[\"success\"].append(idx)\n",
    "        # Compare.\n",
    "        elif trials_are_correct[-1]:  # If fail(s), then succeeds, then only last trial is True.\n",
    "            count_dict[\"compare\"].append(idx)\n",
    "        # Fail.\n",
    "        elif not all(trials_are_correct):  # All trials failed, then fail case.\n",
    "            count_dict[\"fail\"].append(idx)\n",
    "        else:\n",
    "            raise ValueError(f\"Unhandled scenario for trajectory at index {idx}.\")\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "categories = categorize_experiences(experiences)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 4, 5, 8, 11, 13], 1: [0, 2, 3, 6, 7, 9, 10, 12, 14]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_folds(categories: Dict[str, List], n_instances: int, n_folds: int = 2) -> Dict[str, List]:\n",
    "    folds = {fold: [] for fold in range(n_folds)}\n",
    "\n",
    "    # Assign labels for 'compare', 'success', and  'fail'.\n",
    "    for _, indices in categories.items():\n",
    "        random.shuffle(indices)\n",
    "        for count, idx in enumerate(indices):\n",
    "            folds[count % n_folds].append(idx)\n",
    "\n",
    "    # Each fold is a validation set. Take the difference to get the training set of each fold.\n",
    "    folds = {fold: list(set(list(range(n_instances))).difference(values)) for fold, values in folds.items()}\n",
    "\n",
    "    return folds\n",
    "\n",
    "folds = get_folds(categories, 15)\n",
    "folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = folds[0]\n",
    "\n",
    "train_category_idxs = {\n",
    "    category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "        for category, category_idxs in categories.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "from discussion_agents.cog.prompts.expel import (\n",
    "    SYSTEM_TEMPLATE, \n",
    "    SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION,\n",
    "    EXISTING_RULES_AI_NAME,\n",
    "    NON_EXISTENT_RULES_AT_NAME,\n",
    "    HUMAN_CRITIQUE_EXISTING_RULES_TEMPLATE,\n",
    "    CRITIQUE_SUMMARY_SUFFIX_FULL,\n",
    "    CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    ")\n",
    "\n",
    "def _build_compare_prompt(\n",
    "    rules: List[str], \n",
    "    question: str,\n",
    "    success_trial: str, \n",
    "    failed_trial: str, \n",
    "    is_full: bool,\n",
    ") -> List[HumanMessage]:\n",
    "    # is_full = self.max_num_rules <= len(self.rules_with_count)   ->    20 <= len(self.rules_with_count)\n",
    "\n",
    "    critique_history = []\n",
    "\n",
    "    if rules == []:\n",
    "        rules = ['']\n",
    "\n",
    "    # System prompt.\n",
    "    prefix = (\n",
    "        HumanMessagePromptTemplate.from_template(SYSTEM_TEMPLATE)\n",
    "        .format_messages(\n",
    "            ai_name=NON_EXISTENT_RULES_AT_NAME if not rules else EXISTING_RULES_AI_NAME,\n",
    "            instruction=SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION\n",
    "        )\n",
    "    )\n",
    "    critique_history.extend(prefix)\n",
    "\n",
    "    # Task prompt.\n",
    "    human_format_dict = {\n",
    "        'question': question,\n",
    "        'failed_traj': failed_trial,\n",
    "        'success_traj': success_trial,\n",
    "        'existing_rules': '\\n'.join([f'{i}. {r}' for i, r in enumerate(rules, 1)])\n",
    "    }\n",
    "\n",
    "    human_critique_summary_message = HumanMessagePromptTemplate.from_template(HUMAN_CRITIQUE_EXISTING_RULES_TEMPLATE).format_messages(**human_format_dict)[0]\n",
    "    critique_summary_suffix = CRITIQUE_SUMMARY_SUFFIX_FULL if is_full else CRITIQUE_SUMMARY_SUFFIX_NOT_FULL\n",
    "    human_critique_summary_message.content = human_critique_summary_message.content + critique_summary_suffix\n",
    "    critique_history.append(human_critique_summary_message)\n",
    "\n",
    "    return critique_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_traj = experiences['trajectories'][11][0][-1]\n",
    "success_traj = experiences['trajectories'][11][-1][-1]\n",
    "question = experiences['questions'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_rules = 20\n",
    "rules = []\n",
    "rules_with_count = []\n",
    "is_full = max_num_rules < len(rules_with_count)\n",
    "compare_prompt_msgs = _build_compare_prompt(rules, question, failed_traj, success_traj, is_full=is_full)\n",
    "compare_prompt_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.chat import ChatMessage\n",
    "\n",
    "def collapse_prompts(prompt_history: List[ChatMessage]) -> List[ChatMessage]:\n",
    "    \"\"\"Courtesy of GPT4\"\"\"\n",
    "    if not prompt_history:\n",
    "        return []\n",
    "\n",
    "    new_prompt_history = []\n",
    "    scratch_pad = prompt_history[0].content\n",
    "    last_message_type = type(prompt_history[0])\n",
    "\n",
    "    for message in prompt_history[1:]:\n",
    "        current_message_type = type(message)\n",
    "        if current_message_type == last_message_type:\n",
    "            scratch_pad += '\\n' + message.content\n",
    "        else:\n",
    "            new_prompt_history.append(last_message_type(content=scratch_pad))\n",
    "            scratch_pad = message.content\n",
    "            last_message_type = current_message_type\n",
    "\n",
    "    # Handle the last accumulated message.\n",
    "    new_prompt_history.append(last_message_type(content=scratch_pad))\n",
    "\n",
    "    return new_prompt_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_prompt_msgs = collapse_prompts(compare_prompt_msgs)\n",
    "compare_prompt_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HumanMessage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prompt_compare_critique\u001b[39m(compare_prompt_msgs: List[\u001b[43mHumanMessage\u001b[49m], llm: BaseChatModel, replace_newline: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m llm(compare_prompt_msgs)\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m replace_newline:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HumanMessage' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "def _prompt_compare_critique(compare_prompt_msgs: List[HumanMessage], llm: BaseChatModel, replace_newline: bool = False):\n",
    "    out = llm(compare_prompt_msgs).content.strip('\\n').strip()\n",
    "    if replace_newline:\n",
    "        out = out.replace('\\n', '')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _prompt_compare_critique(compare_prompt_msgs, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_rules(llm_text):\n",
    "    pattern = r'((?:REMOVE|EDIT|ADD|AGREE)(?: \\d+|)): (?:[a-zA-Z\\s\\d]+: |)(.*)'\n",
    "    matches = re.findall(pattern, llm_text)\n",
    "\n",
    "    res = []\n",
    "    banned_words = ['ADD', 'AGREE', 'EDIT']\n",
    "    for operation, text in matches:\n",
    "        text = text.strip()\n",
    "        if text != '' and not any([w in text for w in banned_words]) and text.endswith('.'):\n",
    "        # if text is not empty\n",
    "        # if text doesn't contain banned words (avoid weird formatting cases from llm)\n",
    "        # if text ends with a period (avoid cut off sentences from llm)\n",
    "            if 'ADD' in operation:\n",
    "                res.append(('ADD', text))\n",
    "            else:\n",
    "                res.append((operation.strip(), text))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = parse_rules(out)\n",
    "operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def retrieve_rule_index(rules: List[Tuple[str, int]], operation_rule_text: str) -> int:\n",
    "    for i in range(len(rules)):\n",
    "        if rules[i][0] in operation_rule_text:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def is_existing_rule(rules: List[Tuple[str, int]], operation_rule_text: str) -> bool:\n",
    "    for i in range(len(rules)):\n",
    "        if rules[i][0] in operation_rule_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_err_operations(rules: List[Tuple[str, int]], operations: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    cleaned_operations = operations.copy()\n",
    "    \n",
    "    delete_indices = []\n",
    "    for i in range(len(cleaned_operations)):\n",
    "        # Split the operation into action type and optional rule number.\n",
    "        operation, operation_rule_text = cleaned_operations[i]\n",
    "        operation_type = operation.split(' ')[0]\n",
    "        rule_num = int(operation.split(' ')[1]) if ' ' in operation else None\n",
    "\n",
    "        if operation_type == 'ADD':\n",
    "            if is_existing_rule(rules, operation_rule_text): # If new rule_text is an existing rule ('in').\n",
    "                delete_indices.append(i)\n",
    "        else:\n",
    "            if operation_type == 'EDIT':\n",
    "                if is_existing_rule(rules, operation_rule_text): # If rule is matching ('in') existing rule, change it to AGREE.\n",
    "                    rule_num = retrieve_rule_index(rules, operation_rule_text)\n",
    "                    cleaned_operations[i] = (f'AGREE {rule_num+1}', rules[rule_num][0])\n",
    "                elif (rule_num is None) or (rule_num > len(rules)):   # If rule doesn't exist, remove.\n",
    "                    delete_indices.append(i)\n",
    "                    \n",
    "            elif operation_type == 'REMOVE' or operation_type == 'AGREE':\n",
    "                if not is_existing_rule(rules, operation_rule_text): # If new operation_rule_text is not an existing rule.\n",
    "                    delete_indices.append(i)\n",
    "\n",
    "    # Remove problematic operations.\n",
    "    cleaned_operations = [cleaned_operations[i] for i in range(len(cleaned_operations)) if i not in delete_indices]\n",
    "    \n",
    "    return cleaned_operations\n",
    "\n",
    "def update_rules(rules: List[Tuple[str, int]], operations: List[Tuple[str, str]], is_full: bool = False) -> List[Tuple[str, int]]:\n",
    "    updated_rules = rules.copy()\n",
    "    \n",
    "    for op in ['REMOVE', 'AGREE', 'EDIT', 'ADD']: # Order is important\n",
    "        for i in range(len(operations)):\n",
    "            operation, operation_rule_text = operations[i]\n",
    "            operation_type = operation.split(' ')[0]\n",
    "            if operation_type != op:\n",
    "                continue\n",
    "\n",
    "            if operation_type == 'REMOVE': # remove rule: -1\n",
    "                rule_index = retrieve_rule_index(updated_rules, operation_rule_text) # if rule_num doesn't match but text does\n",
    "                remove_strength = 3 if is_full else 1\n",
    "                updated_rules[rule_index] = (updated_rules[rule_index][0], updated_rules[rule_index][1]-remove_strength) # -1 (-3 if list full) to the counter\n",
    "            elif operation_type == 'AGREE': # agree with rule: +1\n",
    "                rule_index = retrieve_rule_index(updated_rules, operation_rule_text) # if rule_num doesn't match but text does\n",
    "                updated_rules[rule_index] = (updated_rules[rule_index][0], updated_rules[rule_index][1]+1) # +1 to the counter\n",
    "            elif operation_type == 'EDIT': # edit the rule: +1 // NEED TO BE AFTER REMOVE AND AGREE\n",
    "                rule_index = int(operation.split(' ')[1])-1\n",
    "                updated_rules[rule_index] = (operation_rule_text, updated_rules[rule_index][1]+1) # +1 to the counter\n",
    "            elif operation_type == 'ADD': # add new rule: +2\n",
    "                updated_rules.append((operation_rule_text, 2))\n",
    "    updated_rules = [updated_rules[i] for i in range(len(updated_rules)) if updated_rules[i][1] > 0] # remove rules when counter reach 0\n",
    "    updated_rules.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return updated_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_full = max_num_rules+5 <= len(rules_with_count)\n",
    "\n",
    "# Remove problematic operations.\n",
    "operations = remove_err_operations(rules_with_count, operations)\n",
    "rules_with_count = update_rules(rules_with_count, operations, is_full=is_full)\n",
    "rules = [rule[0] for rule in rules_with_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def create_rules(\n",
    "    llm: BaseChatModel,\n",
    "    experiences: Dict[str, List], \n",
    "    categories: Dict[str, int], \n",
    "    train_idxs: List[int], \n",
    "    rules: List[str], \n",
    "    rules_with_count: List[Tuple[str, int]],\n",
    "    max_num_rules: int,\n",
    ") -> Tuple[List[str], List[Tuple[str, int]]]:\n",
    "    # Intersect between train_idxs and each category (compare, success, fail).\n",
    "    train_category_idxs = {\n",
    "        category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "            for category, category_idxs in categories.items()\n",
    "    }\n",
    "\n",
    "    # Compare.\n",
    "    for train_idx in train_category_idxs[\"compare\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "\n",
    "        # Compare the successful trial with all previous failed trials.\n",
    "        success_trial = trajectory[-1][-1]\n",
    "        for failed_trial in trajectory[:-1]:\n",
    "            out = _prompt_compare_critique(\n",
    "                rules, \n",
    "                question, \n",
    "                success_trial, \n",
    "                failed_trial, \n",
    "                max_num_rules < len(rules_with_count),\n",
    "                llm\n",
    "            )\n",
    "            operations = parse_rules(out)\n",
    "            operations = remove_err_operations(rules_with_count, operations)\n",
    "\n",
    "            # Update rules_with_count and rules with comparison insights.\n",
    "            rules_with_count = update_rules(rules_with_count, operations, is_full=max_num_rules+5 <= len(rules_with_count))\n",
    "            rules = [rule[0] for rule in rules_with_count]\n",
    "\n",
    "    # Success.\n",
    "    for train_idx in train_category_idxs[\"success\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "\n",
    "    return rules, rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.utils.general import random_divide_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_critique_num = 8\n",
    "\n",
    "for idx, task in enumerate(self.succeeded_trial_history):\n",
    "    if idx in training_ids and len(self.succeeded_trial_history[task]) > 0:\n",
    "        all_success.append((task, self.succeeded_trial_history[task][0].trajectory))\n",
    "all_success = random_divide_list(all_success, self.success_critique_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      2\u001b[0m max_num_rules \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0125\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n\u001b[0;32m      4\u001b[0m experiences \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_15_compare_fake.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m categories \u001b[38;5;241m=\u001b[39m categorize_experiences(experiences)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "max_num_rules = 20\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)\n",
    "experiences = joblib.load(\"exp_15_compare_fake.joblib\")\n",
    "categories = categorize_experiences(experiences)\n",
    "folds = get_folds(categories, len(experiences['idxs']))\n",
    "\n",
    "# for fold, train_idxs in folds.items():\n",
    "#     print(fold, train_idxs)\n",
    "#     new_rules, new_rules_with_count = create_rules(llm, experiences, categories, train_idxs, rules, rules_with_count, max_num_rules)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Always search for relevant keywords related to the topic to gather information.',\n",
       " 'If the direct information is not available, look for related entities that can provide clues or insights.',\n",
       " 'Prioritize searching for the exact entity mentioned in the question to ensure the information obtained is directly relevant to the question asked.',\n",
       " 'Always verify the accuracy and relevance of the information obtained before making a final conclusion.',\n",
       " 'When encountering a question, make sure to follow a systematic approach in gathering information, analyzing it, and making a conclusion based on the evidence found.',\n",
       " 'Always double-check the relevance of the obtained information to the question asked before making a final conclusion.',\n",
       " 'Prioritize searching for specific keywords related to the question to gather relevant information efficiently.',\n",
       " 'Focus on breaking down complex search queries into simpler, more specific parts to gather relevant information efficiently.',\n",
       " 'Prioritize refining the search query based on the specific elements of the question to gather relevant information efficiently.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Always search for relevant keywords related to the topic to gather information.',\n",
       "  2),\n",
       " ('If the direct information is not available, look for related entities that can provide clues or insights.',\n",
       "  2),\n",
       " ('Prioritize searching for the exact entity mentioned in the question to ensure the information obtained is directly relevant to the question asked.',\n",
       "  2),\n",
       " ('Always verify the accuracy and relevance of the information obtained before making a final conclusion.',\n",
       "  2),\n",
       " ('When encountering a question, make sure to follow a systematic approach in gathering information, analyzing it, and making a conclusion based on the evidence found.',\n",
       "  2),\n",
       " ('Always double-check the relevance of the obtained information to the question asked before making a final conclusion.',\n",
       "  2),\n",
       " ('Prioritize searching for specific keywords related to the question to gather relevant information efficiently.',\n",
       "  2),\n",
       " ('Focus on breaking down complex search queries into simpler, more specific parts to gather relevant information efficiently.',\n",
       "  2),\n",
       " ('Prioritize refining the search query based on the specific elements of the question to gather relevant information efficiently.',\n",
       "  2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Always search for relevant keywords related to the topic to gather information.',\n",
       " 'If the direct information is not available, look for related entities that can provide clues or insights.',\n",
       " 'Prioritize searching for the exact entity mentioned in the question to ensure the information obtained is directly relevant to the question asked.',\n",
       " 'Always verify the accuracy and relevance of the information obtained before making a final conclusion.',\n",
       " 'When encountering a question, make sure to follow a systematic approach in gathering information, analyzing it, and making a conclusion based on the evidence found.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Always search for relevant keywords related to the topic to gather information.',\n",
       "  2),\n",
       " ('If the direct information is not available, look for related entities that can provide clues or insights.',\n",
       "  2),\n",
       " ('Prioritize searching for the exact entity mentioned in the question to ensure the information obtained is directly relevant to the question asked.',\n",
       "  2),\n",
       " ('Always verify the accuracy and relevance of the information obtained before making a final conclusion.',\n",
       "  2),\n",
       " ('When encountering a question, make sure to follow a systematic approach in gathering information, analyzing it, and making a conclusion based on the evidence found.',\n",
       "  2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
