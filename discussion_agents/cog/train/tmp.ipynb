{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents.cog.agent.reflexion import ReflexionReActAgent\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>supporting_facts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a7613c15542994ccc9186bf</td>\n",
       "      <td>VIVA Media AG changed it's name in 2004. What ...</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['VIVA Media', 'Gesellschaft mit bes...</td>\n",
       "      <td>{'title': ['Constantin Medien', 'VIVA Poland',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf2fa35542993344016c11</td>\n",
       "      <td>Which of Jonny Craig and Pete Doherty has been...</td>\n",
       "      <td>Jonny\" Craig</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Jonny Craig', 'Jonny Craig', 'Pete...</td>\n",
       "      <td>{'title': ['Pete Doherty', 'Relativity (Emaros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adfdef9554299025d62a36b</td>\n",
       "      <td>Where was the first governor after the The Mis...</td>\n",
       "      <td>Bath, Maine</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Maine gubernatorial election, 1820...</td>\n",
       "      <td>{'title': ['Compromise of 1790', 'Anti-Nebrask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a7180205542994082a3e856</td>\n",
       "      <td>The creator of \"Wallace and Gromit\" also creat...</td>\n",
       "      <td>Creature Comforts</td>\n",
       "      <td>bridge</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Creature Comforts', 'Creature Comf...</td>\n",
       "      <td>{'title': ['Creature Comforts', 'Tata Steel Zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a78bc6b554299148911f979</td>\n",
       "      <td>Woman's Era and Naj are what kind of magazines?</td>\n",
       "      <td>fortnightly women interest magazine</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>{'title': ['Woman's Era', 'Naj'], 'sent_id': [...</td>\n",
       "      <td>{'title': ['Lifestyle trends and media', 'Chin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5a7613c15542994ccc9186bf   \n",
       "1  5adf2fa35542993344016c11   \n",
       "2  5adfdef9554299025d62a36b   \n",
       "3  5a7180205542994082a3e856   \n",
       "4  5a78bc6b554299148911f979   \n",
       "\n",
       "                                            question  \\\n",
       "0  VIVA Media AG changed it's name in 2004. What ...   \n",
       "1  Which of Jonny Craig and Pete Doherty has been...   \n",
       "2  Where was the first governor after the The Mis...   \n",
       "3  The creator of \"Wallace and Gromit\" also creat...   \n",
       "4    Woman's Era and Naj are what kind of magazines?   \n",
       "\n",
       "                                  answer        type level  \\\n",
       "0  Gesellschaft mit beschränkter Haftung      bridge  hard   \n",
       "1                           Jonny\" Craig  comparison  hard   \n",
       "2                            Bath, Maine      bridge  hard   \n",
       "3                      Creature Comforts      bridge  hard   \n",
       "4    fortnightly women interest magazine  comparison  hard   \n",
       "\n",
       "                                    supporting_facts  \\\n",
       "0  {'title': ['VIVA Media', 'Gesellschaft mit bes...   \n",
       "1  {'title': ['Jonny Craig', 'Jonny Craig', 'Pete...   \n",
       "2  {'title': ['Maine gubernatorial election, 1820...   \n",
       "3  {'title': ['Creature Comforts', 'Creature Comf...   \n",
       "4  {'title': ['Woman's Era', 'Naj'], 'sent_id': [...   \n",
       "\n",
       "                                             context  \n",
       "0  {'title': ['Constantin Medien', 'VIVA Poland',...  \n",
       "1  {'title': ['Pete Doherty', 'Relativity (Emaros...  \n",
       "2  {'title': ['Compromise of 1790', 'Anti-Nebrask...  \n",
       "3  {'title': ['Creature Comforts', 'Tata Steel Zo...  \n",
       "4  {'title': ['Lifestyle trends and media', 'Chin...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "hotpot = joblib.load('../agent/hotpot-qa-distractor-sample.joblib').reset_index(drop=True)\n",
    "hotpot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_experience(\n",
    "    reflexion_react_agent: ReflexionReActAgent,\n",
    "    questions: List[str],\n",
    "    keys: List[str],\n",
    "    strategy: Optional[str] = \"reflexion\",\n",
    ") -> Dict[str, List]:\n",
    "    experiences = {\n",
    "        \"idxs\": [],\n",
    "        \"questions\": [],\n",
    "        \"keys\": [],\n",
    "        \"trajectories\": [],\n",
    "        \"reflections\": []\n",
    "    }\n",
    "    for idx, (question, key) in enumerate(zip(questions, keys)):\n",
    "        trajectory = reflexion_react_agent.generate(\n",
    "            question=question, key=key, strategy=strategy, reset=True\n",
    "        )\n",
    "\n",
    "        experiences[\"idxs\"].append(idx)\n",
    "        experiences[\"questions\"].append(question)\n",
    "        experiences[\"keys\"].append(key)\n",
    "        experiences[\"trajectories\"].append(trajectory)\n",
    "        experiences[\"reflections\"].append(reflexion_react_agent.reflector.reflections)\n",
    "        \n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuvin\\anaconda3\\envs\\discussion-agents\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\tuvin\\anaconda3\\envs\\discussion-agents\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "c:\\Users\\tuvin\\anaconda3\\envs\\discussion-agents\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\tuvin\\anaconda3\\envs\\discussion-agents\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    self_reflect_llm=llm,\n",
    "    action_llm=llm,\n",
    "    max_steps=7,\n",
    "    max_trials=3,\n",
    ")\n",
    "\n",
    "experiences = gather_experience(agent, questions=hotpot.question.values.tolist()[:k], keys=hotpot.answer.values.tolist()[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experiences_5.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(experiences, \"experiences_10.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "experiences = joblib.load(\"experiences_10.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compare': [], 'success': [1, 3, 6, 7, 8], 'fail': [0, 2, 4, 5, 9]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize_experiences(experiences: Dict[str, List]) -> Dict[str, List]:\n",
    "    count_dict = {\n",
    "        \"compare\": [],\n",
    "        \"success\": [],\n",
    "        \"fail\": []\n",
    "    }\n",
    "\n",
    "    for idx in experiences[\"idxs\"]:  # Index for a particular task.\n",
    "        trajectory = experiences[\"trajectories\"][idx]\n",
    "        trials_are_correct = [trial[0] for trial in trajectory]  # (is_correct, answer, output)[0]\n",
    "\n",
    "        # Success.\n",
    "        if all(trials_are_correct) and len(trials_are_correct) == 1:  # If success @ first trial, then stop generation.\n",
    "            count_dict[\"success\"].append(idx)\n",
    "        # Compare.\n",
    "        elif trials_are_correct[-1]:  # If fail(s), then succeeds, then only last trial is True.\n",
    "            count_dict[\"compare\"].append(idx)\n",
    "        # Fail.\n",
    "        elif not all(trials_are_correct):  # All trials failed, then fail case.\n",
    "            count_dict[\"fail\"].append(idx)\n",
    "        else:\n",
    "            raise ValueError(f\"Unhandled scenario for trajectory at index {idx}.\")\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "categories = categorize_experiences(experiences)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [3, 5, 6, 9], 1: [0, 1, 2, 4, 7, 8]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_folds(categories: Dict[str, List], n_instances: int, n_folds: int = 2) -> Dict[str, List]:\n",
    "    folds = {fold: [] for fold in range(n_folds)}\n",
    "\n",
    "    # Assign labels for 'compare', 'success', and  'fail'.\n",
    "    for _, indices in categories.items():\n",
    "        random.shuffle(indices)\n",
    "        for count, idx in enumerate(indices):\n",
    "            folds[count % n_folds].append(idx)\n",
    "\n",
    "    # Each fold is a validation set. Take the difference to get the training set of each fold.\n",
    "    folds = {fold: list(set(list(range(n_instances))).difference(values)) for fold, values in folds.items()}\n",
    "\n",
    "    return folds\n",
    "\n",
    "folds = get_folds(categories, 10)\n",
    "folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = folds[0]\n",
    "\n",
    "train_category_idxs = {\n",
    "    category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "        for category, category_idxs in categories.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 6, 9]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compare': [], 'success': [3, 6], 'fail': [9, 5]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_category_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "from discussion_agents.cog.prompts.expel import (\n",
    "    system_template, \n",
    "    SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION,\n",
    "    EXISTING_RULES_AI_NAME,\n",
    "    NON_EXISTENT_RULES_AT_NAME\n",
    ")\n",
    "\n",
    "def _build_compare_prompt(\n",
    "    rule_items: List[str], \n",
    "    fail_history: str, \n",
    "    success_history: str, \n",
    "    question: str,\n",
    "    reflections: Optional[List[str]] = None,\n",
    ") -> List[HumanMessage]:\n",
    "    critique_history = []\n",
    "\n",
    "    # System prompt.\n",
    "    prefix = (\n",
    "        HumanMessagePromptTemplate.from_template(system_template)\n",
    "        .format_messages(\n",
    "            ai_name=NON_EXISTENT_RULES_AT_NAME if not rule_items else EXISTING_RULES_AI_NAME,\n",
    "            instruction=SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION\n",
    "        )\n",
    "    )\n",
    "    critique_history.extend(prefix)\n",
    "\n",
    "    # Task prompt.\n",
    "    human_format_dict = dict(instruction='')\n",
    "    human_format_dict['fail_history'] = fail_history\n",
    "    human_format_dict['task'] = question\n",
    "    human_format_dict['success_history'] = success_history\n",
    "    human_format_dict['existing_rules'] = '\\n'.join([f'{i}. {r}' for i, r in enumerate(rule_items, 1)])\n",
    "    if reflections:\n",
    "        human_format_dict['reflections_list'] = '- ' + '\\n- '.join(reflections)\n",
    "\n",
    "    return critique_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_build_compare_prompt() missing 3 required positional arguments: 'fail_history', 'success_history', and 'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m_build_compare_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: _build_compare_prompt() missing 3 required positional arguments: 'fail_history', 'success_history', and 'question'"
     ]
    }
   ],
   "source": [
    "_build_compare_prompt([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def create_rules(experiences: Dict[str, List], categories: Dict[str, int], train_idxs: List[int]):\n",
    "    rule_items = []\n",
    "    rule_items_with_count: List[Tuple[str, int]] = []\n",
    "\n",
    "    # Intersect between train_idxs and each category (compare, success, fail).\n",
    "    train_category_idxs = {\n",
    "        category: list(set(train_idxs).intersection(set(category_idxs))) \\\n",
    "            for category, category_idxs in categories.items()\n",
    "    }\n",
    "\n",
    "    # Compare.\n",
    "    for train_idx in train_category_idxs[\"compare\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        key = experiences[\"keys\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "        reflection = experiences[\"reflections\"][train_idx]\n",
    "\n",
    "    # Success.\n",
    "    for train_idx in train_category_idxs[\"success\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        key = experiences[\"keys\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "        reflection = experiences[\"reflections\"][train_idx]\n",
    "    \n",
    "    # Fail.\n",
    "    for train_idx in train_category_idxs[\"fail\"]:\n",
    "        question = experiences[\"questions\"][train_idx]\n",
    "        key = experiences[\"keys\"][train_idx]\n",
    "        trajectory = experiences[\"trajectories\"][train_idx]\n",
    "        reflection = experiences[\"reflections\"][train_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, train_idxs in folds.items():\n",
    "    print(fold, train_idxs)\n",
    "    create_rules(experiences, categories, train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, eval_idxs in enumerate(eval_idx_list):\n",
    "    if k < starting_fold:\n",
    "        continue\n",
    "    training_ids = set(range(num_training_tasks)) - set(eval_idxs)\n",
    "    (SAVE_PATH / f\"fold_{k}\").mkdir(exist_ok=True)\n",
    "    log += f'################## FOLD {k} ##################\\n'\n",
    "    log += react_agent.create_rules(\n",
    "        list(training_ids),\n",
    "        cache_fold=k,\n",
    "        logging_dir=str(SAVE_PATH / f\"fold_{k}\"),\n",
    "        run_name=cfg.run_name,\n",
    "        loaded_dict=dicts[-1] if resume and resume_starting_fold == starting_fold else None,\n",
    "        loaded_log=critique_summary_log if resume and resume_starting_fold == starting_fold else None,\n",
    "        eval_idx_list=eval_idx_list,\n",
    "        saving_dict=True,\n",
    "    )\n",
    "    starting_fold += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
