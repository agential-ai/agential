{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<discussion_agents.cog.agent.reflexion.ReflexionReActAgent at 0x289891a7be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from discussion_agents.cog.agent.reflexion import ReflexionReActAgent\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "ReflexionReActAgent(\n",
    "    self_reflect_llm=llm,\n",
    "    action_llm=llm,\n",
    "    max_steps=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'task': f'Question: {row[\"question\"]}',\n",
    "        'env_kwargs': {\n",
    "            'question': row['question'],\n",
    "            'key': row['answer'],\n",
    "        },\n",
    "        'env_name': 'hotpotqa',\n",
    "    } for _, row in (joblib.load(\"./hotpot-qa-distractor-sample.joblib\")\n",
    "                     .reset_index(drop=True)\n",
    "                     .iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.yaml\n",
    "#\n",
    "# defaults:\n",
    "#   - _self_\n",
    "#   - benchmark: hotpotqa\n",
    "#   - agent: expel\n",
    "#   - override hydra/hydra_logging: disabled  \n",
    "#   - override hydra/job_logging: disabled  \n",
    "\n",
    "# ai_name: ${benchmark.ai_name}\n",
    "# agent_type: ${agent.name}\n",
    "# log_dir: logs\n",
    "# run_name: run\n",
    "# testing: true\n",
    "# resume: false\n",
    "\n",
    "# # remove hydra output files\n",
    "# hydra:  \n",
    "#   output_subdir: null  \n",
    "#   run:  \n",
    "#     dir: .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# benchmark/hotpotqa.yaml\n",
    "#\n",
    "# name: hotpotqa\n",
    "# task_prefix: \"Question: \"\n",
    "# task_file: data/hotpotqa/hotpot-qa-distractor-sample.joblib\n",
    "# max_steps: 7\n",
    "# num_fewshots: 6\n",
    "# eval_configs:\n",
    "#   k_folds: 2\n",
    "#   k_folds_seed: 42\n",
    "# ai_name: QA system\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# agent/expel.yaml\n",
    "#\n",
    "# name: expel\n",
    "# llm: gpt-3.5-turbo # gpt-3.5-turbo-0301\n",
    "# max_reflection_depth: 3\n",
    "# max_num_rules: 20\n",
    "# truncate_strategy: null\n",
    "# fewshot_strategy: task_similarity # task_similarity, step_similarity, reflection_similarity, thought_similarity, action_similarity, rotation, task_thought_similarity\n",
    "# critique_truncate_strategy: random\n",
    "# success_critique_num: 8\n",
    "# retrieval_kwargs:\n",
    "#   embedder_path: \"all-mpnet-base-v2\" # sentence-transformers/all-mpnet-base-v2\n",
    "#   embedder_type: huggingface\n",
    "#   retriever_type: knn # knn or svm\n",
    "#   buffer_retrieve_ratio: 4\n",
    "#   reranker: none\n",
    "#   max_fewshot_tokens: auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "ai_name = \"QA system\"\n",
    "SYSTEM_INSTRUCTION = \"\"\"Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:\n",
    "(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
    "(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfully found by Search.\n",
    "(3) Finish[answer], which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "human_instruction_template = \"\"\"{instruction}You may take maximum of {max_steps} steps.\n",
    "Here are some examples:\"\"\"\n",
    "HUMAN_INSTRUCTION = HumanMessagePromptTemplate.from_template(human_instruction_template)\n",
    "\n",
    "system_instruction = SYSTEM_INSTRUCTION\n",
    "human_instruction = HUMAN_INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'task': f'Question: {row[\"question\"]}',\n",
    "        'env_kwargs': {\n",
    "            'question': row['question'],\n",
    "            'key': row['answer'],\n",
    "        },\n",
    "        'env_name': 'hotpotqa',\n",
    "    } for _, row in (joblib.load(\"./hotpot-qa-distractor-sample.joblib\")\n",
    "                     .reset_index(drop=True)\n",
    "                     .iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion_agents import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshots = [\"\"\"Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "Thought 1: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "Action 1: Search[Colorado orogeny]\n",
    "Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "Thought 2: It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "Action 2: Lookup[eastern sector]\n",
    "Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "Thought 3: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "Action 3: Search[High Plains]\n",
    "Observation 3: High Plains refers to one of two distinct land regions:\n",
    "Thought 4: I need to instead search High Plains (United States).\n",
    "Action 4: Search[High Plains (United States)]\n",
    "Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
    "Thought 5: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "Action 5: Finish[1,800 to 7,000 ft]\"\"\",\n",
    "\"\"\"Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "Thought 1: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "Action 1: Search[Milhouse]\n",
    "Observation 1: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "Thought 2: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "Action 2: Lookup[named after]\n",
    "Observation 2: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous. \n",
    "Thought 3: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "Action 3: Finish[Richard Nixon]\"\"\",\n",
    "\"\"\"Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n",
    "Thought 1: I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\n",
    "Action 1: Search[Adam Clayton Powell]\n",
    "Observation 1: Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].\n",
    "Thought 2: To find the documentary, I can search Adam Clayton Powell (film).\n",
    "Action 2: Search[Adam Clayton Powell (film)]\n",
    "Observation 2: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg.\n",
    "The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\n",
    "Thought 3: Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\n",
    "Action 3: Finish[The Saimaa Gesture]\"\"\",\n",
    "\"\"\"Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
    "Thought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
    "Action 1: Search[Nicholas Ray]\n",
    "Observation 1: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 – June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
    "Thought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
    "Action 2: Search[Elia Kazan]\n",
    "Observation 2: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
    "Thought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
    "Action 3: Finish[director, screenwriter, actor]\"\"\",\n",
    "\"\"\"Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
    "Thought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.\n",
    "Action 1: Search[Arthur's Magazine]\n",
    "Observation 1: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century. \n",
    "Thought 2: Arthur's Magazine was started in 1844. I need to search First for Women next.\n",
    "Action 2: Search[First for Women]\n",
    "Observation 2: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989. \n",
    "Thought 3: First for Women was started in 1989. 1844 (Arthur's Magazine) < 1989 (First for Women), so Arthur's Magazine was started first.\n",
    "Action 3: Finish[Arthur's Magazine]\"\"\",\n",
    "\"\"\"Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
    "Thought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
    "Action 1: Search[Pavel Urysohn]\n",
    "Observation 1: Pavel Samuilovich Urysohn (February 3, 1898 â August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
    "Thought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
    "Action 2: Search[Leonid Levin]\n",
    "Observation 2: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist. \n",
    "Thought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work. \n",
    "Action 3: Finish[yes]\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "system_template = \"\"\"You are {ai_name}. {instruction}\"\"\"\n",
    "\n",
    "system_message_prompt = HumanMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "max_steps = 7\n",
    "\n",
    "\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='You are a. b')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "system_template = \"\"\"You are {ai_name}. {instruction}\"\"\"\n",
    "\n",
    "system_message_prompt = HumanMessagePromptTemplate.from_template(system_template)\n",
    "system_message_prompt.format_messages(ai_name=\"a\", instruction=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_fewshots = [\n",
    "\"\"\"\n",
    "Previous Trial:\n",
    "Question: The Rome Protocols were signed by three Prime Ministers one of which was assassinated as part of what?\n",
    "Thought 1: I need to search Rome Protocols, find the three Prime Ministers, then find what they were assassinated as part of.\n",
    "Action 1: Search[Rome Protocols]\n",
    "Observation 1: The Rome Protocols were a series of three international agreements concluded in Rome on 17 March 1934 between the governments of Austria, Hungary and Italy. They were signed by Italian Prime Minister Benito Mussolini, Austrian Chancellor Engelbert Dollfuss and Hungarian Prime Minister Gyula Gömbös. All of the protocols went into effect on 12 July 1934 and were registered in League of Nations Treaty Series on 12 December 1934.\n",
    "Thought 2: The three Prime Ministers were Benito Mussolini, Engelbert Dollfuss, and Gyula Gömbös. I need to search Benito Mussolini and find what he was assassinated as part of.\n",
    "Action 2: Search[Benito Mussolini]\n",
    "Observation 2: Benito Amilcare Andrea Mussolini (UK:  MU(U)SS-ə-LEE-nee, US:  MOOSS-, Italian: [beˈniːto aˈmilkare anˈdrɛːa mussoˈliːni]; 29 July 1883 – 28 April 1945) was an Italian politician and journalist who founded and led the National Fascist Party (PNF). He was Prime Minister of Italy from the March on Rome in 1922 until his deposition in 1943, as well as \"Duce\" of Italian fascism from the establishment of the Italian Fasces of Combat in 1919 until his summary execution in 1945 by Italian partisans. As dictator of Italy and principal founder of fascism, Mussolini inspired and supported the international spread of fascist movements during the inter-war period.Mussolini was originally a socialist politician and a journalist at the Avanti! newspaper. In 1912, he became a member of the National Directorate of the Italian Socialist Party (PSI), but he was expelled from the PSI for advocating military intervention in World War I, in opposition to the party's stance on neutrality. In 1914, Mussolini founded a new journal, Il Popolo d'Italia, and served in the Royal Italian Army during the war until he was wounded and discharged in 1917. Mussolini denounced the PSI, his views now centering on Italian nationalism instead of socialism, and later founded the fascist movement which came to oppose egalitarianism and class conflict, instead advocating \"revolutionary nationalism\" transcending class lines. On 31 October 1922, following the March on Rome (28–30 October), Mussolini was appointed prime minister by King Victor Emmanuel III, becoming the youngest individual to hold the office up to that time. After removing all political opposition through his secret police and outlawing labor strikes, Mussolini and his followers consolidated power through a series of laws that transformed the nation into a one-party dictatorship. Within five years, Mussolini had established dictatorial authority by both legal and illegal means and aspired to create a totalitarian state. In 1929, Mussolini signed the Lateran Treaty with the Holy See to establish Vatican City.\n",
    "Mussolini's foreign policy aimed to restore the ancient grandeur of the Roman Empire by expanding Italian colonial possessions and the fascist sphere of influence. In the 1920s, he ordered the Pacification of Libya, instructed the bombing of Corfu over an incident with Greece, established a protectorate over Albania, and incorporated the city of Fiume into the Italian state via agreements with Yugoslavia. In 1936, Ethiopia was conquered following the Second Italo-Ethiopian War and merged into Italian East Africa (AOI) with Eritrea and Somalia. In 1939, Italian forces annexed Albania. Between 1936 and 1939, Mussolini ordered the successful Italian military intervention in Spain in favor of Francisco Franco during the Spanish Civil War. Mussolini's Italy initially tried to avoid the outbreak of a second global war, sending troops at the Brenner Pass to delay Anschluss and taking part in the Stresa Front, the Lytton Report, the Treaty of Lausanne, the Four-Power Pact and the Munich Agreement. However, Italy then alienated itself from Britain and France by aligning with Germany and Japan. Germany invaded Poland on 1 September 1939, resulting in declarations of war by France and the UK and the start of World War II.\n",
    "On 10 June 1940, Mussolini decided to enter the war on the Axis side. Despite initial success, the subsequent Axis collapse on multiple fronts and eventual Allied invasion of Sicily made Mussolini lose the support of the population and members of the Fascist Party. As a consequence, early on 25 July 1943, the Grand Council of Fascism passed a motion of no confidence in Mussolini; later that day King Victor Emmanuel III dismissed him as head of government and had him placed in custody, appointing Pietro Badoglio to succeed him as Prime Minister. After the king agreed to an armistice with the Allies, on 12 September 1943 Mussolini was rescued from captivity in the Gran Sasso raid by German paratroopers and Waffen-SS commandos led by Major Otto-Harald Mors. Adolf Hitler, after meeting with the rescued former dictator, then put Mussolini in charge of a puppet regime in northern Italy, the Italian Social Republic (Italian: Repubblica Sociale Italiana, RSI), informally known as the Salò Republic, causing a civil war. In late April 1945, in the wake of near total defeat, Mussolini and his mistress Clara Petacci attempted to flee to Switzerland, but both were captured by Italian communist partisans and summarily executed by firing squad on 28 April 1945 near Lake Como. The bodies of Mussolini and his mistress were then taken to Milan, where they were hung upside down at a service station to publicly confirm their demise.\n",
    "Thought 3: Benito Mussolini was assassinated as part of the collapse of the Axis powers during World War II. So the Rome Protocols were signed by three Prime Ministers one of which was assassinated as part of World War II.\n",
    "Action 3: Finish[World War II]\n",
    "\n",
    "Reflection: I searched one of the prime ministers involved in the signing, then attemted to answer right away. I should have searched each of the prime ministers, then looked up 'death' on each of their pages in order to get more information before answering.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Previous Trial:\n",
    "Question: Kam Heskin plays Paige Morgan in a 2004 film directed by who?\n",
    "Thought 1: I need to search Kam Heskin and Paige Morgan, find the 2004 film they are in, then find the director of the film.\n",
    "Action 1: Search[Kam Heskin]\n",
    "Observation 1: Kam Erika Heskin (born May 8, 1973) is an American actress. She began her career playing Caitlin Deschanel on the NBC daytime soap opera Sunset Beach (1998–1999), before appearing in films Planet of the Apes (2001) and Catch Me If You Can (2002). Heskin went on in 2003 to play Elizabeth Bennet in an independent film Pride & Prejudice: A Latter-Day Comedy, Robin in Charmed (2004),  and Paige Morgan in The Prince and Me film franchise (2006–2010).\n",
    "Thought 2: Kam Heskin plays Paige Morgan in The Prince and Me film franchise. I need to search The Prince and Me film franchise and find the director of the 2004 film.\n",
    "Action 2: Search[The Prince and Me film franchise]\n",
    "Observation 2: Could not find [The Prince and Me film franchise]. Similar: ['Kam Heskin', 'Prince of Persia', 'Harry Potter and the Half-Blood Prince (film)', 'Prince of Persia: The Sands of Time (film)', 'The Chronicles of Narnia (film series)', 'Majestic Prince (franchise)', 'Kingsman (franchise)', 'The Little Prince (2015 film)', 'Shrek (franchise)', 'Harry Potter (film series)']\n",
    "Thought 3: I need to search The Prince and Me (2004 film) instead.\n",
    "Action 3: Search[The Prince and Me (2004 film)]\n",
    "Observation 3: Could not find [The Prince and Me (2004 film)]. Similar: ['The Prince & Me', 'The Prince & Me 2: The Royal Wedding', 'Kam Heskin', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince of Persia', 'Brooklynn Prince', 'Prince (musician)']\n",
    "Thought 4: I need to search The Prince & Me (2004 film).\n",
    "Action 4: Search[The Prince & Me (2004 film)]\n",
    "Observation 4: Could not find [The Prince & Me (2004 film)]. Similar: ['The Prince & Me', 'The Prince & Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\n",
    "Thought 5: I need to search The Prince & Me (2004 film) instead.\n",
    "Action 5: Search[The Prince & Me (2004 film)]\n",
    "Observation 5: Could not find [The Prince & Me (2004 film)]. Similar: ['The Prince & Me', 'The Prince & Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\n",
    "Thought 6: I need to search The Prince & Me (2004 film).\n",
    "Action 6: Search[The Prince & Me (2004 film)]\n",
    "Observation 6: Could not find [The Prince & Me (2004 film)]. Similar: ['The Prince & Me', 'The Prince & Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\n",
    "\n",
    "Reflection: I got stuck in a loop where I kept trying to search 'The Prince & Me (2004 film)' but the page could not be found. Instead I should have tried to search the similar results that had a similar name to see and they were made in 2004.\n",
    "\"\"\"]\n",
    "\n",
    "human_instruction_reflection_template = \"\"\"Here are some examples:\"\"\"\n",
    "reflection_task_prompt = HumanMessagePromptTemplate.from_template(human_instruction_reflection_template)\n",
    "\n",
    "reflection_system_instruction = \"\"\"You will be given a previous reasoning trial in which you were given access to a Docstore API environment and a question to answer. You were unsuccessful in answering the question either because you guessed the wrong answer with Finish[<answer>], or you used up your set number of reasoning steps. In a few sentences, Diagnose a possible reason for failure and devise a new, concise, high level plan that aims to mitigate the same failure. Use complete sentences.\"\"\"\n",
    "reflection_system_prompt = \"\"\"Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:\n",
    "(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
    "(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfully found by Search.\n",
    "(3) Finish[answer], which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION = \"\"\"You will be given two previous task trials in which you were given access to a Docstore API environment and a question to answer: one successful and one unsuccessful trial. You failed the trial either because you guessed the wrong answer with Finish[<answer>], or you used up your set number of reasoning steps.\"\"\"\n",
    "SYSTEM_CRITIQUE_ALL_SUCCESS_EXISTING_RULES_INSTRUCTION = \"\"\"You will be given successful tasks trials in which you were given access to a Docstore API environment and a question to answer.\"\"\"\n",
    "max_reflection_depth = 3 # default 0\n",
    "system_critique_instructions = dict(\n",
    "        compare_existing_rules=SYSTEM_CRITIQUE_EXISTING_RULES_INSTRUCTION,\n",
    "        all_success_existing_rules=SYSTEM_CRITIQUE_ALL_SUCCESS_EXISTING_RULES_INSTRUCTION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_RULES_OPERATION_TEMPLATE = \"\"\"<OPERATION> <RULE NUMBER>: <RULE>\n",
    "\n",
    "The available operations are: AGREE (if the existing rule is strongly relevant for the task), REMOVE (if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks). Each needs to CLOSELY follow their corresponding formatting below (any existing rule not edited, not agreed, nor removed is considered copied):\n",
    "\n",
    "AGREE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
    "REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
    "EDIT <EXISTING RULE NUMBER>: <NEW MODIFIED RULE>\n",
    "ADD <NEW RULE NUMBER>: <NEW RULE>\n",
    "\n",
    "Do not mention the trials in the rules because all the rules should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Any operation can be used MULTIPLE times. Do at most 4 operations and each existing rule can only get a maximum of 1 operation. \"\"\"\n",
    "\n",
    "human_critique_existing_rules_template = \"\"\"{instruction}\n",
    "Here are the two previous trials to compare and critique:\n",
    "TRIAL TASK:\n",
    "{task}\n",
    "\n",
    "SUCCESSFUL TRIAL:\n",
    "{success_history}\n",
    "\n",
    "FAILED TRIAL:\n",
    "{fail_history}\n",
    "\n",
    "Here are the EXISTING RULES:\n",
    "{existing_rules}\n",
    "\n",
    "By examining and contrasting to the successful trial, and the list of existing rules, you can perform the following operations: add, edit, remove, or agree so that the new list of rules is GENERAL and HIGH LEVEL critiques of the failed trial or proposed way of Thought so they can be used to avoid similar failures when encountered with different questions in the future. Have an emphasis on critiquing how to perform better Thought and Action. Follow the below format:\n",
    "\n",
    "\"\"\" + FORMAT_RULES_OPERATION_TEMPLATE\n",
    "human_existing_rules_critique = HumanMessagePromptTemplate.from_template(human_critique_existing_rules_template)\n",
    "\n",
    "\n",
    "human_critique_existing_rules_all_success_template = \"\"\"{instruction}\n",
    "Here are the trials:\n",
    "{success_history}\n",
    "\n",
    "Here are the EXISTING RULES:\n",
    "{existing_rules}\n",
    "\n",
    "By examining the successful trials, and the list of existing rules, you can perform the following operations: add, edit, remove, or agree so that the new list of rules are general and high level insights of the successful trials or proposed way of Thought so they can be used as helpful tips to different tasks in the future. Have an emphasis on tips that help the agent perform better Thought and Action. Follow the below format:\n",
    "\n",
    "\"\"\" + FORMAT_RULES_OPERATION_TEMPLATE\n",
    "human_all_success_existing_rules_critique = HumanMessagePromptTemplate.from_template(human_critique_existing_rules_all_success_template)\n",
    "\n",
    "human_critiques = dict(\n",
    "    compare_existing_rules=human_existing_rules_critique,\n",
    "    all_success_existing_rules=human_all_success_existing_rules_critique,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "Here are the two previous trials to compare and critique:\n",
      "TRIAL TASK:\n",
      "b\n",
      "\n",
      "SUCCESSFUL TRIAL:\n",
      "c\n",
      "\n",
      "FAILED TRIAL:\n",
      "d\n",
      "\n",
      "Here are the EXISTING RULES:\n",
      "e\n",
      "\n",
      "By examining and contrasting to the successful trial, and the list of existing rules, you can perform the following operations: add, edit, remove, or agree so that the new list of rules is GENERAL and HIGH LEVEL critiques of the failed trial or proposed way of Thought so they can be used to avoid similar failures when encountered with different questions in the future. Have an emphasis on critiquing how to perform better Thought and Action. Follow the below format:\n",
      "\n",
      "<OPERATION> <RULE NUMBER>: <RULE>\n",
      "\n",
      "The available operations are: AGREE (if the existing rule is strongly relevant for the task), REMOVE (if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks). Each needs to CLOSELY follow their corresponding formatting below (any existing rule not edited, not agreed, nor removed is considered copied):\n",
      "\n",
      "AGREE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
      "REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
      "EDIT <EXISTING RULE NUMBER>: <NEW MODIFIED RULE>\n",
      "ADD <NEW RULE NUMBER>: <NEW RULE>\n",
      "\n",
      "Do not mention the trials in the rules because all the rules should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Any operation can be used MULTIPLE times. Do at most 4 operations and each existing rule can only get a maximum of 1 operation. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "FORMAT_RULES_OPERATION_TEMPLATE = \"\"\"<OPERATION> <RULE NUMBER>: <RULE>\n",
    "\n",
    "The available operations are: AGREE (if the existing rule is strongly relevant for the task), REMOVE (if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks). Each needs to CLOSELY follow their corresponding formatting below (any existing rule not edited, not agreed, nor removed is considered copied):\n",
    "\n",
    "AGREE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
    "REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE>\n",
    "EDIT <EXISTING RULE NUMBER>: <NEW MODIFIED RULE>\n",
    "ADD <NEW RULE NUMBER>: <NEW RULE>\n",
    "\n",
    "Do not mention the trials in the rules because all the rules should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Any operation can be used MULTIPLE times. Do at most 4 operations and each existing rule can only get a maximum of 1 operation. \"\"\"\n",
    "\n",
    "human_critique_existing_rules_template = \"\"\"{instruction}\n",
    "Here are the two previous trials to compare and critique:\n",
    "TRIAL TASK:\n",
    "{task}\n",
    "\n",
    "SUCCESSFUL TRIAL:\n",
    "{success_history}\n",
    "\n",
    "FAILED TRIAL:\n",
    "{fail_history}\n",
    "\n",
    "Here are the EXISTING RULES:\n",
    "{existing_rules}\n",
    "\n",
    "By examining and contrasting to the successful trial, and the list of existing rules, you can perform the following operations: add, edit, remove, or agree so that the new list of rules is GENERAL and HIGH LEVEL critiques of the failed trial or proposed way of Thought so they can be used to avoid similar failures when encountered with different questions in the future. Have an emphasis on critiquing how to perform better Thought and Action. Follow the below format:\n",
    "\n",
    "\"\"\" + FORMAT_RULES_OPERATION_TEMPLATE\n",
    "human_existing_rules_critique = HumanMessagePromptTemplate.from_template(human_critique_existing_rules_template)\n",
    "\n",
    "print(human_existing_rules_critique.format_messages(instruction=\"a\", task=\"b\", success_history=\"c\", fail_history=\"d\", existing_rules=\"e\")[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_rules = 20  # default 0\n",
    "\n",
    "rule_template = HumanMessagePromptTemplate.from_template(\"\"\"The following are some experience you gather on a similar task of question answering using Wikipedia API. Use these as references to help you perform this task:\n",
    "{rules}\n",
    "\"\"\")\n",
    "\n",
    "truncate_strategy = None  # default None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_PARSER(llm_output, step: int, ai_message: bool) -> Tuple[ChatMessage, str, Dict[str, Any]]:\n",
    "    pattern = r'(?i)action\\s*(?:\\d+|)\\s*(?::|)\\s*'\n",
    "    action_pattern = r'(?i)\\w+\\[[^\\]]+(?:\\]|)'\n",
    "\n",
    "    match = re.match(pattern, llm_output)\n",
    "    if match:\n",
    "        action = llm_output[match.end():]\n",
    "        content = f\"Action {step}: {action}\"\n",
    "\n",
    "        if len(re.findall(action_pattern, action)) > 1:\n",
    "            return (\n",
    "                AIMessage(content=content) if ai_message else HumanMessage(content=content),\n",
    "                'action',\n",
    "                {'action': ''} # triggers invalid action\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            AIMessage(content=content) if ai_message else HumanMessage(content=content),\n",
    "            'action',\n",
    "            {'action': action}\n",
    "        )\n",
    "\n",
    "    actions = re.findall(action_pattern, llm_output)\n",
    "    if len(actions) == 1:\n",
    "        action = actions[0]\n",
    "        if action[-1] != ']':\n",
    "            action += ']'\n",
    "        content = f\"Action {step}: {action}\"\n",
    "        return (\n",
    "            AIMessage(content=content) if ai_message else HumanMessage(content=content),\n",
    "            'action',\n",
    "            {'action': action}\n",
    "        )\n",
    "    \n",
    "    if len(actions) > 1:\n",
    "        content = re.sub(r\"(?i)action\\s*(?:\\d*|)\\s*(?::|)\", \"\", llm_output)\n",
    "        return (\n",
    "            AIMessage(content=f\"Action {step}: {content}\"),\n",
    "            'action',\n",
    "            {'action': ''} # triggers invalid action\n",
    "        )\n",
    "\n",
    "    # everthing else will be assumed to be a inner thought\n",
    "    thought_pattern = r'(?i)thought\\s*(?:\\d+|)\\s*(?::|)\\s*(.*)'\n",
    "    match = re.match(thought_pattern, llm_output)\n",
    "    if match:\n",
    "        # Extract the thought word and content\n",
    "        thought_word = match.group(1)\n",
    "        content = f\"Thought {step}: {thought_word.rstrip(':')}\"\n",
    "    else:\n",
    "        content = f\"Thought {step}: {llm_output.rstrip(':')}\"\n",
    "    return (\n",
    "        AIMessage(content=content) if ai_message else HumanMessage(content=content),\n",
    "        'thought',\n",
    "        {}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OBSERVATION_FORMATTER(observation: str, step: int, *args, **kwargs) -> Tuple[ChatMessage, str]:\n",
    "    return HumanMessage(content=f\"Observation {step}: \" + observation.rstrip(':')), 'append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_embedder_key = \"huggingface\"\n",
    "def choose_embedder(key: str) -> Callable:\n",
    "    if key == 'openai':\n",
    "        return OpenAIEmbeddings\n",
    "    if key == 'llama':\n",
    "        return LlamaCppEmbeddings\n",
    "    if key == 'gpt4all':\n",
    "        return GPT4ALLEmbeddings\n",
    "    return HuggingFaceEmbeddings\n",
    "\n",
    "embedder = choose_embedder(choose_embedder_key)\n",
    "embedder_path = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STEP_STRIPPER(step: str, step_type: str):\n",
    "    if step_type == 'observation':\n",
    "        return re.sub(r'^(?i)observation(?:\\s+(\\d+))?:', 'Observation:', step)\n",
    "    if step_type == 'action':\n",
    "        return re.sub(r'^(?i)action(?:\\s+(\\d+))?:', 'Action:', step)\n",
    "    if step_type == 'thought':\n",
    "        return re.sub(r'^(?i)thought(?:\\s+(\\d+))?:', 'Thought:', step)\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_retriever_key = \"knn\"\n",
    "def choose_retriever(key: str) -> Callable:\n",
    "    if key == 'knn':\n",
    "        return KNNRetriever\n",
    "    if key == 'svm':\n",
    "        return SVMRetriever\n",
    "    return KNNRetriever\n",
    "\n",
    "retriever_cls = choose_retriever(choose_embedder_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CYCLER(lines: str) -> List[str]:\n",
    "    new_lines = []\n",
    "    scratch_pad = ''\n",
    "    for line in lines.split('\\n'):\n",
    "\n",
    "        # line is action\n",
    "        pattern = re.compile(r'^(?i)action(?:\\s+(\\d+))?:')\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            if scratch_pad != '':\n",
    "                new_lines.append(scratch_pad.strip())\n",
    "                scratch_pad = ''\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        # line is thought\n",
    "        pattern = re.compile(r'^(?i)thought(?:\\s+(\\d+))?:')\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            if scratch_pad != '':\n",
    "                new_lines.append(scratch_pad.strip())\n",
    "                scratch_pad = ''\n",
    "            new_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        # step is observation\n",
    "        scratch_pad += line + '\\n'\n",
    "\n",
    "    # the rest of the scratch pad\n",
    "    if scratch_pad != '':\n",
    "        new_lines.append(scratch_pad.strip())\n",
    "    return new_lines\n",
    "\n",
    "def STEP_IDENTIFIER(line: str) -> str:\n",
    "    line = line.strip()\n",
    "    pattern = re.compile(r'^(?i)action(?:\\s+(\\d+))?:')\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        return 'action'\n",
    "    pattern = re.compile(r'^(?i)observation(?:\\s+(\\d+))?:')\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        return 'observation'\n",
    "    return 'thought'\n",
    "\n",
    "def STEP_CYCLER(benchmark: str, lines: str, cycler: Callable, step_identifier: Callable, stripper: Callable = lambda x, y: x) -> List[str]:\n",
    "    steps = []\n",
    "    scratch_pad = ''\n",
    "    for line in cycler(lines):\n",
    "        step_type = step_identifier(line)\n",
    "        stripped_line = stripper(line, step_type)\n",
    "        scratch_pad += stripped_line + '\\n'\n",
    "        if step_type == 'observation':\n",
    "            steps.append(scratch_pad.strip())\n",
    "            scratch_pad = ''\n",
    "    if scratch_pad != '':\n",
    "        steps.append(scratch_pad.strip())\n",
    "    return steps\n",
    "\n",
    "reflection_prefix = '\\nReflection:'\n",
    "\n",
    "def PREVIOUS_TRIALS_FORMATTER(reflections: List[str], include_prefix: bool = True) -> str:\n",
    "    if reflections == []:\n",
    "        return ''\n",
    "    if include_prefix:\n",
    "        memory_prefix = \"You have attempted to solve the task before but failed. The following reflection(s) give a plan to avoid failing the task in the same way you did previously. Use them to improve your strategy of solving the task successfully.\"\n",
    "    else:\n",
    "        memory_prefix = ''\n",
    "    memory_prefix += '\\nReflections:'\n",
    "    for reflection in reflections:\n",
    "        memory_prefix += f\"\\n- {reflection.strip()}\"\n",
    "    return memory_prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_critique_num = 8\n",
    "fewshot_strategy = \"task_similarity\"\n",
    "critique_truncate_strategy = \"random\"\n",
    "critique_summary_suffix = dict(full = \"\"\"Focus on REMOVE rules first, and stop ADD rule unless the new rule is VERY insightful and different from EXISTING RULES. Below are the operations you do to the above list of EXISTING RULES:\n",
    "\"\"\", not_full = \"\"\"Below are the operations you do to the above list of EXISTING RULES:\n",
    "\"\"\")\n",
    "testing = True\n",
    "benchmark_name = \"hotpotqa\"\n",
    "reranker = None\n",
    "buffer_retrieve_ratio = 4\n",
    "\n",
    "def token_counter(text: str, llm: str = 'gpt-3.5-turbo', tokenizer: Callable = None) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to be counted.\n",
    "        llm: The language model name.\n",
    "        tokenizer: The tokenizer to be used.\n",
    "    \"\"\"\n",
    "    if 'gpt' in llm:\n",
    "        return len(tiktoken.encoding_for_model(llm).encode(text))\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def get_fewshot_max_tokens(benchmark: str) -> int:\n",
    "    \"\"\"\n",
    "    Gets the maximum number of tokens in the fewshot tasks for the given benchmark.\n",
    "    \n",
    "    Args:\n",
    "        benchmark: The benchmark name.\n",
    "    \n",
    "    Returns:\n",
    "        The maximum number of tokens.\n",
    "    \"\"\"\n",
    "    fewshots = FEWSHOTS[benchmark]\n",
    "    if isinstance(fewshots, dict):\n",
    "        return max([max([token_counter(f) for f in fs]) for fs in fewshots.values()])\n",
    "    elif isinstance(fewshots, list):\n",
    "        return max([token_counter(f) for f in fewshots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    react_agent = AGENT[cfg.agent_type](\n",
    "        name=cfg.ai_name,  \n",
    "        system_instruction=SYSTEM_INSTRUCTION[cfg.benchmark.name],\n",
    "        human_instruction=HUMAN_INSTRUCTION[cfg.benchmark.name],\n",
    "        tasks=INIT_TASKS_FN[cfg.benchmark.name](cfg),\n",
    "        fewshots=FEWSHOTS[cfg.benchmark.name],\n",
    "        system_prompt=system_message_prompt,\n",
    "        env=ENVS[cfg.benchmark.name],     # NEED TO IMPLEMENT\n",
    "        max_steps=cfg.benchmark.max_steps,\n",
    "        openai_api_key=openai_api_key,\n",
    "        llm=cfg.agent.llm,\n",
    "        llm_builder=LLM_CLS,  # NEED TO IMPLEMENT\n",
    "        reflection_fewshots=REFLECTION_FEWSHOTS[cfg.benchmark.name],\n",
    "        reflection_task_prompt=HUMAN_REFLECTION_INSTRUCTION[cfg.benchmark.name],\n",
    "        reflection_system_instruction=SYSTEM_REFLECTION_INSTRUCTION[cfg.benchmark.name],\n",
    "        reflection_system_prompt=SYSTEM_INSTRUCTION[cfg.benchmark.name],\n",
    "        max_relfection_depth=cfg.agent.max_reflection_depth if 'max_reflection_depth' in cfg.agent.keys() else 0,\n",
    "        system_critique_instructions=SYSTEM_CRITIQUE_INSTRUCTION[cfg.benchmark.name],\n",
    "        human_critiques=HUMAN_CRITIQUES,\n",
    "        max_num_rules=cfg.agent.max_num_rules if 'max_num_rules' in cfg.agent.keys() else 0,\n",
    "        rule_template=RULE_TEMPLATE[cfg.benchmark.name],\n",
    "        truncate_strategy=cfg.agent.truncate_strategy if 'truncate_strategy' in cfg.agent.keys() else None,\n",
    "        llm_parser=LLM_PARSER[cfg.benchmark.name],   # FN; NEED TO DOUBLE CHECK!\n",
    "        observation_formatter=OBSERVATION_FORMATTER[cfg.benchmark.name],   # FN; NEED TO DOUBLE CHECK!\n",
    "        embedder=EMBEDDERS(cfg.agent.retrieval_kwargs.embedder_type),  # FN; NEED TO DOUBLE CHECK!\n",
    "        embedder_path=cfg.agent.retrieval_kwargs.embedder_path,\n",
    "        step_stripper=STEP_STRIPPER[cfg.benchmark.name],  # FN; NEED TO DOUBLE CHECK!\n",
    "        retriever_cls=RETRIEVERS(cfg.agent.retrieval_kwargs.retriever_type),  # FN; NEED TO DOUBLE CHECK!\n",
    "        message_splitter=CYCLER[cfg.benchmark.name],  # FN; NEED TO DOUBLE CHECK!\n",
    "        identifier=STEP_IDENTIFIER[cfg.benchmark.name],  # FN; NEED TO DOUBLE CHECK!\n",
    "        message_step_splitter=partial(STEP_CYCLER, benchmark=cfg.benchmark.name),  # FN; NEED TO DOUBLE CHECK!\n",
    "        reflection_prefix=REFLECTION_PREFIX[cfg.benchmark.name],  \n",
    "        previous_trials_formatter=PREVIOUS_TRIALS_FORMATTER[cfg.benchmark.name],  # FN; NEED TO DOUBLE CHECK!\n",
    "        success_critique_num=cfg.agent.success_critique_num,\n",
    "        fewshot_strategy=cfg.agent.fewshot_strategy,\n",
    "        critique_truncate_strategy=cfg.agent.critique_truncate_strategy,\n",
    "        critique_summary_suffix=CRITIQUE_SUMMARY_SUFFIX,\n",
    "        testing=cfg.testing,\n",
    "        task_idx=dicts[-1]['task_idx'] if len(dicts) > 0 else 0,   # NEED TO IMPLEMENT; defaults to 0\n",
    "        benchmark_name=cfg.benchmark.name,\n",
    "        reranker=cfg.agent.retrieval_kwargs.reranker,\n",
    "        buffer_retrieve_ratio=cfg.agent.retrieval_kwargs.buffer_retrieve_ratio,\n",
    "\n",
    "        # NEED TO IMPLEMENT\n",
    "        max_fewshot_tokens=get_fewshot_max_tokens(cfg.benchmark.name) if cfg.agent.retrieval_kwargs.max_fewshot_tokens == 'auto' else cfg.agent.retrieval_kwargs.max_fewshot_tokens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
