{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_num=\"4\"\n",
    "var_num=1\n",
    "env_step_limit=2\n",
    "num_episodes=1\n",
    "gpt_model=\"gpt_4_0613\"\n",
    "summarize_end_of_episode=1\n",
    "device=\"cpu\"\n",
    "temperature=0.0\n",
    "use_gold_memory_in_ep0=0\n",
    "gold_traces=\"\"\n",
    "use_last_k_memories=3\n",
    "quadrant=1\n",
    "simplifications_preset=\"easy\"\n",
    "output_path_prefix = 'logs/testrun/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = None\n",
    "task_num = \"4\"\n",
    "var_num = 1\n",
    "env_step_limit = 100\n",
    "num_episodes = 2\n",
    "gpt_model = \"gpt-4-0613\"\n",
    "summarize_end_of_episode = 1\n",
    "device = None\n",
    "temperature = 0.0\n",
    "use_gold_memory_in_ep0 = 0\n",
    "gold_traces = \"\"\n",
    "use_last_k_memories = 3\n",
    "quadrant = None\n",
    "output_path_prefix = \"save-histories\"\n",
    "simplifications_preset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CLIN agent for taskIdx:4, varIdx:1\n"
     ]
    }
   ],
   "source": [
    "task_num = int(task_num)\n",
    "var_num = int(var_num)\n",
    "print(f\"Running CLIN agent for taskIdx:{task_num}, varIdx:{var_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agential/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/agential/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "exitCommands = [\"quit\", \"exit\"]\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sent_transformer_model = SentenceTransformer('bert-base-nli-mean-tokens', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no gold path -_-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryfname = \"logs/\" + \"summary.txt\"\n",
    "summaryFile = open(summaryfname, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Names: Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring\n"
     ]
    }
   ],
   "source": [
    "task = 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring' \n",
    "print(\"Task Names: \" + str(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CRITIC structured output module.\"\"\"\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agential.agents.base.output import BaseAgentOutput\n",
    "from agential.core.llm import Response\n",
    "\n",
    "\n",
    "class ClinStepOutput(BaseModel):\n",
    "    \"\"\"Critic step Pydantic output class.\n",
    "\n",
    "    Attributes:\n",
    "        answer (str): The answer generated by the agent.\n",
    "        critique (str): The critique of the answer generated by the agent.\n",
    "        external_tool_info (Dict[str, Any]): The query requested by the agent.\n",
    "        answer_response (List[Response]): The answer responses generated by the agent.\n",
    "        critique_response (List[Response]): The critique responses generated by the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(..., description=\"The query requested by the agent.\")\n",
    "    answer: str = Field(..., description=\"The answer generated by the agent.\")\n",
    "    observation: str = Field(..., description=\"The answer's observation.\")\n",
    "    external_tool_info: Dict[str, Any] = Field(\n",
    "        ..., description=\"The external tool outputs.\"\n",
    "    )\n",
    "    answer_response: List[Response] = Field(\n",
    "        ..., description=\"The answer responses generated by the agent.\"\n",
    "    )\n",
    "    observation_response: List[Response] = Field(\n",
    "        ..., description=\"The observation responses generated by the agent.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ClinOutput(BaseAgentOutput):\n",
    "    \"\"\"Critic Pydantic output class.\n",
    "\n",
    "    Attributes:\n",
    "        additional_info (List[CriticStepOutput]): The additional info.\n",
    "    \"\"\"\n",
    "\n",
    "    additional_info: List[ClinStepOutput] = Field(\n",
    "        ..., description=\"The additional info.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scienceworld\n",
      "  Downloading scienceworld-1.2.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting py4j (from scienceworld)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading scienceworld-1.2.1-py3-none-any.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, scienceworld\n",
      "Successfully installed py4j-0.10.9.7 scienceworld-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scienceworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jar_path, env_step_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scienceworld import ScienceWorldEnv\n",
    "\n",
    "env = ScienceWorldEnv(\"\", jar_path, envStepLimit=env_step_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Description: Your task is to boil water. For compounds without a boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/rtd2z3dd2nqg15b702qbw4m40000gn/T/ipykernel_94449/2966492216.py:1: UserWarning: \u001b[91m You are using the camel case api. This feature is deprecated. Please migrate to the snake_case api. \u001b[00m\n",
      "  taskNames = env.getTaskNames()\n",
      "/var/folders/s8/rtd2z3dd2nqg15b702qbw4m40000gn/T/ipykernel_94449/2966492216.py:5: UserWarning: \u001b[91m You are using the camel case api. This feature is deprecated. Please migrate to the snake_case api. \u001b[00m\n",
      "  print(\"Task Description: \" + str(env.getTaskDescription()))\n"
     ]
    }
   ],
   "source": [
    "taskNames = env.getTaskNames()\n",
    "env.load(taskNames[0], 0, \"\")\n",
    "\n",
    "\n",
    "print(\"Task Description: \" + str(env.getTaskDescription()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXTOKENSINHISTORY = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetTokenLength\u001b[39m(strIn):\n\u001b[1;32m      3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(strIn)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenizer'"
     ]
    }
   ],
   "source": [
    "import tokenizer\n",
    "def getTokenLength(strIn):\n",
    "    tokens = tokenizer.encode(strIn)\n",
    "    numTokens = len(tokens)\n",
    "    return numTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "def call_gpt(messages,\n",
    "        model_name,\n",
    "        max_tokens,\n",
    "        temperature):\n",
    "     response = openai.ChatCompletion.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_action(\n",
    "        question , \n",
    "        previous_rationales,\n",
    "        previous_actions,\n",
    "        previous_observations,\n",
    "        current_obs,\n",
    "        model,\n",
    "        summary,\n",
    "        temperature,\n",
    "        quadrant,\n",
    "        episodeIdx\n",
    "):\n",
    "    next_action_query = \"\"\n",
    "\n",
    "    \n",
    "    sw_prompt_task += f\"Below you can see the most recent history of you actions \" \\\n",
    "                      f\"to help you decide your next action.\"\n",
    "\n",
    "    new_messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are an AI agent helping execute a science experiment \"\n",
    "                    \"in a simulated environment with limited number of objects \"\n",
    "                    \"and actions available at each step.\"\n",
    "         },\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"{sw_prompt_task}\"\n",
    "         }\n",
    "    ]\n",
    "\n",
    "    if len(previous_actions):\n",
    "\n",
    "        # Hyperparameter -- the maximum tokens in the history.\n",
    "        maxTokensInHistory = MAXTOKENSINHISTORY\n",
    "\n",
    "        # Create an action history. This is the history of actions and observations that have been taken so far.\n",
    "        # This is used to help the model understand what has happened so far, and what the next action should be.\n",
    "        # This is built from the most-recent to least-recent, and stops when meeting a maximum token threshold.\n",
    "\n",
    "        action_observation_history_str = \"\"\n",
    "        previous_messages = []\n",
    "        # Iterate from the last to the 0th index\n",
    "        for i in range(len(previous_actions) - 1, max(len(previous_actions) - 6, -1) , -1): # we are only taking last three a-o pair to fit to the token length\n",
    "            rationale = previous_rationales[i]\n",
    "            action = previous_actions[i]\n",
    "            observation = previous_observations[i]\n",
    "\n",
    "            # Note we prepend at the start of list\n",
    "            # So we are prepending in reverse order\n",
    "            # observation after ith action, selected ith action, generated rationale for ith action\n",
    "            if i < len(previous_actions) - 1:\n",
    "                previous_messages[:0] = [{\n",
    "                    \"role\": \"user\", \"content\": f\"{observation}\\n\\nWhat action would you like to do next?\"\n",
    "                }]\n",
    "            else:\n",
    "                previous_messages[:0] = [{\n",
    "                    \"role\": \"user\", \"content\": f\"{observation}\"\n",
    "                }]\n",
    "\n",
    "            previous_messages[:0] = [{\n",
    "                \"role\": \"user\", \"content\": f\"Selected action: {action}\"\n",
    "            }]\n",
    "\n",
    "            previous_messages[:0] = [{\n",
    "                \"role\": \"assistant\", \"content\": f\"{rationale}\"\n",
    "            }]\n",
    "\n",
    "            action_observation_history_str_candidate = f\"assistant: {rationale} step: {i}\\n action: {action}\\n observation: {observation}\\n\\n\" + action_observation_history_str\n",
    "\n",
    "            # If we have reached the maximum number of tokens, stop.\n",
    "            if getTokenLength(action_observation_history_str_candidate) > maxTokensInHistory:\n",
    "                break\n",
    "            else:\n",
    "                action_observation_history_str = action_observation_history_str_candidate\n",
    "\n",
    "        for prev_message in previous_messages:\n",
    "            new_messages.append(prev_message)\n",
    "\n",
    "    next_actions_set = [x for x in next_actions_set if 'focus' not in x]\n",
    "    actions_str = '\\n '.join(next_actions_set)\n",
    "\n",
    "    next_action_query += f\"Here is what is the current observation:\" \\\n",
    "                 f\"\\n\" \\\n",
    "                 f\" {current_obs}\" \\\n",
    "                 \n",
    "    next_action_query += f\"\\nYour next action should be in one of the following formats:\" \\\n",
    "                 f\"\\nPossible actions:\" \\\n",
    "                 f\"\\n {actions_str}\" \\\n",
    "                 f\"\\n\\n\" \\\n",
    "                 f\"If I say \\\"Ambiguous request\\\", your action might mean multiple things. In that case, respond with the number corresponding to the action you want to take.\\n\" \\\n",
    "                 f\"\\n\\n\" \\\n",
    "                 f\"What action would you like to do next?\\n\" \\\n",
    "                 f\"First, scan the (unordered) list of learnings, if provided. Decide if any of the learnings are applicable given the last observation to make progress in this task. Then only use selected learnings, if any, to construct a rationale for picking the next action. If no Learning is selected, construct the rationale based on the last observation. Format your response as follows:\\n\" \\\n",
    "                 f\"Write 'I used learning id(s):' as a comma separated list; the list can be empty if no learnings selected. Then, write $$$ followed by the rationale. Finally, write ### followed by the single next action you would like to take.\" \\\n",
    "                 f\"If you think you have completed the task, please write TASK_COMPLETE as the next action.\" \\\n",
    "                 f\"If the task requires you to 'focus' on something (OBJ), please write FOCUS ON <OBJ> as the next action. FOCUS is a extremely critical action that can be only used the number of times 'focus' is mentioned in the task description. Using it more than that or inappropiately (such as on a wrong object) will terminate the session and the task will be rendered as incomplete.\" \\\n",
    "                 f\"If you performed an action that requires waiting to see the effect, please write 'wait' as the next action.\"  \\\n",
    "    \n",
    "    \n",
    "\n",
    "    new_messages.append({\n",
    "        \"role\": \"user\", \"content\": f\"{next_action_query}\"\n",
    "    })\n",
    "\n",
    "    prompt_str = [rec['role'] + \": \" + rec['content'] for rec in new_messages]\n",
    "    # print(\n",
    "        # f\"ChatGPT prompt:\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n{prompt_str}\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    response = call_gpt(\n",
    "        messages=new_messages,\n",
    "        model_name=model,\n",
    "        max_tokens=256,\n",
    "        temperature=temperature,  # 0 for greedy best decoding       # PJ modified from 0.7 to 0.0\n",
    "    )\n",
    "    # print(f\"current_state:{current_obs}\")\n",
    "    # print(f\"=====\\nNEXT ACTION \\ntask:{task}\\n current_state:{current_state}\"\n",
    "    #       f\"\\nobjects_set: {objects_set}\\nnext_actions_set:{next_actions_set}\")\n",
    "    # print(f\"response:{response}\")\n",
    "    # if out_logs_file:\n",
    "    #     out_logs_file.write(f\"next action\\t{prompt_str}\\t{json.dumps(response)}\")\n",
    "\n",
    "    # Sometimes ChatGPT returns a long string with actions mentioned in \"\"\n",
    "    # Extract strings within double quotes\n",
    "\n",
    "    response_str = response['choices'][0]['message']['content']\n",
    "    response['response_str'] = response_str\n",
    "    # print(\"RAW RESPONSE STRING:\")\n",
    "    # print(response_str)\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    # next_actions = re.findall('\"([^\"]*)\"', response_str)\n",
    "    ## If no such quoted actions found, consider entire generation as next action\n",
    "    # if not next_actions:\n",
    "    #    next_actions = [response['choices'][0]['message']['content']]\n",
    "\n",
    "    # actions should be between \"###\" blocks. Take the first one (in case it generated multiple ones) as the next action\n",
    "    possibleActions = response_str.split(\"###\")\n",
    "    reasoningStr = \" \"\n",
    "    next_action_str = \" \"\n",
    "    if len(possibleActions) > 1:\n",
    "        reasoningStr = possibleActions[0]\n",
    "        next_action_str = possibleActions[1].lower().strip()  # The first index should be it's reasoning, the second should be it's action.\n",
    "        # It's possible it might put multiple actions in the next_action_str, that are comma delimited. Trim out all but the first one.\n",
    "        # next_action_str = next_action_str.split(\",\")[0].lower().strip()\n",
    "\n",
    "    # next_action_str = next_actions[0].lower().strip()\n",
    "    next_action_str = next_action_str.replace(\".\", \"\").replace(\"i would like to \", \"\") # .split(' and ')[0]\n",
    "    response['pred_next_action'] = next_action_str\n",
    "\n",
    "    # Check to make sure the reasoningStr and next actions are not blank (to prevent the data structure from crashing with blank strings)\n",
    "    if len(reasoningStr) < 1:\n",
    "        reasoningStr = \" \"\n",
    "    if (len(next_action_str) < 1):\n",
    "        next_action_str = \" UNKNOWN \"\n",
    "\n",
    "    response['reasoningStr'] = reasoningStr\n",
    "\n",
    "    # Append ChatGPT response and our action selection to message history\n",
    "    new_messages.append({\n",
    "        \"role\": \"assistant\", \"content\": response_str})\n",
    "    new_messages.append({\n",
    "        \"role\": \"user\", \"content\": f\"Selected action: {next_action_str}\"\n",
    "    })\n",
    "    # print(f\"pred_next_action:{next_actions[0]}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from agential.utils.parse import remove_newline\n",
    "\n",
    "\n",
    "def generate_observation(\n",
    "        self, idx: int, scratchpad: str, action_type: str, query: str\n",
    "    ) -> Tuple[str, str, str, bool, Dict[str, Any]]:\n",
    "        \"\"\"Generates an observation based on the provided action type and query.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the current observation.\n",
    "            scratchpad (str): The current state of the scratchpad.\n",
    "            action_type (str): The type of action performed (e.g. \"search\", \"lookup\", \"finish\").\n",
    "            query (str): The query for the action.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, str, str, bool, Dict[str, Any]]: The updated scratchpad, the answer, the observation, a flag indicating if the task is finished, and a dictionary containing external tool information.\n",
    "        \"\"\"\n",
    "        answer = \"\"\n",
    "        finished = False\n",
    "        external_tool_info = {\"search_result\": \"\", \"lookup_result\": \"\"}\n",
    "\n",
    "        scratchpad += f\"\\nObservation {idx}: \"\n",
    "        if action_type.lower() == \"finish\":\n",
    "            answer = query\n",
    "            finished = True\n",
    "            obs = query\n",
    "        elif action_type.lower() == \"search\":\n",
    "            try:\n",
    "                search_result = self.docstore.search(query)\n",
    "                external_tool_info[\"search_result\"] = search_result\n",
    "                obs = remove_newline(search_result)\n",
    "            except Exception:\n",
    "                obs = \"Could not find that page, please try again.\"\n",
    "        elif action_type.lower() == \"lookup\":\n",
    "            try:\n",
    "                lookup_result = self.docstore.lookup(query)\n",
    "                external_tool_info[\"lookup_result\"] = lookup_result\n",
    "                obs = remove_newline(lookup_result)\n",
    "\n",
    "            except ValueError:\n",
    "                obs = \"The last page Searched was not found, so you cannot Lookup a keyword in it. Please try one of the similar pages given.\"\n",
    "        else:\n",
    "            obs = \"Invalid Action. Valid Actions are Lookup[<topic>] Search[<topic>] and Finish[<answer>].\"\n",
    "        scratchpad += obs\n",
    "\n",
    "        return scratchpad, answer, obs, finished, external_tool_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from agential.utils.docstore import DocstoreExplorer\n",
    "from tiktoken.core import Encoding\n",
    "from agential.core.llm import BaseLLM\n",
    "from langchain_community.docstore.wikipedia import Wikipedia\n",
    "\n",
    "class ClinQAStrategy():\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: BaseLLM,\n",
    "        max_iteration: int = 6,\n",
    "        max_tokens: int = 5000,\n",
    "        enc: Encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    "        docstore: DocstoreExplorer = DocstoreExplorer(Wikipedia()),\n",
    "        testing: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.max_iteration = max_iteration\n",
    "        self.max_tokens = max_tokens\n",
    "        self.enc = enc\n",
    "        self.docstore = docstore\n",
    "        self.testing = testing\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        question: str,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "\n",
    "    ): \n",
    "   \n",
    "        summary = \"\"\n",
    "\n",
    "        prev_episode_summary_str = \"\"\n",
    "\n",
    "        previous_actions = []\n",
    "        previous_observations = []\n",
    "        previous_rationales = []\n",
    "        # Start running episodes\n",
    "        for episodeIdx in range(0, self.max_iteration):\n",
    "            # Run histories\n",
    "            runHistories = []\n",
    "            score = 0.0\n",
    "            score_positive = 0.0\n",
    "            isCompleted = False\n",
    "            curIter = 0\n",
    "            observation = \"\"\n",
    "            for i in range(0, self.max_iteration):\n",
    "                \n",
    "                # Generate actio\n",
    "                action = generate_action(\n",
    "                    question , \n",
    "                    previous_rationales,\n",
    "                    previous_actions,\n",
    "                    previous_observations,\n",
    "                    observation,\n",
    "                    model,\n",
    "                    summary,\n",
    "                    temperature,\n",
    "                    quadrant,\n",
    "                    episodeIdx\n",
    "                )\n",
    "                \n",
    "                generated_rationale_str = action['response_str']\n",
    "                generated_action_str = action[\"pred_next_action\"].lower().strip()\n",
    "                print(\"GPT4 generated action: \" + str(generated_action_str))\n",
    "                observation , reward , isCompleted , info = generate_observation(question , action , summary)\n",
    "\n",
    "                if isCompleted:\n",
    "                    break\n",
    "                       \n",
    "                # Reasoning rationale\n",
    "                print(\"Reasoning rationale: \" + str(generated_rationale_str))\n",
    "                previous_rationales.append(action[\"reasoningStr\"])\n",
    "\n",
    "                print(\"Action: \" + str(action[\"pred_next_action\"]))\n",
    "                previous_actions.append(generated_action_str)\n",
    "\n",
    "                print(\"Observation: \" + str(observation))\n",
    "                previous_observations.append(observation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.agents.base.agent import BaseAgent\n",
    "from agential.core.llm import BaseLLM\n",
    "\n",
    "class Clin(BaseAgent):\n",
    "    \"\"\"Clin Agent.\n",
    "\n",
    "    Attributes:\n",
    "        llm (BaseLLM): An instance of a language model used for generating initial answers\n",
    "            and critiques.\n",
    "        benchmark (str): The benchmark.\n",
    "        testing (bool): Whether to run in testing mode. Defaults to False.\n",
    "        **strategy_kwargs (Any): Additional strategy-specific arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: BaseLLM,\n",
    "        benchmark: str,\n",
    "        testing: bool = False,\n",
    "        **strategy_kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super().__init__(llm=llm, benchmark=benchmark, testing=testing)\n",
    "\n",
    "        self.strategy = Clin.get_strategy(\n",
    "            benchmark=self.benchmark, llm=self.llm, testing=testing, **strategy_kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_variations_to_run = 1\n",
    "for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
