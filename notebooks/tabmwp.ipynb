{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"output\"\n",
    "method_name = \"standard\"\n",
    "benchmark = \"tabmwp\"\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "eval_model = \"gpt-4o-mini\"\n",
    "seed = 42\n",
    "num_retries = 1\n",
    "warming = [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.prompting.standard.prompting import Standard\n",
    "method = Standard(\n",
    "        llm=llm,\n",
    "        benchmark=benchmark,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.core.llm import LLM\n",
    "import os\n",
    "llm = LLM(\n",
    "        model, \n",
    "        api_key = open_api,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        seed=seed\n",
    ")\n",
    "eval_llm = LLM(\n",
    "    eval_model,\n",
    "    api_key = open_api,\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "data = load_dataset(\"Arietem/tabmwp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.eval.metrics.classification import f1, fuzzy_EM, llm_as_judge_eval, precision, recall, EM\n",
    "import re\n",
    "\n",
    "# Initialize score lists\n",
    "em_scores = []\n",
    "\n",
    "# Iterate through the first 10 instances in the training data\n",
    "for index in range(min(len(data['train']['question']), 10)):\n",
    "    # Extract data for the current instance\n",
    "    question = data['train']['question'][index]\n",
    "    table_title = data['train']['table_title'][index]\n",
    "    table = data['train']['table'][index]\n",
    "    answer = data['train'][\"answer\"][index]\n",
    "\n",
    "    # Clean up the answer\n",
    "    answer = str(answer).replace(',', '')  # Remove commas, convert to string\n",
    "\n",
    "    # Combine question components\n",
    "    question = '\\n'.join(filter(None, [question, table_title, table]))\n",
    "\n",
    "    # Inference\n",
    "    out = method.generate(\n",
    "        question=question,\n",
    "        key=answer,\n",
    "        num_retries=num_retries,\n",
    "        warming=warming\n",
    "    )\n",
    "\n",
    "    # Process the output\n",
    "    code_str = out.answer.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    pred_answers, _ = safe_execute(code_string=code_str)\n",
    "    pred_answer = str(pred_answers[0]).lower()\n",
    "\n",
    "    # Determine the final predicted answer\n",
    "    if any(word in pred_answer for word in [\"yes\", \"true\"]):\n",
    "        pred_answer = \"yes\"\n",
    "    elif any(word in pred_answer for word in [\"no\", \"false\"]):\n",
    "        pred_answer = \"no\"\n",
    "\n",
    "    print(\"Predicted answer:\", pred_answer)\n",
    "    print(\"Actual answer:\", answer)\n",
    "\n",
    "    # Evaluate correctness\n",
    "    is_correct = int(EM(pred_answer, answer, is_numeric=True))\n",
    "    em_scores.append(is_correct)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
