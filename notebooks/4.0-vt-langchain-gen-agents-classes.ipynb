{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/langchain-ai/langchain/tree/master/libs/experimental/langchain_experimental/generative_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import BaseMemory, Document\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.utils import mock_now\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class GenerativeAgentMemory(BaseMemory):\n",
    "    \"\"\"Memory for the generative agent.\"\"\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    \"\"\"The core language model.\"\"\"\n",
    "    memory_retriever: TimeWeightedVectorStoreRetriever\n",
    "    \"\"\"The retriever to fetch related memories.\"\"\"\n",
    "    verbose: bool = False\n",
    "    reflection_threshold: Optional[float] = None\n",
    "    \"\"\"When aggregate_importance exceeds reflection_threshold, stop to reflect.\"\"\"\n",
    "    current_plan: List[str] = []\n",
    "    \"\"\"The current plan of the agent.\"\"\"\n",
    "    # A weight of 0.15 makes this less important than it\n",
    "    # would be otherwise, relative to salience and time\n",
    "    importance_weight: float = 0.15\n",
    "    \"\"\"How much weight to assign the memory importance.\"\"\"\n",
    "    aggregate_importance: float = 0.0  # : :meta private:\n",
    "    \"\"\"Track the sum of the 'importance' of recent memories.\n",
    "\n",
    "    Triggers reflection when it reaches reflection_threshold.\"\"\"\n",
    "\n",
    "    max_tokens_limit: int = 1200  # : :meta private:\n",
    "    # input keys\n",
    "    queries_key: str = \"queries\"\n",
    "    most_recent_memories_token_key: str = \"recent_memories_token\"\n",
    "    add_memory_key: str = \"add_memory\"\n",
    "    # output keys\n",
    "    relevant_memories_key: str = \"relevant_memories\"\n",
    "    relevant_memories_simple_key: str = \"relevant_memories_simple\"\n",
    "    most_recent_memories_key: str = \"most_recent_memories\"\n",
    "    now_key: str = \"now\"\n",
    "    reflecting: bool = False\n",
    "\n",
    "    def chain(self, prompt: PromptTemplate) -> LLMChain:\n",
    "        return LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_list(text: str) -> List[str]:\n",
    "        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n",
    "        lines = re.split(r\"\\n\", text.strip())\n",
    "        lines = [line for line in lines if line.strip()]  # remove empty lines\n",
    "        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n",
    "\n",
    "    def _get_topics_of_reflection(self, last_k: int = 50) -> List[str]:\n",
    "        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"{observations}\\n\\n\"\n",
    "            \"Given only the information above, what are the 3 most salient \"\n",
    "            \"high-level questions we can answer about the subjects in the statements?\\n\"\n",
    "            \"Provide each question on a new line.\"\n",
    "        )\n",
    "        observations = self.memory_retriever.memory_stream[-last_k:]\n",
    "        observation_str = \"\\n\".join(\n",
    "            [self._format_memory_detail(o) for o in observations]\n",
    "        )\n",
    "        result = self.chain(prompt).run(observations=observation_str)\n",
    "        return self._parse_list(result)\n",
    "\n",
    "    def _get_insights_on_topic(\n",
    "        self, topic: str, now: Optional[datetime] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"Statements relevant to: '{topic}'\\n\"\n",
    "            \"---\\n\"\n",
    "            \"{related_statements}\\n\"\n",
    "            \"---\\n\"\n",
    "            \"What 5 high-level novel insights can you infer from the above statements \"\n",
    "            \"that are relevant for answering the following question?\\n\"\n",
    "            \"Do not include any insights that are not relevant to the question.\\n\"\n",
    "            \"Do not repeat any insights that have already been made.\\n\\n\"\n",
    "            \"Question: {topic}\\n\\n\"\n",
    "            \"(example format: insight (because of 1, 5, 3))\\n\"\n",
    "        )\n",
    "\n",
    "        related_memories = self.fetch_memories(topic, now=now)\n",
    "        related_statements = \"\\n\".join(\n",
    "            [\n",
    "                self._format_memory_detail(memory, prefix=f\"{i+1}. \")\n",
    "                for i, memory in enumerate(related_memories)\n",
    "            ]\n",
    "        )\n",
    "        result = self.chain(prompt).run(\n",
    "            topic=topic, related_statements=related_statements\n",
    "        )\n",
    "        # TODO: Parse the connections between memories and insights\n",
    "        return self._parse_list(result)\n",
    "\n",
    "    def pause_to_reflect(self, now: Optional[datetime] = None) -> List[str]:\n",
    "        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n",
    "        if self.verbose:\n",
    "            logger.info(\"Character is reflecting\")\n",
    "        new_insights = []\n",
    "        topics = self._get_topics_of_reflection()\n",
    "        for topic in topics:\n",
    "            insights = self._get_insights_on_topic(topic, now=now)\n",
    "            for insight in insights:\n",
    "                self.add_memory(insight, now=now)\n",
    "            new_insights.extend(insights)\n",
    "        return new_insights\n",
    "\n",
    "    def _score_memory_importance(self, memory_content: str) -> float:\n",
    "        \"\"\"Score the absolute importance of the given memory.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"On the scale of 1 to 10, where 1 is purely mundane\"\n",
    "            + \" (e.g., brushing teeth, making bed) and 10 is\"\n",
    "            + \" extremely poignant (e.g., a break up, college\"\n",
    "            + \" acceptance), rate the likely poignancy of the\"\n",
    "            + \" following piece of memory. Respond with a single integer.\"\n",
    "            + \"\\nMemory: {memory_content}\"\n",
    "            + \"\\nRating: \"\n",
    "        )\n",
    "        score = self.chain(prompt).run(memory_content=memory_content).strip()\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Importance score: {score}\")\n",
    "        match = re.search(r\"^\\D*(\\d+)\", score)\n",
    "        if match:\n",
    "            return (float(match.group(1)) / 10) * self.importance_weight\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # `memory_content` -> format_memories_simple.\n",
    "    def _score_memories_importance(self, memory_content: str) -> List[float]:\n",
    "        \"\"\"Score the absolute importance of the given memory.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"On the scale of 1 to 10, where 1 is purely mundane\"\n",
    "            + \" (e.g., brushing teeth, making bed) and 10 is\"\n",
    "            + \" extremely poignant (e.g., a break up, college\"\n",
    "            + \" acceptance), rate the likely poignancy of the\"\n",
    "            + \" following piece of memory. Always answer with only a list of numbers.\"\n",
    "            + \" If just given one memory still respond in a list.\"\n",
    "            + \" Memories are separated by semi colons (;)\"\n",
    "            + \"\\Memories: {memory_content}\"\n",
    "            + \"\\nRating: \"\n",
    "        )\n",
    "        scores = self.chain(prompt).run(memory_content=memory_content).strip()\n",
    "\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Importance scores: {scores}\")\n",
    "\n",
    "        # Split into list of strings and convert to floats\n",
    "        scores_list = [float(x) for x in scores.split(\";\")]\n",
    "\n",
    "        return scores_list\n",
    "\n",
    "    def add_memories(\n",
    "        self, memory_content: str, now: Optional[datetime] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Add an observations or memories to the agent's memory.\"\"\"\n",
    "        importance_scores = self._score_memories_importance(memory_content)\n",
    "\n",
    "        self.aggregate_importance += max(importance_scores)\n",
    "        memory_list = memory_content.split(\";\")\n",
    "        documents = []\n",
    "\n",
    "        for i in range(len(memory_list)):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=memory_list[i],\n",
    "                    metadata={\"importance\": importance_scores[i]},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        result = self.memory_retriever.add_documents(documents, current_time=now)\n",
    "\n",
    "        # After an agent has processed a certain amount of memories (as measured by\n",
    "        # aggregate importance), it is time to reflect on recent events to add\n",
    "        # more synthesized memories to the agent's memory stream.\n",
    "        if (\n",
    "            self.reflection_threshold is not None\n",
    "            and self.aggregate_importance > self.reflection_threshold\n",
    "            and not self.reflecting\n",
    "        ):\n",
    "            self.reflecting = True\n",
    "            self.pause_to_reflect(now=now)\n",
    "            # Hack to clear the importance from reflection\n",
    "            self.aggregate_importance = 0.0\n",
    "            self.reflecting = False\n",
    "        return result\n",
    "\n",
    "    def add_memory(\n",
    "        self, memory_content: str, now: Optional[datetime] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n",
    "        importance_score = self._score_memory_importance(memory_content)\n",
    "        self.aggregate_importance += importance_score\n",
    "        document = Document(\n",
    "            page_content=memory_content, metadata={\"importance\": importance_score}\n",
    "        )\n",
    "        result = self.memory_retriever.add_documents([document], current_time=now)\n",
    "\n",
    "        # After an agent has processed a certain amount of memories (as measured by\n",
    "        # aggregate importance), it is time to reflect on recent events to add\n",
    "        # more synthesized memories to the agent's memory stream.\n",
    "        if (\n",
    "            self.reflection_threshold is not None\n",
    "            and self.aggregate_importance > self.reflection_threshold\n",
    "            and not self.reflecting\n",
    "        ):\n",
    "            self.reflecting = True\n",
    "            self.pause_to_reflect(now=now)\n",
    "            # Hack to clear the importance from reflection\n",
    "            self.aggregate_importance = 0.0\n",
    "            self.reflecting = False\n",
    "        return result\n",
    "\n",
    "    def fetch_memories(\n",
    "        self, observation: str, now: Optional[datetime] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Fetch related memories.\"\"\"\n",
    "        if now is not None:\n",
    "            with mock_now(now):\n",
    "                return self.memory_retriever.get_relevant_documents(observation)\n",
    "        else:\n",
    "            return self.memory_retriever.get_relevant_documents(observation)\n",
    "\n",
    "    def format_memories_detail(self, relevant_memories: List[Document]) -> str:\n",
    "        content = []\n",
    "        for mem in relevant_memories:\n",
    "            content.append(self._format_memory_detail(mem, prefix=\"- \"))\n",
    "        return \"\\n\".join([f\"{mem}\" for mem in content])\n",
    "\n",
    "    def _format_memory_detail(self, memory: Document, prefix: str = \"\") -> str:\n",
    "        created_time = memory.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "        return f\"{prefix}[{created_time}] {memory.page_content.strip()}\"\n",
    "\n",
    "    def format_memories_simple(self, relevant_memories: List[Document]) -> str:\n",
    "        return \"; \".join([f\"{mem.page_content}\" for mem in relevant_memories])\n",
    "\n",
    "    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n",
    "        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n",
    "        result = []\n",
    "        for doc in self.memory_retriever.memory_stream[::-1]:\n",
    "            if consumed_tokens >= self.max_tokens_limit:\n",
    "                break\n",
    "            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n",
    "            if consumed_tokens < self.max_tokens_limit:\n",
    "                result.append(doc)\n",
    "        return self.format_memories_simple(result)\n",
    "\n",
    "    @property\n",
    "    def memory_variables(self) -> List[str]:\n",
    "        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n",
    "        return []\n",
    "\n",
    "    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n",
    "        queries = inputs.get(self.queries_key)\n",
    "        now = inputs.get(self.now_key)\n",
    "        if queries is not None:\n",
    "            relevant_memories = [\n",
    "                mem for query in queries for mem in self.fetch_memories(query, now=now)\n",
    "            ]\n",
    "            return {\n",
    "                self.relevant_memories_key: self.format_memories_detail(\n",
    "                    relevant_memories\n",
    "                ),\n",
    "                self.relevant_memories_simple_key: self.format_memories_simple(\n",
    "                    relevant_memories\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        most_recent_memories_token = inputs.get(self.most_recent_memories_token_key)\n",
    "        if most_recent_memories_token is not None:\n",
    "            return {\n",
    "                self.most_recent_memories_key: self._get_memories_until_limit(\n",
    "                    most_recent_memories_token\n",
    "                )\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save the context of this model run to memory.\"\"\"\n",
    "        # TODO: fix the save memory key\n",
    "        mem = outputs.get(self.add_memory_key)\n",
    "        now = outputs.get(self.now_key)\n",
    "        if mem:\n",
    "            self.add_memory(mem, now=now)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear memory contents.\"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "\n",
    "from langchain_experimental.generative_agents.memory import GenerativeAgentMemory\n",
    "from langchain_experimental.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class GenerativeAgent(BaseModel):\n",
    "    \"\"\"An Agent as a character with memory and innate characteristics.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    \"\"\"The character's name.\"\"\"\n",
    "    age: Optional[int] = None\n",
    "    \"\"\"The optional age of the character.\"\"\"\n",
    "    traits: str = \"N/A\"\n",
    "    \"\"\"Permanent traits to ascribe to the character.\"\"\"\n",
    "    status: str\n",
    "    \"\"\"The traits of the character you wish not to change.\"\"\"\n",
    "    memory: GenerativeAgentMemory\n",
    "    \"\"\"The memory object that combines relevance, recency, and 'importance'.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    \"\"\"The underlying language model.\"\"\"\n",
    "    verbose: bool = False\n",
    "    summary: str = \"\"  #: :meta private:\n",
    "    \"\"\"Stateful self-summary generated via reflection on the character's memory.\"\"\"\n",
    "    summary_refresh_seconds: int = 3600  #: :meta private:\n",
    "    \"\"\"How frequently to re-generate the summary.\"\"\"\n",
    "    last_refreshed: datetime = Field(default_factory=datetime.now)  # : :meta private:\n",
    "    \"\"\"The last time the character's summary was regenerated.\"\"\"\n",
    "    daily_summaries: List[str] = Field(default_factory=list)  # : :meta private:\n",
    "    \"\"\"Summary of the events in the plan that the agent took.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    # LLM-related methods\n",
    "    @staticmethod\n",
    "    def _parse_list(text: str) -> List[str]:\n",
    "        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n",
    "        lines = re.split(r\"\\n\", text.strip())\n",
    "        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n",
    "\n",
    "    def chain(self, prompt: PromptTemplate) -> LLMChain:\n",
    "        return LLMChain(\n",
    "            llm=self.llm, prompt=prompt, verbose=self.verbose, memory=self.memory\n",
    "        )\n",
    "\n",
    "    def _get_entity_from_observation(self, observation: str) -> str:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"What is the observed entity in the following observation? {observation}\"\n",
    "            + \"\\nEntity=\"\n",
    "        )\n",
    "        return self.chain(prompt).run(observation=observation).strip()\n",
    "\n",
    "    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"What is the {entity} doing in the following observation? {observation}\"\n",
    "            + \"\\nThe {entity} is\"\n",
    "        )\n",
    "        return (\n",
    "            self.chain(prompt).run(entity=entity_name, observation=observation).strip()\n",
    "        )\n",
    "\n",
    "    def summarize_related_memories(self, observation: str) -> str:\n",
    "        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            {q1}?\n",
    "            Context from memory:\n",
    "            {relevant_memories}\n",
    "            Relevant context: \n",
    "            \"\"\"\n",
    "        )\n",
    "        entity_name = self._get_entity_from_observation(observation)\n",
    "        entity_action = self._get_entity_action(observation, entity_name)\n",
    "        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\n",
    "        q2 = f\"{entity_name} is {entity_action}\"\n",
    "        return self.chain(prompt=prompt).run(q1=q1, queries=[q1, q2]).strip()\n",
    "\n",
    "    def _generate_reaction(\n",
    "        self, observation: str, suffix: str, now: Optional[datetime] = None\n",
    "    ) -> str:\n",
    "        \"\"\"React to a given observation or dialogue act.\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"{agent_summary_description}\"\n",
    "            + \"\\nIt is {current_time}.\"\n",
    "            + \"\\n{agent_name}'s status: {agent_status}\"\n",
    "            + \"\\nSummary of relevant context from {agent_name}'s memory:\"\n",
    "            + \"\\n{relevant_memories}\"\n",
    "            + \"\\nMost recent observations: {most_recent_memories}\"\n",
    "            + \"\\nObservation: {observation}\"\n",
    "            + \"\\n\\n\"\n",
    "            + suffix\n",
    "        )\n",
    "        agent_summary_description = self.get_summary(now=now)\n",
    "        relevant_memories_str = self.summarize_related_memories(observation)\n",
    "        current_time_str = (\n",
    "            datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "            if now is None\n",
    "            else now.strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "        )\n",
    "        kwargs: Dict[str, Any] = dict(\n",
    "            agent_summary_description=agent_summary_description,\n",
    "            current_time=current_time_str,\n",
    "            relevant_memories=relevant_memories_str,\n",
    "            agent_name=self.name,\n",
    "            observation=observation,\n",
    "            agent_status=self.status,\n",
    "        )\n",
    "        consumed_tokens = self.llm.get_num_tokens(\n",
    "            prompt.format(most_recent_memories=\"\", **kwargs)\n",
    "        )\n",
    "        kwargs[self.memory.most_recent_memories_token_key] = consumed_tokens\n",
    "        return self.chain(prompt=prompt).run(**kwargs).strip()\n",
    "\n",
    "    def _clean_response(self, text: str) -> str:\n",
    "        return re.sub(f\"^{self.name} \", \"\", text.strip()).strip()\n",
    "\n",
    "    def generate_reaction(\n",
    "        self, observation: str, now: Optional[datetime] = None\n",
    "    ) -> Tuple[bool, str]:\n",
    "        \"\"\"React to a given observation.\"\"\"\n",
    "        call_to_action_template = (\n",
    "            \"Should {agent_name} react to the observation, and if so,\"\n",
    "            + \" what would be an appropriate reaction? Respond in one line.\"\n",
    "            + ' If the action is to engage in dialogue, write:\\nSAY: \"what to say\"'\n",
    "            + \"\\notherwise, write:\\nREACT: {agent_name}'s reaction (if anything).\"\n",
    "            + \"\\nEither do nothing, react, or say something but not both.\\n\\n\"\n",
    "        )\n",
    "        full_result = self._generate_reaction(\n",
    "            observation, call_to_action_template, now=now\n",
    "        )\n",
    "        result = full_result.strip().split(\"\\n\")[0]\n",
    "        # AAA\n",
    "        self.memory.save_context(\n",
    "            {},\n",
    "            {\n",
    "                self.memory.add_memory_key: f\"{self.name} observed \"\n",
    "                f\"{observation} and reacted by {result}\",\n",
    "                self.memory.now_key: now,\n",
    "            },\n",
    "        )\n",
    "        if \"REACT:\" in result:\n",
    "            reaction = self._clean_response(result.split(\"REACT:\")[-1])\n",
    "            return False, f\"{self.name} {reaction}\"\n",
    "        if \"SAY:\" in result:\n",
    "            said_value = self._clean_response(result.split(\"SAY:\")[-1])\n",
    "            return True, f\"{self.name} said {said_value}\"\n",
    "        else:\n",
    "            return False, result\n",
    "\n",
    "    def generate_dialogue_response(\n",
    "        self, observation: str, now: Optional[datetime] = None\n",
    "    ) -> Tuple[bool, str]:\n",
    "        \"\"\"React to a given observation.\"\"\"\n",
    "        call_to_action_template = (\n",
    "            \"What would {agent_name} say? To end the conversation, write:\"\n",
    "            ' GOODBYE: \"what to say\". Otherwise to continue the conversation,'\n",
    "            ' write: SAY: \"what to say next\"\\n\\n'\n",
    "        )\n",
    "        full_result = self._generate_reaction(\n",
    "            observation, call_to_action_template, now=now\n",
    "        )\n",
    "        result = full_result.strip().split(\"\\n\")[0]\n",
    "        if \"GOODBYE:\" in result:\n",
    "            farewell = self._clean_response(result.split(\"GOODBYE:\")[-1])\n",
    "            self.memory.save_context(\n",
    "                {},\n",
    "                {\n",
    "                    self.memory.add_memory_key: f\"{self.name} observed \"\n",
    "                    f\"{observation} and said {farewell}\",\n",
    "                    self.memory.now_key: now,\n",
    "                },\n",
    "            )\n",
    "            return False, f\"{self.name} said {farewell}\"\n",
    "        if \"SAY:\" in result:\n",
    "            response_text = self._clean_response(result.split(\"SAY:\")[-1])\n",
    "            self.memory.save_context(\n",
    "                {},\n",
    "                {\n",
    "                    self.memory.add_memory_key: f\"{self.name} observed \"\n",
    "                    f\"{observation} and said {response_text}\",\n",
    "                    self.memory.now_key: now,\n",
    "                },\n",
    "            )\n",
    "            return True, f\"{self.name} said {response_text}\"\n",
    "        else:\n",
    "            return False, result\n",
    "\n",
    "    ######################################################\n",
    "    # Agent stateful' summary methods.                   #\n",
    "    # Each dialog or response prompt includes a header   #\n",
    "    # summarizing the agent's self-description. This is  #\n",
    "    # updated periodically through probing its memories  #\n",
    "    ######################################################\n",
    "    def _compute_agent_summary(self) -> str:\n",
    "        \"\"\"\"\"\"\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"How would you summarize {name}'s core characteristics given the\"\n",
    "            + \" following statements:\\n\"\n",
    "            + \"{relevant_memories}\"\n",
    "            + \"Do not embellish.\"\n",
    "            + \"\\n\\nSummary: \"\n",
    "        )\n",
    "        # The agent seeks to think about their core characteristics.\n",
    "        return (\n",
    "            self.chain(prompt)\n",
    "            .run(name=self.name, queries=[f\"{self.name}'s core characteristics\"])\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "    def get_summary(\n",
    "        self, force_refresh: bool = False, now: Optional[datetime] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Return a descriptive summary of the agent.\"\"\"\n",
    "        current_time = datetime.now() if now is None else now\n",
    "        since_refresh = (current_time - self.last_refreshed).seconds\n",
    "        if (\n",
    "            not self.summary\n",
    "            or since_refresh >= self.summary_refresh_seconds\n",
    "            or force_refresh\n",
    "        ):\n",
    "            self.summary = self._compute_agent_summary()\n",
    "            self.last_refreshed = current_time\n",
    "        age = self.age if self.age is not None else \"N/A\"\n",
    "        return (\n",
    "            f\"Name: {self.name} (age: {age})\"\n",
    "            + f\"\\nInnate traits: {self.traits}\"\n",
    "            + f\"\\n{self.summary}\"\n",
    "        )\n",
    "\n",
    "    def get_full_header(\n",
    "        self, force_refresh: bool = False, now: Optional[datetime] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n",
    "        now = datetime.now() if now is None else now\n",
    "        summary = self.get_summary(force_refresh=force_refresh, now=now)\n",
    "        current_time_str = now.strftime(\"%B %d, %Y, %I:%M %p\")\n",
    "        return (\n",
    "            f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This will differ depending on a few things:\n",
    "    # - the distance / similarity metric used by the VectorStore\n",
    "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
    "    # This function converts the euclidean norm of normalized embeddings\n",
    "    # (0 is most similar, sqrt(2) most dissimilar)\n",
    "    # to a similarity function (0 to 1)\n",
    "    return 1.0 - score / math.sqrt(2)\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    embedding_size = 1536\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(\n",
    "        embeddings_model.embed_query,\n",
    "        index,\n",
    "        InMemoryDocstore({}),\n",
    "        {},\n",
    "        relevance_score_fn=relevance_score_fn,\n",
    "    )\n",
    "    return TimeWeightedVectorStoreRetriever(\n",
    "        vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15\n",
    "    )\n",
    "\n",
    "USER_NAME = \"Person A\"  # The name you want to use when interviewing the agent.\n",
    "LLM = ChatOpenAI(max_tokens=1500)  # Can be any LLM you want.\n",
    "\n",
    "tommies_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "tommie = GenerativeAgent(\n",
    "    name=\"Tommie\",\n",
    "    age=25,\n",
    "    traits=\"anxious, likes design, talkative\",  # You can add more persistent traits here\n",
    "    status=\"looking for a job\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=tommies_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Tommie (age: 25)\n",
      "Innate traits: anxious, likes design, talkative\n",
      "Tommie is a straightforward and honest individual who values authenticity and simplicity. They also have a strong sense of self and are not easily influenced by external factors or opinions.\n"
     ]
    }
   ],
   "source": [
    "# The current \"Summary\" of a character can't be made because the agent hasn't made\n",
    "# any observations yet.\n",
    "print(tommie.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add memories directly to the memory object\n",
    "tommie_observations = [\n",
    "    \"Tommie remembers his dog, Bruno, from when he was a kid\",\n",
    "    \"Tommie feels tired from driving so far\",\n",
    "    \"Tommie sees the new home\",\n",
    "    \"The new neighbors have a cat\",\n",
    "    \"The road is noisy at night\",\n",
    "    \"Tommie is hungry\",\n",
    "    \"Tommie tries to get some rest.\",\n",
    "]\n",
    "for observation in tommie_observations:\n",
    "    tommie.memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discussion-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
