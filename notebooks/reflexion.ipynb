{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.cog.agent.reflexion import ReflexionCoTAgent, ReflexionReActAgent\n",
    "from agential.cog.prompts.agent.reflexion import (\n",
    "    REFLEXION_COT_INSTRUCTION_HOTPOTQA,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    REFLEXION_COT_REFLECT_INSTRUCTION_HOTPOTQA,\n",
    "    REFLEXION_REACT_INSTRUCTION_HOTPOTQA,\n",
    "    REFLEXION_REACT_REFLECT_INSTRUCTION_HOTPOTQA,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "\n",
    "    FEVER_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    FEVER_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "    REFLEXION_COT_INSTRUCTION_FEVER,\n",
    "    REFLEXION_REACT_INSTRUCTION_FEVER,\n",
    "    REFLEXION_COT_REFLECT_INSTRUCTION_FEVER,\n",
    "    REFLEXION_REACT_REFLECT_INSTRUCTION_FEVER,\n",
    "\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "    REFLEXION_COT_INSTRUCTION_AMBIGNQ,\n",
    "    REFLEXION_REACT_INSTRUCTION_AMBIGNQ,\n",
    "    REFLEXION_COT_REFLECT_INSTRUCTION_AMBIGNQ,\n",
    "    REFLEXION_REACT_REFLECT_INSTRUCTION_AMBIGNQ,\n",
    "\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "    REFLEXION_COT_INSTRUCTION_TRIVIAQA,\n",
    "    REFLEXION_REACT_INSTRUCTION_TRIVIAQA,\n",
    "    REFLEXION_COT_REFLECT_INSTRUCTION_TRIVIAQA,\n",
    "    REFLEXION_REACT_REFLECT_INSTRUCTION_TRIVIAQA,\n",
    "\n",
    "    GSM8K_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    GSM8K_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "    REFLEXION_COT_INSTRUCTION_GSM8K,\n",
    "    REFLEXION_REACT_INSTRUCTION_GSM8K,\n",
    "    REFLEXION_COT_REFLECT_INSTRUCTION_GSM8K,\n",
    "    REFLEXION_REACT_REFLECT_INSTRUCTION_GSM8K\n",
    ")\n",
    "from agential.cog.prompts.benchmark.hotpotqa import (\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_COT,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.fever import (\n",
    "    FEVER_FEWSHOT_EXAMPLES_COT,\n",
    "    FEVER_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.triviaqa import (\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_COT,\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.ambignq import (\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_COT,\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.gsm8k import (\n",
    "    GSM8K_FEWSHOT_EXAMPLES_COT\n",
    ")\n",
    "\n",
    "import tiktoken\n",
    "from langchain_community.docstore.wikipedia import Wikipedia\n",
    "from agential.utils.docstore import DocstoreExplorer\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"VIVA Media AG changed it's name in 2004. What does their new acronym stand for?\"\n",
    "key = \"Gesellschaft mit beschränkter Haftung\"\n",
    "\n",
    "\n",
    "agent = ReflexionCoTAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"hotpotqa\"},\n",
    "    max_reflections=3,\n",
    "    max_trials=1,\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=HOTPOTQA_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_HOTPOTQA, \n",
    "    reflect_examples=HOTPOTQA_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_HOTPOTQA,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"VIVA Media AG changed it's name in 2004. What does their new acronym stand for?\"\n",
    "key = \"Gesellschaft mit beschränkter Haftung\"\n",
    "\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"hotpotqa\"},\n",
    "    max_steps=6,\n",
    "    max_tokens=3896,\n",
    "    docstore=DocstoreExplorer(Wikipedia()),\n",
    "    enc=tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=HOTPOTQA_FEWSHOT_EXAMPLES_REACT,\n",
    "    prompt=REFLEXION_REACT_INSTRUCTION_HOTPOTQA, \n",
    "    reflect_examples=HOTPOTQA_FEWSHOT_EXAMPLES_REFLEXION_REACT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_REACT_REFLECT_INSTRUCTION_HOTPOTQA,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_steps=6,\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "key = \"REFUTES\"\n",
    "\n",
    "\n",
    "agent = ReflexionCoTAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"fever\"},\n",
    "    max_reflections=3,\n",
    "    max_trials=1,\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=FEVER_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_FEVER, \n",
    "    reflect_examples=FEVER_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_FEVER,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "key = \"REFUTES\"\n",
    "\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"fever\"},\n",
    "    max_steps=6,\n",
    "    max_tokens=3896,\n",
    "    docstore=DocstoreExplorer(Wikipedia()),\n",
    "    enc=tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=FEVER_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_FEVER, \n",
    "    reflect_examples=FEVER_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_FEVER,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_steps=6,\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AmbigNQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did the simpsons first air on television?\"\n",
    "key = \"1989\"\n",
    "\n",
    "agent = ReflexionCoTAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"ambignq\"},\n",
    "    max_reflections=3,\n",
    "    max_trials=1,\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=AMBIGNQ_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_AMBIGNQ, \n",
    "    reflect_examples=AMBIGNQ_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_AMBIGNQ,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did the simpsons first air on television?\"\n",
    "key = \"1989\"\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"ambignq\"},\n",
    "    max_steps=6,\n",
    "    max_tokens=3896,\n",
    "    docstore=DocstoreExplorer(Wikipedia()),\n",
    "    enc=tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=AMBIGNQ_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_AMBIGNQ, \n",
    "    reflect_examples=AMBIGNQ_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_AMBIGNQ,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_steps=6,\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"\n",
    "key = \"Sinclair Lewis\"\n",
    "\n",
    "agent = ReflexionCoTAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"triviaqa\"},\n",
    "    max_reflections=3,\n",
    "    max_trials=1,\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=TRIVIAQA_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_TRIVIAQA, \n",
    "    reflect_examples=TRIVIAQA_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_TRIVIAQA,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"\n",
    "key = \"Sinclair Lewis\"\n",
    "\n",
    "agent = ReflexionReActAgent(\n",
    "    llm=llm,\n",
    "    mode={\"qa\": \"triviaqa\"},\n",
    "    max_steps=6,\n",
    "    max_tokens=3896,\n",
    "    docstore=DocstoreExplorer(Wikipedia()),\n",
    "    enc=tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
    ")\n",
    "out = agent.generate(\n",
    "    question=question, \n",
    "    key=key, \n",
    "    examples=TRIVIAQA_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_TRIVIAQA, \n",
    "    reflect_examples=TRIVIAQA_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_TRIVIAQA,\n",
    "    reflect_strategy=\"reflexion\",\n",
    "    additional_keys={},\n",
    "    reflect_additional_keys={},\n",
    "    max_steps=6,\n",
    "    max_trials=3,\n",
    "    patience=3,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agential.cog.eval.reflexion import EM\n",
    "EM(\"-123\", \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9867630\u001b[39m\n\u001b[0;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m ReflexionCoTAgent(llm\u001b[38;5;241m=\u001b[39mllm, mode\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsm8k\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGSM8K_FEWSHOT_EXAMPLES_COT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREFLEXION_COT_INSTRUCTION_GSM8K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreflect_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGSM8K_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreflect_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREFLEXION_COT_REFLECT_INSTRUCTION_GSM8K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreflect_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflexion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\agential\\agential\\cog\\agent\\reflexion.py:124\u001b[0m, in \u001b[0;36mReflexionCoTAgent.generate\u001b[1;34m(self, question, key, examples, prompt, reflect_examples, reflect_prompt, reflect_strategy, additional_keys, reflect_additional_keys, patience, reset, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m idx, patience_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalting_condition\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# Reflect if possible.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     reflections: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    128\u001b[0m     reflections_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\agential\\agential\\cog\\strategies\\reflexion\\math.py:186\u001b[0m, in \u001b[0;36mReflexionCoTMathStrategy.halting_condition\u001b[1;34m(self, idx, key, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhalting_condition\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    185\u001b[0m     max_trials \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_trials\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_trials)\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_trials\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\agential\\agential\\cog\\eval\\reflexion.py:19\u001b[0m, in \u001b[0;36mEM\u001b[1;34m(answer, key)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEM\u001b[39m(answer: \u001b[38;5;28mstr\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compares two strings, `answer` and `key`, after normalizing them.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    The Exact Match grading 'metric' compares for an exact match between 2 strings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        bool: True if the normalized `answer` and `key` match, else False.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalize_answer(answer) \u001b[38;5;241m==\u001b[39m \u001b[43mnormalize_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\agential\\agential\\utils\\parse.py:65\u001b[0m, in \u001b[0;36mnormalize_answer\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_answer\u001b[39m(s: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Normalize an answer by removing articles, fixing white spaces, and removing punctuation.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        str: The normalized string.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m white_space_fix(remove_articles(remove_punc(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m())))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with 4933828. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "key = -9867630\n",
    "\n",
    "agent = ReflexionCoTAgent(llm=llm, mode={\"math\": \"gsm8k\"})\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    key=key,\n",
    "    examples=GSM8K_FEWSHOT_EXAMPLES_COT,\n",
    "    prompt=REFLEXION_COT_INSTRUCTION_GSM8K,\n",
    "    reflect_examples=GSM8K_FEWSHOT_EXAMPLES_REFLEXION_COT_REFLECT,\n",
    "    reflect_prompt=REFLEXION_COT_REFLECT_INSTRUCTION_GSM8K,\n",
    "    reflect_strategy=\"reflexion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabMWP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MBPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
