{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agential.cog.agent.critic import CriticAgent\n",
    "from langchain_community.utilities.google_search import GoogleSearchAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from agential.cog.prompts.benchmark.hotpotqa import (\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_COT,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_DIRECT,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.fever import (\n",
    "    FEVER_FEWSHOT_EXAMPLES_COT,\n",
    "    FEVER_FEWSHOT_EXAMPLES_DIRECT,\n",
    "    FEVER_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.triviaqa import (\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_COT,\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_DIRECT,\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.ambignq import (\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_COT,\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_DIRECT,\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_REACT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.gsm8k import (\n",
    "    GSM8K_FEWSHOT_EXAMPLES_POT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.svamp import (\n",
    "    SVAMP_FEWSHOT_EXAMPLES_POT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.tabmwp import (\n",
    "    TABMWP_FEWSHOT_EXAMPLES_POT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.humaneval import (\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_POT,\n",
    ")\n",
    "from agential.cog.prompts.benchmark.mbpp import (\n",
    "    MBPP_FEWSHOT_EXAMPLES_POT,\n",
    ")\n",
    "\n",
    "from agential.cog.prompts.agent.critic import (\n",
    "    # QA.\n",
    "    CRITIC_INSTRUCTION_HOTPOTQA,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_HOTPOTQA,\n",
    "    HOTPOTQA_FEWSHOT_EXAMPLES_CRITIC,\n",
    "\n",
    "    CRITIC_INSTRUCTION_FEVER,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_FEVER,\n",
    "    FEVER_FEWSHOT_EXAMPLES_CRITIC,\n",
    "\n",
    "    CRITIC_INSTRUCTION_AMBIGNQ,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_AMBIGNQ,\n",
    "    AMBIGNQ_FEWSHOT_EXAMPLES_CRITIC,\n",
    "\n",
    "    CRITIC_INSTRUCTION_TRIVIAQA,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_TRIVIAQA,\n",
    "    TRIVIAQA_FEWSHOT_EXAMPLES_CRITIC,\n",
    "\n",
    "    # Math.\n",
    "    CRITIC_POT_INSTRUCTION_GSM8K,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_GSM8K,\n",
    "    GSM8K_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_GSM8K,\n",
    "    GSM8K_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "\n",
    "    CRITIC_POT_INSTRUCTION_SVAMP,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_SVAMP,\n",
    "    SVAMP_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_SVAMP,\n",
    "    SVAMP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "\n",
    "    CRITIC_POT_INSTRUCTION_TABMWP,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_TABMWP,\n",
    "    TABMWP_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_TABMWP,\n",
    "    TABMWP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "\n",
    "    # Code.\n",
    "    CRITIC_POT_INSTRUCTION_HUMANEVAL,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_HUMANEVAL,\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_HUMANEVAL,\n",
    "    HUMANEVAL_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "\n",
    "    CRITIC_POT_INSTRUCTION_MBPP,\n",
    "    CRITIC_CRITIQUE_INSTRUCTION_MBPP,\n",
    "    MBPP_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_MBPP,\n",
    "    MBPP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    ")\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring'\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, \n",
    "    mode={\"qa\": \"hotpotqa\"}, \n",
    "    search=search, \n",
    "    evidence_length=400,\n",
    "    num_results=8\n",
    ")\n",
    "use_tool = True\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=HOTPOTQA_FEWSHOT_EXAMPLES_COT,  # HOTPOTQA_FEWSHOT_EXAMPLES_DIRECT, HOTPOTQA_FEWSHOT_EXAMPLES_REACT\n",
    "    prompt=CRITIC_INSTRUCTION_HOTPOTQA,\n",
    "    critique_examples=HOTPOTQA_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_HOTPOTQA,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True,\n",
    "    # kwargs.\n",
    "    evidence_length=400,\n",
    "    num_results=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer=\"Thought: Let's think step by step. The person described was once considered the best kick boxer in the world but has been involved in controversies and crimes of violence. This description matches with the career and controversies of Badr Hari, a Moroccan-Dutch kickboxer. So the answer is Badr Hari.\\nAction: Finish[Badr Hari]\", critique='\\nThe question asks for the name of the kickboxer described, and the answer \"Badr Hari\" is a name. So it\\'s plausible.\\n\\n2. Truthfulness:\\n\\nLet\\'s search the question in google:\\n\\n> Search Query: Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring\\n> Evidence: [tyzhu/lmind_hotpot_train500_eval300_v1_qa · Datasets at Hugging ...] ... he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring. Answer\\xa0...\\n\\n', external_tool_info={'search_query': 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring', 'search_result': {'title': 'tyzhu/lmind_hotpot_train500_eval300_v1_qa · Datasets at Hugging ...', 'link': 'https://huggingface.co/datasets/tyzhu/lmind_hotpot_train500_eval300_v1_qa/viewer/default/train_qa', 'snippet': '... he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring. Answer\\xa0...'}}),\n",
       " CriticOutput(answer=\"Thought: Let's think step by step. The person described was once considered the best kick boxer in the world but has been involved in controversies and crimes of violence. This description matches with the career and controversies of Badr Hari, a Moroccan-Dutch kickboxer. So the answer is Badr Hari.\\nAction: Finish[Badr Hari]\", critique='\\nThe question asks for the name of the kickboxer described, and the answer \"Badr Hari\" is a name. So it\\'s plausible.\\n\\n2. Truthfulness:\\n\\nLet\\'s search the question in google:\\n\\n> Search Query: Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring\\n> Evidence: [tyzhu/lmind_hotpot_train500_eval300_v1_qa · Datasets at Hugging ...] ... he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring. Answer\\xa0...\\n\\n\\nThe evidence does not provide any useful information about the question.\\n\\nLet\\'s search the proposed answer in google:\\n\\n> Search Query: Badr Hari kickboxer controversies\\n> Evidence: [Wow, shit just got real. Badr Hari now facing attempted ...] Jul 27, 2012 ... Hari is not a stud. He is a thug with known ties to criminal organizations that disgrace the sport of kickboxing. This isn\\'t the first time he\\xa0...\\n\\nLet\\'s give the most possible answer.\\n\\nQuestion: Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring\\nHere\\'s ', external_tool_info={'search_query': 'Badr Hari kickboxer controversies', 'search_result': {'title': 'Wow, shit just got real. Badr Hari now facing attempted ...', 'link': 'https://www.reddit.com/r/MMA/comments/x9bx8/wow_shit_just_got_real_badr_hari_now_facing/', 'snippet': \"Jul 27, 2012 ... Hari is not a stud. He is a thug with known ties to criminal organizations that disgrace the sport of kickboxing. This isn't the first time he\\xa0...\"}}),\n",
       " CriticOutput(answer='The person described is Badr Hari, a Moroccan-Dutch kickboxer known for being involved in controversies and crimes of violence.', critique='The person described is Badr Hari, a Moroccan-Dutch kickboxer known for being involved in controversies and crimes of violence.', external_tool_info={'search_query': '', 'search_result': ''})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, \n",
    "    mode={\"qa\": \"fever\"}, \n",
    "    search=search,\n",
    "    evidence_length=400,\n",
    "    num_results=8\n",
    ")\n",
    "use_tool = False\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=FEVER_FEWSHOT_EXAMPLES_COT,  # FEVER_FEWSHOT_EXAMPLES_DIRECT, FEVER_FEWSHOT_EXAMPLES_REACT\n",
    "    prompt=CRITIC_INSTRUCTION_FEVER,\n",
    "    critique_examples=FEVER_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_FEVER,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True,\n",
    "    # kwargs.\n",
    "    evidence_length=400,\n",
    "    num_results=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='Yes, Nikolaj Coster-Waldau worked with the Fox Broadcasting Company as he appeared in the 2009 Fox television film Virtuality. So the answer is: SUPPORTS.', critique='Yes, Nikolaj Coster-Waldau worked with the Fox Broadcasting Company as he appeared in the 2009 Fox television film Virtuality. So the answer is: SUPPORTS.', external_tool_info={'search_query': '', 'search_result': ''})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AmbigNQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did the simpsons first air on television?\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, \n",
    "    mode={\"qa\": \"ambignq\"}, \n",
    "    search=search,\n",
    "    evidence_length=400,\n",
    "    num_results=8\n",
    ")\n",
    "use_tool = False\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=AMBIGNQ_FEWSHOT_EXAMPLES_COT,  # AMBIGNQ_FEWSHOT_EXAMPLES_DIRECT, AMBIGNQ_FEWSHOT_EXAMPLES_REACT\n",
    "    prompt=CRITIC_INSTRUCTION_AMBIGNQ,\n",
    "    critique_examples=AMBIGNQ_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_AMBIGNQ,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True,\n",
    "    # kwargs.\n",
    "    evidence_length=400,\n",
    "    num_results=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='1989', critique='\\n\\nThe question asks for a specific time, and the answer is \"1989\", which is a year. So it\\'s plausible.\\n\\n2. Truthfulness:\\n\\nLet\\'s verify the accuracy of the answer by searching the question in Google:\\n\\n> Search Query: When did the simpsons first air on television?\\n> Evidence: [The Simpsons - Wikipedia] The Simpsons first aired on television on December 17, 1989.\\n\\nThe evidence confirms that The Simpsons first aired on television on December 17, 1989, not just in 1989. Therefore, the answer should specify the exact date.\\n\\nQuestion: When did the simpsons first air on television?\\nHere\\'s the most possible answer: The Simpsons first aired on television on December 17, 1989. So the answer is: December 17, 1989.', external_tool_info={'search_query': 'When did the simpsons first air on television?', 'search_result': \"[The Simpsons - Wikipedia] The Simpsons first aired on television on December 17, 1989.\\n\\nThe evidence confirms that The Simpsons first aired on television on December 17, 1989, not just in 1989. Therefore, the answer should specify the exact date.\\n\\nQuestion: When did the simpsons first air on television?\\nHere's the most possible answer: The Simpsons first aired on television on December 17, 1989. So the answer is: December 17, 1989.\"}),\n",
       " CriticOutput(answer='The Simpsons first aired on television on December 17, 1989. So the answer is: December 17, 1989.', critique='The Simpsons first aired on television on December 17, 1989. So the answer is: December 17, 1989.', external_tool_info={'search_query': '', 'search_result': ''})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which American-born Sinclair won the Nobel Prize for Literature in 1930?\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, \n",
    "    mode={\"qa\": \"triviaqa\"}, \n",
    "    search=search,\n",
    "    evidence_length=400,\n",
    "    num_results=8\n",
    ")\n",
    "use_tool = False\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=TRIVIAQA_FEWSHOT_EXAMPLES_COT,  # TRIVIAQA_FEWSHOT_EXAMPLES_DIRECT, TRIVIAQA_FEWSHOT_EXAMPLES_REACT\n",
    "    prompt=CRITIC_INSTRUCTION_TRIVIAQA,\n",
    "    critique_examples=TRIVIAQA_FEWSHOT_EXAMPLES_CRITIC,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_TRIVIAQA,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True,\n",
    "    # kwargs.\n",
    "    evidence_length=400,\n",
    "    num_results=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='Upton Sinclair', critique='\\n\\nThe question asks for the name of an American-born Sinclair who won the Nobel Prize for Literature in 1930, and the answer is \"Upton Sinclair\", which is a plausible name. So it\\'s plausible.\\n\\n2. Truthfulness:\\n\\nLet\\'s search the question in google:\\n\\n> Search Query: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\\n> Evidence: [Winners of the Nobel Prize in Literature - Wikipedia] The 1930 Nobel Prize in Literature was awarded to Sinclair Lewis, an American writer.\\n\\nThe evidence shows that Sinclair Lewis, not Upton Sinclair, won the Nobel Prize for Literature in 1930. Therefore, the proposed answer is incorrect.\\n\\nQuestion: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\\nHere\\'s the most possible answer: Sinclair Lewis won the Nobel Prize for Literature in 1930. So the answer is: Sinclair Lewis.', external_tool_info={'search_query': 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?', 'search_result': \"[Winners of the Nobel Prize in Literature - Wikipedia] The 1930 Nobel Prize in Literature was awarded to Sinclair Lewis, an American writer.\\n\\nThe evidence shows that Sinclair Lewis, not Upton Sinclair, won the Nobel Prize for Literature in 1930. Therefore, the proposed answer is incorrect.\\n\\nQuestion: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\\nHere's the most possible answer: Sinclair Lewis won the Nobel Prize for Literature in 1930. So the answer is: Sinclair Lewis.\"}),\n",
       " CriticOutput(answer='Sinclair Lewis won the Nobel Prize for Literature in 1930. So the answer is: Sinclair Lewis.', critique='Sinclair Lewis won the Nobel Prize for Literature in 1930. So the answer is: Sinclair Lewis.', external_tool_info={'search_query': '', 'search_result': ''})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with 4933828. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, mode={\"math\": \"gsm8k\"}, patience=2\n",
    ")\n",
    "\n",
    "use_tool = True\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=GSM8K_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_GSM8K,\n",
    "    critique_examples=GSM8K_FEWSHOT_EXAMPLES_CRITIC if use_tool else GSM8K_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_GSM8K if use_tool else CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_GSM8K,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='eggs_laid_per_day = 16\\neggs_used_for_breakfast = 3\\neggs_used_for_muffins = 4933828\\neggs_sold_per_day = eggs_laid_per_day - eggs_used_for_breakfast - eggs_used_for_muffins\\nmoney_made_per_day = eggs_sold_per_day * 2\\nanswer = money_made_per_day', critique=\"1. The amount of money made per day should be a positive number, -9867630 < 0, which is not reasonable.\\n\\n2. Let's check the code:\\n\\n- `eggs_laid_per_day = 16` - This defines the number of eggs laid per day, which is correct.\\n- `eggs_used_for_breakfast = 3` - This defines the number of eggs used for breakfast, which is correct.\\n- `eggs_used_for_muffins = 4933828` - This defines the number of eggs used for muffins, which seems extremely high and might be a mistake.\\n- `eggs_sold_per_day = eggs_laid_per_day - eggs_used_for_breakfast - eggs_used_for_muffins` - This calculates the number of eggs sold per day, but the calculation might be incorrect due to the large number of eggs used for muffins.\\n- `money_made_per_day = eggs_sold_per_day * 2` - This calculates the money made per day at the farmers' market, but the negative output indicates an issue with the previous calculations.\\n\\nGiven the negative output, it seems that the calculation for the number of eggs sold per day might be incorrect due to the extremely high number of eggs used for muffins.\\n\\nTo solve this problem, let's reassess the number of eggs used for muffins and calculate the correct number of eggs sold per day.\\n\\n\", external_tool_info={'execution_status': 'Done', 'code_answer': -9867630}),\n",
       " CriticOutput(answer=\"eggs_laid_per_day = 16\\neggs_used_for_breakfast = 3\\neggs_used_for_muffins = 12  # Let's assume a more reasonable number for eggs used in muffins\\neggs_sold_per_day = eggs_laid_per_day - eggs_used_for_breakfast - eggs_used_for_muffins\\nmoney_made_per_day = eggs_sold_per_day * 2\\nanswer = money_made_per_day\", critique=\"The code provided calculates the amount of money Janet makes each day by selling the remaining duck eggs at the farmers' market. However, there are some issues with the code:\\n\\n1. The calculation for the number of eggs used for muffins seems incorrect. If Janet eats 3 eggs for breakfast and uses 12 eggs for muffins, that totals to 15 eggs, leaving only 1 egg to sell. This could be a mistake in the assumption of eggs used for muffins.\\n\\n2. The calculation for the number of eggs sold per day is incorrect. It should be `eggs_laid_per_day - eggs_used_for_breakfast - eggs_used_for_muffins` to get the correct number of eggs available for sale.\\n\\n3. The final calculation for the money made per day is correct based on the number of eggs sold.\\n\\nTo improve the code and make it more accurate, we can correct the calculation for the number of eggs used for muffins and update the calculation for the number of eggs sold per day. \", external_tool_info={'execution_status': 'Done', 'code_answer': 2}),\n",
       " CriticOutput(answer='eggs_laid_per_day = 16\\neggs_used_for_breakfast = 3\\neggs_used_for_muffins = 10  # Assume 10 eggs are used for muffins\\neggs_sold_per_day = eggs_laid_per_day - eggs_used_for_breakfast - eggs_used_for_muffins\\nmoney_made_per_day = eggs_sold_per_day * 2\\nanswer = money_made_per_day', critique='The problem with the above code is the calculation of the eggs used for muffins. The code assumes that 10 eggs are used for muffins, but the question does not explicitly state that. \\n\\nTo correct this, we need to adjust the calculation of eggs used for muffins based on the information provided in the question. \\n\\n', external_tool_info={'execution_status': 'Done', 'code_answer': 6})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups. How big is each group of bananas?\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, mode={\"math\": \"svamp\"}, patience=2\n",
    ")\n",
    "\n",
    "use_tool = True\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=SVAMP_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_SVAMP,\n",
    "    critique_examples=SVAMP_FEWSHOT_EXAMPLES_CRITIC if use_tool else SVAMP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_SVAMP if use_tool else CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_SVAMP,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='num_oranges = 87\\nnum_bananas = 290\\nnum_orange_groups = 93\\nnum_banana_groups = 2\\nanswer = num_bananas / num_banana_groups', critique=\"1. The answer, 145.0, is a reasonable result.\\n2. Let's check the code:\\n\\nThe code correctly calculates the size of each group of bananas by dividing the total number of bananas by the number of banana groups. There is no problem with the above code.\", external_tool_info={'execution_status': 'Done', 'code_answer': 145.0}),\n",
       " CriticOutput(answer='num_oranges = 87\\nnum_bananas = 290\\nnum_orange_groups = 93\\nnum_banana_groups = 2\\n\\n# Calculate the size of each group of bananas\\nbanana_group_size = num_bananas / num_banana_groups\\nanswer = banana_group_size', critique=\"1. The answer, 145, is a reasonable result.\\n2. Let's check the code:\\n\\nThe code correctly calculates the size of each group of bananas by dividing the total number of bananas by the number of banana groups. There is no problem with the above code. \\n\\nTherefore, there is no issue with the provided code for this question.\", external_tool_info={'execution_status': 'Done', 'code_answer': 145.0}),\n",
       " CriticOutput(answer='num_oranges = 87\\nnum_bananas = 290\\nnum_orange_groups = 93\\nnum_banana_groups = 2\\n\\n# Calculate the size of each group of bananas\\nbanana_group_size = num_bananas / num_banana_groups\\nanswer = banana_group_size', critique=\"1. The answer, 145.0, is a reasonable result.\\n2. Let's check the code:\\n\\n> banana_group_size = num_bananas / num_banana_groups\\n\\nThe code correctly calculates the size of each group of bananas by dividing the total number of bananas by the number of groups they are organized into. There is no problem with the above code.\", external_tool_info={'execution_status': 'Done', 'code_answer': 145.0})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabMWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Read the following table regarding \"Bowling Scores\" and then write Python code to answer a question:\n",
    "\n",
    "Name | Score\n",
    "Amanda | 117\n",
    "Sam | 236\n",
    "Irma | 144\n",
    "Mike | 164\n",
    "\n",
    "Question: Some friends went bowling and kept track of their scores. How many more points did Mike score than Irma?\"\"\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, mode={\"math\": \"tabmwp\"}, patience=2\n",
    ")\n",
    "\n",
    "use_tool = True\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=TABMWP_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_TABMWP,\n",
    "    critique_examples=TABMWP_FEWSHOT_EXAMPLES_CRITIC if use_tool else TABMWP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_TABMWP if use_tool else CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_TABMWP,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='# Python Code, return answer\\nmike_score = 164\\nirma_score = 144\\ndifference = mike_score - irma_score\\ndifference', critique='There is no problem with the above code. It correctly calculates the difference in scores between Mike and Irma and stores it in the `difference` variable. The code answers the question accurately.', external_tool_info={'execution_status': 'Done', 'code_answer': ''}),\n",
       " CriticOutput(answer='mike_score = 164\\nirma_score = 144\\ndifference = mike_score - irma_score\\nanswer = difference\\nanswer', critique='There is no problem with the above code. It correctly calculates the difference in scores between Mike and Irma and provides the answer as output.', external_tool_info={'execution_status': 'Done', 'code_answer': 20}),\n",
       " CriticOutput(answer='# Given bowling scores\\nmike_score = 164\\nirma_score = 144\\n\\n# Calculate the difference in scores\\ndifference = mike_score - irma_score\\n\\n# Store the answer in a variable\\nanswer = difference\\n\\n# Print the answer\\nprint(answer)', critique='There is no problem with the above code. It correctly calculates the difference in scores between Mike and Irma and stores the answer in the variable \"answer.\"', external_tool_info={'execution_status': 'Done', 'code_answer': 20})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = {\"task_id\": \"HumanEval/0\", \"prompt\": \"from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \\\"\\\"\\\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \\\"\\\"\\\"\\n\", \"entry_point\": \"has_close_elements\", \"canonical_solution\": \"    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n\", \"test\": \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\"}\n",
    "question = inst['prompt']\n",
    "tests = f\"{inst['test']}\\ncheck({inst['entry_point']})\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, mode={\"code\": \"humaneval\"}\n",
    ")\n",
    "use_tool = False\n",
    "\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=HUMANEVAL_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_HUMANEVAL,\n",
    "    critique_examples=HUMANEVAL_FEWSHOT_EXAMPLES_CRITIC if use_tool else HUMANEVAL_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_HUMANEVAL if use_tool else CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_HUMANEVAL,\n",
    "    additional_keys={},\n",
    "    critique_additional_keys={\"tests\": tests},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='def has_close_elements(numbers, threshold):\\n    return any(abs(a - b) < threshold for i, a in enumerate(numbers) for b in numbers[i+1:])', critique='The code snippet provided has a logical error in the `has_close_elements` function implementation. ', external_tool_info={}),\n",
       " CriticOutput(answer='    return any(abs(a - b) < threshold for i, a in enumerate(numbers) for b in numbers[i+1:])', critique=\"The function `has_close_elements` has a logical flaw in how it checks for close elements in the list. \\n\\n1. The function uses a nested loop to compare each number with all subsequent numbers, which can lead to redundant comparisons and incorrect results.\\n\\n2. Let's analyze the code:\\n\\n- The function iterates over each number (a) in the list and then compares it with all subsequent numbers (b) in the list.\\n\\n- The condition `abs(a - b) < threshold` checks if the absolute difference between any two numbers is less than the threshold.\\n\\n- The function returns `True` if any two numbers are closer than the threshold.\\n\\nThe problem with this approach is that it can lead to redundant comparisons and incorrect results. For example, if the list contains `[1.0, 2.0, 3.0, 4.0]` and the threshold is set to `0.5`, the function would return `True` because `abs(1.0 - 2.0) < 0.5`, `abs(1.0 - 3.0) < 0.5`, and `abs(2.0 - 3.0) < 0.5` are all true, even though there are no numbers closer than `0.5` in the list.\\n\\n3. To improve the function, we can iterate over unique pairs of numbers in the list to avoid redundant comparisons. Additionally, we should ensure that we are comparing distinct numbers in the list.\", external_tool_info={}),\n",
       " CriticOutput(answer='    n = len(numbers)\\n    for i in range(n):\\n        for j in range(i + 1, n):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False', critique='The code provided is correct and functions as intended. It defines a function `has_close_elements` that checks if there are any two numbers in a list that are closer to each other than a given threshold. The function uses nested loops to compare each pair of numbers in the list and returns True if any pair is closer than the threshold, otherwise it returns False.\\n\\nAdditionally, there is a `check` function that tests the `has_close_elements` function with different test cases and asserts the results. The test cases cover scenarios where the function should return True or False based on the proximity of numbers in the list.\\n\\nOverall, there are no issues with the code provided.', external_tool_info={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MBPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a python function to find the first repeated character in a given string.\"\n",
    "tests = \"\"\"assert first_repeated_char(\"abcabc\") == \"a\"\n",
    "assert first_repeated_char(\"abc\") == None\n",
    "assert first_repeated_char(\"123123\") == \"1\\\"\"\"\"\n",
    "\n",
    "agent = CriticAgent(\n",
    "    llm=llm, mode={\"code\": \"mbpp\"}\n",
    ")\n",
    "use_tool = True\n",
    "\n",
    "\n",
    "out = agent.generate(\n",
    "    question=question,\n",
    "    examples=MBPP_FEWSHOT_EXAMPLES_POT,\n",
    "    prompt=CRITIC_POT_INSTRUCTION_MBPP,\n",
    "    critique_examples=MBPP_FEWSHOT_EXAMPLES_CRITIC if use_tool else MBPP_FEWSHOT_EXAMPLES_CRITIC_NO_TOOL,\n",
    "    critique_prompt=CRITIC_CRITIQUE_INSTRUCTION_MBPP if use_tool else CRITIC_CRITIQUE_NO_TOOL_INSTRUCTION_MBPP,\n",
    "    additional_keys={\"tests\": tests},\n",
    "    critique_additional_keys={\"tests\": tests},\n",
    "    max_interactions=3,\n",
    "    use_tool=use_tool,\n",
    "    reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CriticOutput(answer='def first_repeated_char(s):\\n    char_count = {}\\n    for char in s:\\n        if char in char_count:\\n            return char\\n        else:\\n            char_count[char] = 1\\n    return None\\n\\n# Testing the function with the provided test cases\\nassert first_repeated_char(\"abcabc\") == \"a\"\\nassert first_repeated_char(\"abc\") == None\\nassert first_repeated_char(\"123123\") == \"1\"', critique='There is no problem with the above code. The function `first_repeated_char` correctly finds the first repeated character in a given string by keeping track of character counts in a dictionary and returning the first character that is repeated. The function passes the provided test cases successfully.', external_tool_info={'execution_status': 'Done'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
